{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8827c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aece326",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load data from Google Drive\n",
    "data_path = '/content/drive/MyDrive/SWE-Bench-CL.json'\n",
    "data = load_swe_bench_cl(data_path)\n",
    "\n",
    "# Display basic dataset info\n",
    "print(f\"Available repositories: {data['metadata']['repositories']}\")\n",
    "print(f\"Total tasks: {data['metadata']['total_tasks']}\")\n",
    "print(f\"Number of sequences: {data['metadata']['num_sequences']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e42500",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load CodeLlama model\n",
    "print(\"Loading CodeLlama model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-hf\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test the model with a sample task\n",
    "first_sequence = data['sequences'][0]\n",
    "first_task = first_sequence['tasks'][0]\n",
    "prompt = format_prompt(first_task)\n",
    "print(f\"\\nExample prompt:\\n{prompt}\")\n",
    "\n",
    "# Generate a solution\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=4096,\n",
    "        temperature=0.2,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "solution = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(f\"\\nGenerated solution:\\n{solution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d50749",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_small_experiment(sequence, num_tasks=5):\n",
    "    \"\"\"Run a small experiment on a sequence with first N tasks.\"\"\"\n",
    "    results = []\n",
    "    augmenter = ContextAugmenter()\n",
    "    \n",
    "    # Get first N tasks from the sequence\n",
    "    tasks = sequence['tasks'][:num_tasks]\n",
    "    \n",
    "    for task in tasks:\n",
    "        print(f\"\\nEvaluating task {task['metadata']['instance_id']}\")\n",
    "        \n",
    "        # Zero-shot evaluation\n",
    "        zero_shot_result = evaluate_zero_shot(task)\n",
    "        \n",
    "        # Track tool usage\n",
    "        tool_tracker.analyze_response(zero_shot_result[\"task_id\"], zero_shot_result[\"solution\"])\n",
    "        \n",
    "        # Augmented evaluation\n",
    "        augmenter.add_to_memory(task, zero_shot_result[\"solution\"])\n",
    "        augmented_prompt = augmenter.format_augmented_prompt(task)\n",
    "        augmented_result = evaluate_zero_shot(task, prompt=augmented_prompt)\n",
    "        \n",
    "        results.append({\n",
    "            \"task_id\": task[\"metadata\"][\"instance_id\"],\n",
    "            \"zero_shot\": zero_shot_result,\n",
    "            \"augmented\": augmented_result\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiment on first sequence\n",
    "first_sequence = data['sequences'][0]\n",
    "experiment_results = run_small_experiment(first_sequence)\n",
    "\n",
    "# Calculate metrics\n",
    "zero_shot_success = sum(1 for r in experiment_results if r[\"zero_shot\"][\"success\"]) / len(experiment_results)\n",
    "augmented_success = sum(1 for r in experiment_results if r[\"augmented\"][\"success\"]) / len(experiment_results)\n",
    "\n",
    "print(f\"\\nExperiment Results:\")\n",
    "print(f\"Zero-shot success rate: {zero_shot_success:.2f}\")\n",
    "print(f\"Augmented success rate: {augmented_success:.2f}\")\n",
    "print(f\"Improvement: {augmented_success - zero_shot_success:.2f}\")\n",
    "\n",
    "# Visualize tool usage\n",
    "tool_tracker.visualize_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8cd78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "import json\n",
    "\n",
    "def save_experiment_results(results, filename=\"swe_bench_cl_experiment_results.json\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Save our experiment results\n",
    "save_experiment_results(experiment_results)\n",
    "\n",
    "# Visualize results\n",
    "def plot_experiment_results(results):\n",
    "    zero_shot_rates = [1 if r[\"zero_shot\"][\"success\"] else 0 for r in results]\n",
    "    augmented_rates = [1 if r[\"augmented\"][\"success\"] else 0 for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(zero_shot_rates, label=\"Zero-shot\")\n",
    "    plt.plot(augmented_rates, label=\"Augmented\")\n",
    "    plt.xlabel(\"Task Number\")\n",
    "    plt.ylabel(\"Success (1) / Failure (0)\")\n",
    "    plt.title(\"Performance Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot our results\n",
    "plot_experiment_results(experiment_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
