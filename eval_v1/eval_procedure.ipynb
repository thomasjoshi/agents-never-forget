{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # SWE-Bench-CL: Comprehensive Continual Learning Evaluation for LLMs (with Resumability)\n",
    "\n",
    "\n",
    "\n",
    "    This notebook implements a framework for evaluating coding LLMs on the SWE-Bench-CL dataset,\n",
    "\n",
    "    focusing on continual learning capabilities. It includes:\n",
    "\n",
    "    - Sequential task processing with an optional semantic memory (RAG) system.\n",
    "\n",
    "    - Automated execution of the `swebench.harness.run_evaluation` script.\n",
    "\n",
    "    - Calculation of various continual learning metrics (ACC, F, FT, BWT, AULC, CL-Score).\n",
    "\n",
    "    - State management for resumability of long evaluation runs.\n",
    "\n",
    "    - A quick test block for debugging the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q datasets pandas python-dotenv langchain langchain_anthropic langchain_openai langchain_google_genai langchain-ollama faiss-cpu tiktoken numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import time\n",
    "import subprocess # For running harness\n",
    "import shutil # For cleaning up testbeds\n",
    "import numpy as np\n",
    "import torch # For checking MPS availability\n",
    "from pathlib import Path # For rglob and path manipulation\n",
    "import re\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "# LangChain components\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Embeddings and Vector Stores\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Automatic SWE-bench setup ---\n",
    "def setup_swe_bench(repo_path: str):\n",
    "    \"\"\"Checks for SWE-bench repository and attempts to set it up if not found.\"\"\"\n",
    "    swe_bench_dir = Path(repo_path)\n",
    "    if swe_bench_dir.is_dir() and (swe_bench_dir / \".git\").is_dir():\n",
    "        logger.info(f\"SWE-bench repository found at {repo_path}.\")\n",
    "        # Optionally, check if pip install -e . was run (e.g., by checking for swebench.egg-info)\n",
    "        # For simplicity, we'll assume if the repo is there, it might be set up.\n",
    "        # A more robust check could try importing swebench.\n",
    "        try:\n",
    "            import swebench\n",
    "            logger.info(\"SWE-bench package is importable.\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            logger.warning(\"SWE-bench repository found, but package not importable. Attempting to install.\")\n",
    "            try:\n",
    "                subprocess.run([\"pip\", \"install\", \"-e\", \".\"], cwd=repo_path, check=True, capture_output=True, text=True)\n",
    "                logger.info(f\"Successfully ran 'pip install -e .' in {repo_path}.\")\n",
    "                # Verify import again\n",
    "                import swebench\n",
    "                logger.info(\"SWE-bench package is now importable.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to install SWE-bench from {repo_path}: {e.stderr if hasattr(e, 'stderr') else e}\")\n",
    "                logger.error(\"Please manually install SWE-bench: cd into the SWE-bench directory and run 'pip install -e .'\")\n",
    "                return False\n",
    "    else:\n",
    "        logger.warning(f\"SWE-bench repository not found at {repo_path}. Attempting to clone and install.\")\n",
    "        try:\n",
    "            # Create parent directory if it doesn't exist, to avoid clone errors if repo_path is like \"./foo/SWE-bench\"\n",
    "            swe_bench_dir.parent.mkdir(parents=True, exist_ok=True)\n",
    "            logger.info(f\"Cloning SWE-bench into {repo_path}...\")\n",
    "            subprocess.run(\n",
    "                [\"git\", \"clone\", \"git@github.com:princeton-nlp/SWE-bench.git\", str(swe_bench_dir)], \n",
    "                check=True, capture_output=True, text=True\n",
    "            )\n",
    "            logger.info(f\"Successfully cloned SWE-bench to {repo_path}.\")\n",
    "            logger.info(f\"Installing SWE-bench from {repo_path}...\")\n",
    "            subprocess.run(\n",
    "                [\"pip\", \"install\", \"-e\", \".\"], \n",
    "                cwd=repo_path, check=True, capture_output=True, text=True\n",
    "            )\n",
    "            logger.info(f\"Successfully ran 'pip install -e .' in {repo_path}.\")\n",
    "            # Verify import\n",
    "            import swebench\n",
    "            logger.info(\"SWE-bench package is now importable.\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"Error during SWE-bench setup: {e.stderr}\")\n",
    "            logger.error(\"Please ensure Git is installed and you have SSH access to GitHub if using git@github.com URLs.\")\n",
    "            logger.error(\"Alternatively, clone SWE-bench manually and install with 'pip install -e .'\")\n",
    "            return False\n",
    "        except ImportError:\n",
    "            logger.error(f\"Even after attempted install, SWE-bench package is not importable from {repo_path}.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred during SWE-bench setup: {e}\")\n",
    "            return False\n",
    "\n",
    "# Call the setup function early\n",
    "# Note: SWE_BENCH_REPO_PATH is defined in the Configuration section below.\n",
    "# We will call setup_swe_bench() after SWE_BENCH_REPO_PATH is defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 2. Configuration\n",
    "\n",
    "    Set paths, model configurations, and experiment parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "# NOTE: Update these paths according to your local setup\n",
    "DOTENV_PATH = '../.env'\n",
    "SWE_BENCH_CL_DATASET_PATH = \"../data/SWE-Bench-CL-Curriculum.json\" # Path to your SWE-Bench-CL JSON file\n",
    "SWE_BENCH_REPO_PATH = \"./SWE-bench\"  # Path to your cloned swe-bench repository (for running the harness)\n",
    "BASE_OUTPUT_DIR = \"eval_results\" # Base directory for all outputs\n",
    "STATE_FILE_PATH = os.path.join(BASE_OUTPUT_DIR, \"eval_state.json\")\n",
    "\n",
    "# --- Attempt to set up SWE-bench ---\n",
    "if not setup_swe_bench(SWE_BENCH_REPO_PATH):\n",
    "    logger.error(\"SWE-bench setup failed. The script may not run correctly. Please set up SWE-bench manually.\")\n",
    "    # Depending on strictness, you might want to raise an error here:\n",
    "    # raise RuntimeError(\"SWE-bench setup failed. Halting.\")\n",
    "else:\n",
    "    logger.info(\"SWE-bench setup check complete.\")\n",
    "\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "MODELS_TO_EVALUATE = [\n",
    "    # \"google/gemini-1.5-flash-latest\",\n",
    "    \"google/gemini-2.0-flash\",\n",
    "    # \"openai/gpt-4o-mini\",\n",
    "    # \"ollama/llama3\"\n",
    "]\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS_OUTPUT = 4096\n",
    "\n",
    "# --- Semantic Memory Configuration ---\n",
    "EMBEDDING_MODEL_CONFIG = {\n",
    "    \"name\": \"ollama/nomic-embed-text\",\n",
    "    \"max_context_tokens_for_retrieval\": 2000,\n",
    "    \"num_retrieved_memories\": 3\n",
    "}\n",
    "\n",
    "# --- Experiment Configuration ---\n",
    "EXPERIMENT_CONDITIONS = {\n",
    "    \"memory_enabled\": {\"memory_enabled\": True},\n",
    "    \"memory_disabled\": {\"memory_enabled\": False},\n",
    "}\n",
    "SEQUENCES_TO_RUN = None # Example: [\"django_django_sequence\"] or None for all\n",
    "TASKS_PER_SEQUENCE_LIMIT = None # Example: 2 or None for all\n",
    "\n",
    "# --- Harness Configuration ---\n",
    "HARNESS_TIMEOUT_PER_TASK = 900\n",
    "HARNESS_MAX_WORKERS = 4\n",
    "HARNESS_RUN_ID_PREFIX = \"\"\n",
    "\n",
    "# --- CL Metrics Configuration ---\n",
    "CL_SCORE_WEIGHTS = {\n",
    "    \"lambda_F\": 1.0, \"lambda_FT\": 1.0, \"lambda_BWT\": 1.0, \"lambda_AULC\": 1.0,\n",
    "}\n",
    "\n",
    "# Create base output directory if it doesn't exist\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 2.1. State Management for Resumability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_initialize_state(force_fresh_start=False) -> Dict:\n",
    "    \"\"\"Loads evaluation state from STATE_FILE_PATH or initializes a new one.\"\"\"\n",
    "    if os.path.exists(STATE_FILE_PATH) and not force_fresh_start:\n",
    "        try:\n",
    "            with open(STATE_FILE_PATH, 'r') as f:\n",
    "                state = json.load(f)\n",
    "            logger.info(f\"Loaded existing state from {STATE_FILE_PATH} for run {state.get('evaluation_run_timestamp')}\")\n",
    "            \n",
    "            # Check if dataset path matches, if not, force fresh state for safety\n",
    "            if state.get('current_swe_bench_cl_dataset_path') != os.path.abspath(SWE_BENCH_CL_DATASET_PATH):\n",
    "                logger.warning(\"Dataset path in state file does not match current configuration. Forcing a fresh state.\")\n",
    "                return load_or_initialize_state(force_fresh_start=True) # Recursive call for fresh start\n",
    "            \n",
    "            # Ensure backward compatibility for task_eval_progress structure if loading older states\n",
    "            # No explicit change needed here for adding a new key, as older entries just won't have it.\n",
    "            # Code reading from it should use .get(\"gold_patch\") for safety.\n",
    "            return state\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading state file {STATE_FILE_PATH}: {e}. Initializing fresh state.\")\n",
    "            # Fall through to initialize fresh state\n",
    "    \n",
    "    # Initialize fresh state\n",
    "    new_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logger.info(f\"Initializing fresh evaluation state with run timestamp: {new_timestamp}\")\n",
    "    state = {\n",
    "        \"evaluation_run_timestamp\": new_timestamp,\n",
    "        \"current_swe_bench_cl_dataset_path\": os.path.abspath(SWE_BENCH_CL_DATASET_PATH),\n",
    "        \"task_eval_progress\": {}, # model_name -> condition_name -> {instance_id: {\"model_patch\": str, \"harness_result\": bool, \"raw_llm_output\": Optional[str], \"gold_patch\": Optional[str]}}\n",
    "        \"parsed_harness_results_data\": {},# model -> condition -> {seq_id: {instance_id: pass_status}}\n",
    "        \"overall_results_list\": [],      # List of dicts for final summary\n",
    "        \"overall_results_summary_df_path\": None\n",
    "    }\n",
    "    save_state(state)\n",
    "    return state\n",
    "\n",
    "def save_state(state: Dict):\n",
    "    \"\"\"Saves the current evaluation state to STATE_FILE_PATH.\"\"\"\n",
    "    try:\n",
    "        with open(STATE_FILE_PATH, 'w') as f:\n",
    "            json.dump(state, f, indent=4)\n",
    "        logger.debug(f\"Saved state to {STATE_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving state to {STATE_FILE_PATH}: {e}\")\n",
    "\n",
    "# Load or initialize global state\n",
    "current_evaluation_state = load_or_initialize_state()\n",
    "EVALUATION_RUN_TIMESTAMP = current_evaluation_state[\"evaluation_run_timestamp\"]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 3. LLM and Embedding Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys\n",
    "try:\n",
    "    if os.path.exists(DOTENV_PATH):\n",
    "        load_dotenv(DOTENV_PATH)\n",
    "        logger.info(f\"Loaded .env file from: {DOTENV_PATH}\")\n",
    "    else:\n",
    "        logger.warning(f\".env file not found at {DOTENV_PATH}. API calls to proprietary models might fail if keys not in environment.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading .env file: {e}\")\n",
    "\n",
    "# Function to initialize LLM (adapted from your provided code)\n",
    "def get_llm(model_str: str, temp: float, max_tokens: int):\n",
    "    provider, model_name = model_str.split(\"/\", 1)\n",
    "    try:\n",
    "        if provider == \"anthropic\":\n",
    "            api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "            if not api_key: logger.warning(f\"ANTHROPIC_API_KEY not found for {model_str}.\"); return None\n",
    "            return ChatAnthropic(model=model_name, temperature=temp, max_tokens=max_tokens, api_key=api_key)\n",
    "        elif provider == \"openai\":\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key: logger.warning(f\"OPENAI_API_KEY not found for {model_str}.\"); return None\n",
    "            return ChatOpenAI(model=model_name, temperature=temp, max_tokens=max_tokens, api_key=api_key)\n",
    "        elif provider == \"google\":\n",
    "            api_key = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
    "            if not api_key: logger.warning(f\"GEMINI_API_KEY/GOOGLE_API_KEY not found for {model_str}.\"); return None\n",
    "            return ChatGoogleGenerativeAI(model=model_name, temperature=temp, max_output_tokens=max_tokens, google_api_key=api_key)\n",
    "        elif provider == \"ollama\":\n",
    "            return ChatOllama(model=model_name, temperature=temp) # max_tokens often part of ollama run options or model file\n",
    "        else:\n",
    "            logger.error(f\"Unsupported provider: {provider}\"); return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize LLM {model_str}: {e}\"); return None\n",
    "\n",
    "# Initialize configured LLMs\n",
    "initialized_llms = {}\n",
    "for name in MODELS_TO_EVALUATE:\n",
    "    llm = get_llm(name, TEMPERATURE, MAX_TOKENS_OUTPUT)\n",
    "    if llm:\n",
    "        initialized_llms[name] = llm\n",
    "        logger.info(f\"Successfully initialized LLM: {name}\")\n",
    "    else:\n",
    "        logger.warning(f\"Could not initialize LLM: {name}. It will be skipped.\")\n",
    "\n",
    "if not initialized_llms:\n",
    "    raise RuntimeError(\"No LLMs were successfully initialized. Halting.\")\n",
    "logger.info(f\"LLMs ready for evaluation: {list(initialized_llms.keys())}\")\n",
    "\n",
    "\n",
    "# Initialize Embedding Model and Tokenizer\n",
    "active_embedding_model = None\n",
    "tokenizer_for_counting = None\n",
    "try:\n",
    "    emb_config_name = EMBEDDING_MODEL_CONFIG[\"name\"]\n",
    "    if emb_config_name.startswith(\"openai/\"):\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key: logger.warning(\"OpenAI API key not found for embeddings.\")\n",
    "        else: active_embedding_model = OpenAIEmbeddings(model=emb_config_name.split(\"/\")[1], openai_api_key=api_key)\n",
    "    elif emb_config_name.startswith(\"ollama/\"):\n",
    "        model_ollama_name = emb_config_name.split(\"/\")[1]\n",
    "        try:\n",
    "            active_embedding_model = OllamaEmbeddings(model=model_ollama_name)\n",
    "            active_embedding_model.embed_query(\"test connection\")\n",
    "            logger.info(f\"Ollama embedding model '{model_ollama_name}' initialized.\")\n",
    "        except Exception as ollama_err:\n",
    "            logger.error(f\"Failed to use Ollama embedding model '{model_ollama_name}'. Error: {ollama_err}\")\n",
    "            active_embedding_model = None\n",
    "    else:\n",
    "        logger.error(f\"Unsupported embedding model provider for: {emb_config_name}\")\n",
    "\n",
    "    if active_embedding_model:\n",
    "        try:\n",
    "            import tiktoken\n",
    "            tokenizer_for_counting = tiktoken.get_encoding(\"cl100k_base\")\n",
    "            logger.info(\"Using tiktoken for precise token counting in memory context.\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not initialize tiktoken: {e}. Using space-based split for token counting.\")\n",
    "    else:\n",
    "        logger.error(\"Embedding model not initialized. Semantic memory will be effectively disabled.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing embedding model components: {e}\")\n",
    "    active_embedding_model = None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 4. Load SWE-Bench-CL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_swe_bench_cl_dataset_content(file_path: str) -> Optional[Dict]: # Renamed to avoid conflict\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        logger.info(f\"Successfully loaded SWE-Bench-CL dataset from {file_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading SWE-Bench-CL dataset from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "swe_bench_cl_full_data = load_swe_bench_cl_dataset_content(SWE_BENCH_CL_DATASET_PATH)\n",
    "if not swe_bench_cl_full_data:\n",
    "    raise RuntimeError(\"SWE-Bench-CL dataset could not be loaded. Halting.\")\n",
    "\n",
    "if SEQUENCES_TO_RUN:\n",
    "    swe_bench_cl_data_sequences = [\n",
    "        seq for seq in swe_bench_cl_full_data.get(\"sequences\", []) if seq.get(\"id\") in SEQUENCES_TO_RUN\n",
    "    ]\n",
    "    if not swe_bench_cl_data_sequences:\n",
    "        raise ValueError(f\"None of the specified SEQUENCES_TO_RUN found: {SEQUENCES_TO_RUN}\")\n",
    "    logger.info(f\"Running on {len(swe_bench_cl_data_sequences)} selected sequences: {SEQUENCES_TO_RUN}\")\n",
    "else:\n",
    "    swe_bench_cl_data_sequences = swe_bench_cl_full_data.get(\"sequences\", [])\n",
    "    logger.info(f\"Running on all {len(swe_bench_cl_data_sequences)} sequences from the dataset.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 5. Semantic Memory System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticMemory:\n",
    "    \"\"\"Stores and retrieves task experiences using vector embeddings.\"\"\"\n",
    "    def __init__(self, embedding_model: Any, k_results: int = 3):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.k_results = k_results\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.doc_counter = 0\n",
    "\n",
    "    def add_entry(self, task_id: str, sequence_id: str, problem_statement: str, attempted_solution: str, harness_result: Optional[bool], metadata: Optional[Dict] = None):\n",
    "        if not self.embedding_model: return \n",
    "        meta = metadata or {}\n",
    "        meta.update({\"task_id\": task_id, \"sequence_id\": sequence_id, \"harness_result\": harness_result, \"doc_id\": self.doc_counter})\n",
    "        self.doc_counter += 1\n",
    "        \n",
    "        if harness_result is True:\n",
    "            status_prefix = \"[SUCCESSFUL SOLUTION (Verified by Harness)]\"\n",
    "        elif harness_result is False:\n",
    "            status_prefix = \"[FAILED ATTEMPT (Verified by Harness)]\"\n",
    "        else: # harness_result is None (e.g. harness error, or not run yet)\n",
    "            status_prefix = \"[ATTEMPT (Outcome Unknown/Not Verified)]\"\n",
    "            \n",
    "        full_content = (f\"{status_prefix} for Task {task_id} in Sequence {sequence_id}:\\n\"\n",
    "                        f\"Problem: {problem_statement[:500]}...\\n\"\n",
    "                        f\"Attempted Solution (Patch):\\n{attempted_solution[:1000]}...\\n\")\n",
    "        doc = Document(page_content=full_content, metadata=meta)\n",
    "        self.documents.append(doc)\n",
    "        try:\n",
    "            texts_for_faiss = [d.page_content for d in self.documents]\n",
    "            metadatas_for_faiss = [d.metadata for d in self.documents]\n",
    "            self.index = FAISS.from_texts(texts_for_faiss, self.embedding_model, metadatas=metadatas_for_faiss)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error updating FAISS index: {e}\")\n",
    "            if self.documents: self.documents.pop() # Remove the problematic document\n",
    "            self.doc_counter -=1\n",
    "\n",
    "    def retrieve_relevant(self, query: str, sequence_id_filter: Optional[str] = None, num_results: Optional[int] = None) -> List[Dict]:\n",
    "        if not self.index or not self.embedding_model: return []\n",
    "        k = num_results if num_results is not None else self.k_results\n",
    "        try:\n",
    "            candidate_k = max(k * 5 if sequence_id_filter else k, k) # Fetch more candidates if filtering by sequence\n",
    "            actual_k_for_search = min(candidate_k, len(self.documents))\n",
    "            if actual_k_for_search == 0: return []\n",
    "            \n",
    "            results_with_scores = self.index.similarity_search_with_score(query, k=actual_k_for_search)\n",
    "            \n",
    "            filtered_results = []\n",
    "            seen_task_ids = set() \n",
    "            for doc, score in results_with_scores:\n",
    "                # Filter by sequence_id if provided\n",
    "                if sequence_id_filter and doc.metadata.get(\"sequence_id\") != sequence_id_filter:\n",
    "                    continue\n",
    "                \n",
    "                task_id = doc.metadata.get(\"task_id\", \"unknown_task\")\n",
    "                # Ensure we don't include multiple memories of the same task unless explicitly allowed by k\n",
    "                # (Current logic adds most recent attempt for a task to FAISS, overwriting is not how FAISS works here, so multiple versions could exist if add_entry is called multiple times for same task_id)\n",
    "                # This logic picks the highest scoring (most similar) unique task_id.\n",
    "                if task_id in seen_task_ids: \n",
    "                    continue\n",
    "\n",
    "                filtered_results.append({\n",
    "                    \"task_id\": task_id, \n",
    "                    \"sequence_id\": doc.metadata.get(\"sequence_id\", \"unknown_seq\"),\n",
    "                    \"content\": doc.page_content, \n",
    "                    \"harness_result\": doc.metadata.get(\"harness_result\"), # Changed from success_placeholder\n",
    "                    \"score\": float(score) \n",
    "                })\n",
    "                seen_task_ids.add(task_id)\n",
    "                if len(filtered_results) >= k:\n",
    "                    break\n",
    "            \n",
    "            return sorted(filtered_results, key=lambda x: x[\"score\"]) # Sort by relevance score (lower is better for FAISS L2, higher for cosine)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during similarity search: {e}\"); return []\n",
    "\n",
    "    def clear(self):\n",
    "        self.index = None; self.documents = []; self.doc_counter = 0\n",
    "        logger.debug(\"Semantic memory cleared.\")\n",
    "\n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer: return len(self.tokenizer.encode(text))\n",
    "        return len(text.split()) # Fallback\n",
    "\n",
    "    def add_experience_to_memory(self, task_id: str, sequence_id: str, problem_statement: str, generated_patch: str, harness_result: Optional[bool]):\n",
    "        if not self.embedding_model: return\n",
    "        self.embedding_model.add_documents([Document(page_content=f\"{problem_statement}\\n{generated_patch}\", metadata={\"task_id\": task_id, \"sequence_id\": sequence_id, \"harness_result\": harness_result})])\n",
    "\n",
    "    def get_relevant_context_for_prompt(self, current_task_problem_statement: str, current_sequence_id: str, num_memories: int) -> str:\n",
    "        if not self.embedding_model: return \"\"\n",
    "        retrieved = self.retrieve_relevant(current_task_problem_statement, current_sequence_id, num_memories)\n",
    "        context_str = \"\"\n",
    "        if retrieved:\n",
    "            context_str += \"\\n\\n--- Relevant Past Experiences (from Semantic Memory) ---\\n\"\n",
    "            current_tokens = 0\n",
    "            for mem in retrieved:\n",
    "                mem_text = (f\"Past Experience (Task: {mem['task_id']}, Relevance Score: {mem['score']:.2f}):\\n\"\n",
    "                            f\"{mem['content']}\\n---\\n\")\n",
    "                mem_tokens = self._count_tokens(mem_text)\n",
    "                if current_tokens + mem_tokens <= self.embedding_model.max_tokens:\n",
    "                    context_str += mem_text; current_tokens += mem_tokens\n",
    "                else:\n",
    "                    logger.debug(f\"Memory context limit ({self.embedding_model.max_tokens} tokens) reached.\"); break\n",
    "            context_str += \"--- End of Past Experiences ---\\n\"\n",
    "        return context_str\n",
    "\n",
    "class MemorySystem:\n",
    "    \"\"\"Manages semantic memory and context building for the agent.\"\"\"\n",
    "    def __init__(self, semantic_memory: SemanticMemory, max_context_tokens: int, tokenizer: Optional[Any]):\n",
    "        self.semantic_memory = semantic_memory\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _count_tokens(self, text: str) -> int:\n",
    "        if self.tokenizer: return len(self.tokenizer.encode(text))\n",
    "        return len(text.split()) # Fallback\n",
    "\n",
    "    def add_experience_to_memory(self, task_id: str, sequence_id: str, problem_statement: str, generated_patch: str, harness_result: Optional[bool]):\n",
    "        if not self.semantic_memory.embedding_model: return\n",
    "        self.semantic_memory.add_entry(task_id, sequence_id, problem_statement, generated_patch, harness_result)\n",
    "\n",
    "    def get_relevant_context_for_prompt(self, current_task_problem_statement: str, current_sequence_id: str, num_memories: int) -> str:\n",
    "        if not self.semantic_memory.embedding_model: return \"\"\n",
    "        retrieved = self.semantic_memory.retrieve_relevant(current_task_problem_statement, current_sequence_id, num_memories)\n",
    "        context_str = \"\"\n",
    "        if retrieved:\n",
    "            context_str += \"\\n\\n--- Relevant Past Experiences (from Semantic Memory) ---\\n\"\n",
    "            current_tokens = 0\n",
    "            for mem in retrieved:\n",
    "                mem_text = (f\"Past Experience (Task: {mem['task_id']}, Relevance Score: {mem['score']:.2f}):\\n\"\n",
    "                            f\"{mem['content']}\\n---\\n\")\n",
    "                mem_tokens = self._count_tokens(mem_text)\n",
    "                if current_tokens + mem_tokens <= self.max_context_tokens:\n",
    "                    context_str += mem_text; current_tokens += mem_tokens\n",
    "                else:\n",
    "                    logger.debug(f\"Memory context limit ({self.max_context_tokens} tokens) reached.\"); break\n",
    "            context_str += \"--- End of Past Experiences ---\\n\"\n",
    "        return context_str\n",
    "\n",
    "    def clear_memory(self): self.semantic_memory.clear()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 6. Prompt Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_EXAMPLE = \"\"\"--- a/file.py\n",
    "+++ b/file.py\n",
    "@@ -1,27 +1,35 @@\n",
    " def euclidean(a, b):\n",
    "-    while b:\n",
    "-        a, b = b, a % b\n",
    "-    return a\n",
    "+    if b == 0:\n",
    "+        return a\n",
    "+    return euclidean(b, a % b)\n",
    " \n",
    " \n",
    " def bresenham(x0, y0, x1, y1):\n",
    "     points = []\n",
    "     dx = abs(x1 - x0)\n",
    "     dy = abs(y1 - y0)\n",
    "-    sx = 1 if x0 < x1 else -1\n",
    "-    sy = 1 if y0 < y1 else -1\n",
    "-    err = dx - dy\n",
    "+    x, y = x0, y0\n",
    "+    sx = -1 if x0 > x1 else 1\n",
    "+    sy = -1 if y0 > y1 else 1\n",
    " \n",
    "-    while True:\n",
    "-        points.append((x0, y0))\n",
    "-        if x0 == x1 and y0 == y1:\n",
    "-            break\n",
    "-        e2 = 2 * err\n",
    "-        if e2 > -dy:\n",
    "+    if dx > dy:\n",
    "+        err = dx / 2.0\n",
    "+        while x != x1:\n",
    "+            points.append((x, y))\n",
    "             err -= dy\n",
    "-            x0 += sx\n",
    "-        if e2 < dx:\n",
    "-            err += dx\n",
    "-            y0 += sy\n",
    "+            if err < 0:\n",
    "+                y += sy\n",
    "+                err += dx\n",
    "+            x += sx\n",
    "+    else:\n",
    "+        err = dy / 2.0\n",
    "+        while y != y1:\n",
    "+            points.append((x, y))\n",
    "+            err -= dx\n",
    "+            if err < 0:\n",
    "+                x += sx\n",
    "+                err += dy\n",
    "+            y += sy\n",
    " \n",
    "+    points.append((x, y))\n",
    "     return points\"\"\"\n",
    "\n",
    "prompt_template_str_with_memory = \"\"\"You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n",
    "The goal is to generate a code patch in the **unified diff format** that resolves the issue.\n",
    "\n",
    "<issue>\n",
    "{problem_statement}\n",
    "</issue>\n",
    "\n",
    "Relevant context from the repository ({repo} at commit {base_commit}):\n",
    "<code>\n",
    "{retrieved_context}\n",
    "\n",
    "**Hints (if any from the original issue):**\n",
    "{hints_text}\n",
    "\n",
    "**Files to consider (based on gold solution, try to identify which files to modify):**\n",
    "{text_files}\n",
    "</code>\n",
    "\n",
    "Here is an example of a patch file. It consists of changes to the codebase. It specifies the file names, the line numbers of each change, and the removed and added lines. A single patch file can contain changes to multiple files. \n",
    "\n",
    "<patch>\n",
    "{patch_example_content}\n",
    "</patch>\n",
    "\n",
    "I need you to solve the provded issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the format shown above.\n",
    "\n",
    "Respond below:\"\"\".replace(\"{patch_example_content}\", PATCH_EXAMPLE)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 7. Patch Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_generated_patch(patch_text: str) -> str:\n",
    "    \"\"\"Cleans the LLM-generated patch text.\"\"\"\n",
    "    # Regex to capture content within ``` variations\n",
    "    # It handles ```, ```diff, ```patch, etc.\n",
    "    # It uses a non-greedy match for the content between the fences.\n",
    "    # It also handles optional language specifier on the same line as ```\n",
    "    # e.g. ```python or ```patch\n",
    "    fence_match = re.search(r\"^```(?:[a-zA-Z0-9_]*\\n)?(.*?)```$\", patch_text, re.DOTALL | re.MULTILINE)\n",
    "    \n",
    "    if fence_match:\n",
    "        content_inside_fences = fence_match.group(1)\n",
    "    else:\n",
    "        # If no fences, assume the whole text is the patch.\n",
    "        content_inside_fences = patch_text\n",
    "\n",
    "    # Strip leading/trailing whitespace from the extracted (or original) content\n",
    "    # This helps if there's whitespace before the actual diff content or after it (before a closing fence)\n",
    "    current_patch_content = content_inside_fences.strip()\n",
    "\n",
    "    # If the LLM still includes \"patch\" or \"diff\" as the first line *inside* the fences (or if no fences)\n",
    "    lines = current_patch_content.split('\\n')\n",
    "    if lines and lines[0].strip().lower() in [\"patch\", \"diff\"] and \\\n",
    "       not lines[0].strip().startswith(\"---\") and \\\n",
    "       not lines[0].strip().startswith(\"diff --git\"): # Avoid stripping actual diff header lines\n",
    "        # Remove this introductory line (\"patch\" or \"diff\")\n",
    "        cleaned_patch_str = '\\n'.join(lines[1:])\n",
    "    else:\n",
    "        cleaned_patch_str = current_patch_content\n",
    "        \n",
    "    # Final strip to remove any extraneous newlines or spaces an LLM might add at the very beginning/end of the patch itself\n",
    "    cleaned_patch_str = cleaned_patch_str.strip()\n",
    "\n",
    "    # Ensure the patch ends with a single newline if it's not empty\n",
    "    if cleaned_patch_str: # Only add newline if patch is not empty\n",
    "        if not cleaned_patch_str.endswith('\\n'):\n",
    "            cleaned_patch_str += '\\n'\n",
    "    # If the patch was empty to begin with (e.g., LLM returned empty string after cleaning),\n",
    "    # it should remain an empty string, not just \"\\n\".\n",
    "    # An empty patch is valid for SWE-bench (means model predicts no change).\n",
    "            \n",
    "    return cleaned_patch_str\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## 8. SWE-bench Harness Execution and Result Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_swe_bench_harness(\n",
    "    predictions_path: str,\n",
    "    model_name: str,\n",
    "    experiment_condition_name: str, \n",
    "    run_id_suffix: str = \"\" # Unique suffix for this specific harness invocation\n",
    ") -> Optional[str]:\n",
    "    if not os.path.exists(SWE_BENCH_REPO_PATH):\n",
    "        logger.error(f\"SWE-bench repository not found at {SWE_BENCH_REPO_PATH}. Cannot run harness.\")\n",
    "        return None\n",
    "    try:\n",
    "        subprocess.run([\"docker\", \"ps\"], check=True, capture_output=True, timeout=10)\n",
    "    except Exception:\n",
    "        logger.error(\"Docker does not appear to be running/installed or responsive. Harness requires Docker.\")\n",
    "        return None\n",
    "\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    # Use the global EVALUATION_RUN_TIMESTAMP for the main log folder, then specific run_id for subfolder\n",
    "    harness_log_base_dir = os.path.join(BASE_OUTPUT_DIR, \"harness_logs\", EVALUATION_RUN_TIMESTAMP)\n",
    "    \n",
    "    # This specific invocation's ID for subfolder within the main log dir\n",
    "    if HARNESS_RUN_ID_PREFIX:\n",
    "        current_harness_invocation_id = f\"{HARNESS_RUN_ID_PREFIX}_{safe_model_name}_{experiment_condition_name}_{run_id_suffix}\"\n",
    "    else:\n",
    "        current_harness_invocation_id = f\"{safe_model_name}_{experiment_condition_name}_{run_id_suffix}\"\n",
    "    \n",
    "    log_dir_for_this_run = os.path.join(harness_log_base_dir, current_harness_invocation_id)\n",
    "    # For per-task runs, we want to ensure the parent of the specific invocation ID dir exists,\n",
    "    # but the specific invocation ID dir itself should be clean or newly created by the harness script if it handles it.\n",
    "    # If harness doesn't create current_harness_invocation_id, we should.\n",
    "    # The `report_dir` argument to the harness implies it will create subdirectories within it.\n",
    "    # So, we ensure harness_log_base_dir exists. The harness should handle creation under that.\n",
    "    # Let's adjust: log_dir_for_this_run IS the --report_dir. The harness will put its outputs (like run_logs) INSIDE this.\n",
    "    \n",
    "    # Ensure the base for all reports for this timestamp exists\n",
    "    os.makedirs(harness_log_base_dir, exist_ok=True) \n",
    "    # If the specific invocation's log dir already exists (e.g. from a previous failed attempt for this task),\n",
    "    # it might be better to clear it to ensure fresh logs for this attempt.\n",
    "    if os.path.exists(log_dir_for_this_run): \n",
    "        logger.warning(f\"Report directory {log_dir_for_this_run} already exists. Clearing it for a fresh harness run.\")\n",
    "        shutil.rmtree(log_dir_for_this_run)\n",
    "    os.makedirs(log_dir_for_this_run, exist_ok=True) # Create it clean\n",
    "\n",
    "    logger.info(f\"Running SWE-bench harness for: {model_name} ({experiment_condition_name}), Invocation ID (Task): {run_id_suffix}\")\n",
    "    logger.info(f\"Report directory for this task: {log_dir_for_this_run}\")\n",
    "\n",
    "    cmd = [\n",
    "        \"python\", \"-m\", \"swebench.harness.run_evaluation\",\n",
    "        \"--predictions_path\", os.path.abspath(predictions_path),\n",
    "        \"--dataset_name\", \"princeton-nlp/SWE-bench\", \n",
    "        \"--split\", \"test\", \n",
    "        \"--report_dir\", os.path.abspath(log_dir_for_this_run),\n",
    "        \"--timeout\", str(HARNESS_TIMEOUT_PER_TASK),\n",
    "        \"--max_workers\", str(HARNESS_MAX_WORKERS),\n",
    "        \"--run_id\", current_harness_invocation_id,\n",
    "    ]\n",
    "    if torch.backends.mps.is_available():\n",
    "        cmd.extend([\"--namespace\", \"\"])\n",
    "\n",
    "    logger.info(f\"Executing harness command: {' '.join(cmd)}\")\n",
    "    try:\n",
    "        process = subprocess.Popen(cmd, cwd=SWE_BENCH_REPO_PATH, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        # Define the target path where the copied/renamed aggregate report will be placed for parsing\n",
    "        target_results_file_for_parsing = os.path.join(log_dir_for_this_run, \"instance_results.jsonl\")\n",
    "\n",
    "        # Determine the expected filename of the aggregate JSON report generated by the harness in its CWD\n",
    "        # Harness uses: <model_slug_from_prediction_file>.<run_id_passed_to_harness>.json\n",
    "        # model_slug_from_prediction_file is model_name_or_path.replace(\"/\", \"__\")\n",
    "        # In our case, model_name_or_path is effectively the 'model_name' argument to this function.\n",
    "        harness_model_slug = model_name.replace('/', '__').replace(':', '_') \n",
    "        expected_aggregate_json_filename = f\"{harness_model_slug}.{current_harness_invocation_id}.json\"\n",
    "        source_aggregate_json_path = os.path.join(SWE_BENCH_REPO_PATH, expected_aggregate_json_filename)\n",
    "        \n",
    "        if process.returncode == 0:\n",
    "            logger.info(f\"SWE-bench harness process completed with return code 0.\")\n",
    "            if os.path.exists(source_aggregate_json_path):\n",
    "                logger.info(f\"Found harness aggregate report at: {source_aggregate_json_path}\")\n",
    "                try:\n",
    "                    shutil.copy(source_aggregate_json_path, target_results_file_for_parsing)\n",
    "                    logger.info(f\"Copied aggregate report to {target_results_file_for_parsing} for parsing.\")\n",
    "                    # Remove the original from SWE-bench CWD to avoid clutter if desired, optional\n",
    "                    # os.remove(source_aggregate_json_path) \n",
    "                    logger.info(f\"Check for detailed instance logs (e.g., run_logs/) which might be in: {log_dir_for_this_run} (if --report_dir is respected for them) or {SWE_BENCH_REPO_PATH}\")\n",
    "                    return target_results_file_for_parsing\n",
    "                except Exception as e_copy:\n",
    "                    logger.error(f\"Failed to copy aggregate report from {source_aggregate_json_path} to {target_results_file_for_parsing}: {e_copy}\")\n",
    "                    return None\n",
    "            else:\n",
    "                logger.error(f\"SWE-bench harness exited successfully (ret: 0), BUT the expected aggregate JSON report was NOT found at {source_aggregate_json_path}.\")\n",
    "                logger.error(f\"Please check harness logs. Detailed logs (run_logs/) might be in {log_dir_for_this_run} (if --report_dir worked for those) or within {SWE_BENCH_REPO_PATH}.\")\n",
    "                return None\n",
    "        else: # process.returncode != 0\n",
    "            logger.error(f\"SWE-bench harness failed with return code: {process.returncode}.\")\n",
    "            logger.error(f\"Look for specific error logs. Detailed logs (run_logs/) might be in {log_dir_for_this_run} (if --report_dir worked for those) or within {SWE_BENCH_REPO_PATH}.\")\n",
    "            logger.error(f\"Harness STDOUT:\\n{stdout}...\")\n",
    "            logger.error(f\"Harness STDERR:\\n{stderr}...\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An exception occurred while running the harness: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_harness_results(results_file_path: str) -> Dict[str, bool]:\n",
    "    if not results_file_path or not os.path.exists(results_file_path):\n",
    "        logger.error(f\"Cannot parse harness results: file not found at {results_file_path}\")\n",
    "        return {}\n",
    "    \n",
    "    instance_results = {}\n",
    "    try:\n",
    "        with open(results_file_path, 'r') as f:\n",
    "            # Load the single JSON object (copied aggregate report)\n",
    "            report_data = json.load(f)\n",
    "        \n",
    "        resolved_ids = set(report_data.get(\"resolved_ids\", []))\n",
    "        # Use \"submitted_ids\" as the list of tasks the harness was asked to process from the predictions file.\n",
    "        # Fallback to \"completed_ids\" if \"submitted_ids\" is not present or empty, though \"submitted_ids\" from your example seems correct.\n",
    "        tasks_processed_by_harness = report_data.get(\"submitted_ids\")\n",
    "        if not tasks_processed_by_harness: # Fallback\n",
    "            tasks_processed_by_harness = report_data.get(\"completed_ids\", [])\n",
    "            if tasks_processed_by_harness:\n",
    "                 logger.warning(f\"Parsing results from {results_file_path}: 'submitted_ids' was empty/missing, using 'completed_ids' as list of processed tasks.\")\n",
    "            else:\n",
    "                logger.warning(f\"Parsing results from {results_file_path}: Both 'submitted_ids' and 'completed_ids' are empty/missing in the report. No results can be parsed.\")\n",
    "                return {}\n",
    "\n",
    "\n",
    "        error_ids = set(report_data.get(\"error_ids\", []))\n",
    "        unresolved_ids = set(report_data.get(\"unresolved_ids\", [])) # Tasks that completed but didn't pass tests\n",
    "\n",
    "        for instance_id in tasks_processed_by_harness:\n",
    "            passed = instance_id in resolved_ids\n",
    "            instance_results[instance_id] = passed\n",
    "            \n",
    "            if not passed:\n",
    "                if instance_id in error_ids:\n",
    "                    logger.warning(f\"Instance {instance_id} from {results_file_path} reported as an 'error' in the harness summary (did not complete evaluation).\")\n",
    "                elif instance_id in unresolved_ids:\n",
    "                    logger.info(f\"Instance {instance_id} from {results_file_path} was completed but 'unresolved' (failed tests).\")\n",
    "                # If it's in tasks_processed_by_harness but not resolved, and not error/unresolved, it's still a fail.\n",
    "                # This covers cases where it might be in \"completed_ids\" but not explicitly in \"unresolved_ids\" or \"error_ids\"\n",
    "                # (though ideally, it should be in one of those if not resolved).\n",
    "                elif instance_id in report_data.get(\"completed_ids\", []): # Check if it at least completed\n",
    "                     logger.info(f\"Instance {instance_id} from {results_file_path} completed but was not resolved and not in error/unresolved lists. Marking as failed.\")\n",
    "                else: # Was submitted but didn't even make it to completed_ids (very unlikely if submit succeeded)\n",
    "                    logger.warning(f\"Instance {instance_id} from {results_file_path} was submitted but not found in completed, resolved, unresolved, or error lists. Marking as failed.\")\n",
    "\n",
    "\n",
    "        if not instance_results and report_data.get(\"submitted_instances\", 0) > 0 and not tasks_processed_by_harness :\n",
    "             logger.warning(f\"Report {results_file_path} indicates {report_data.get('submitted_instances')} submitted instance(s), but no instance_ids could be determined for parsing from 'submitted_ids' or 'completed_ids'. Resulting map will be empty.\")\n",
    "\n",
    "        logger.info(f\"Parsed {len(instance_results)} results from the aggregate report: {results_file_path}\")\n",
    "        return instance_results\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error decoding JSON from aggregate report {results_file_path}: {e}. File content might not be a valid single JSON object.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing aggregate harness results file {results_file_path}: {e}\")\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 8.1. Quick Test and Debugging Block\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    This block runs a single task from the first sequence with the first model to test the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DEBUG_TEST = False\n",
    "if RUN_DEBUG_TEST:\n",
    "    logger.info(\"\\n\\n\" + \"=\"*20 + \" Starting Quick Test and Debugging \" + \"=\"*20)\n",
    "    if not initialized_llms:\n",
    "        logger.error(\"No LLMs initialized. Skipping quick test.\")\n",
    "    if not swe_bench_cl_data_sequences:\n",
    "        logger.error(\"No sequences loaded. Skipping quick test.\")\n",
    "\n",
    "    test_model_name = list(initialized_llms.keys())[0]\n",
    "    test_llm_instance = initialized_llms[test_model_name]\n",
    "\n",
    "    test_sequence_data = swe_bench_cl_data_sequences[0]\n",
    "    if not test_sequence_data.get(\"tasks\"):\n",
    "        logger.error(f\"First sequence {test_sequence_data.get('id')} has no tasks. Skipping quick test.\")\n",
    "\n",
    "    # Test with memory disabled for simplicity\n",
    "    test_memory_system = None\n",
    "    test_memory_enabled = False\n",
    "\n",
    "    logger.info(f\"Quick Test: Model={test_model_name}, Sequence={test_sequence_data['id']}, First Task, Memory Disabled.\")\n",
    "\n",
    "    # --- THIS BLOCK IS HEAVILY MODIFIED FOR PER-TASK ---\n",
    "    # Get the first task\n",
    "    test_task_detail = test_sequence_data[\"tasks\"][0]\n",
    "    test_task_meta = test_task_detail.get(\"metadata\", {})\n",
    "    test_instance_id = test_task_meta.get(\"instance_id\")\n",
    "    test_base_commit = test_task_meta.get(\"base_commit\")\n",
    "    test_repo_name = test_sequence_data[\"repo\"]\n",
    "    \n",
    "    test_task_content = test_task_detail.get(\"task\", {})\n",
    "    test_problem_statement = test_task_content.get(\"problem_statement\")\n",
    "    test_hints_text = test_task_content.get(\"hints_text\", \"No hints provided.\")\n",
    "    test_gold_patch_text = test_task_detail.get(\"evaluation\", {}).get(\"patch\", \"\")\n",
    "\n",
    "    if not all([test_instance_id, test_problem_statement, test_repo_name, test_base_commit]):\n",
    "        logger.error(\"Quick Test: Critical data missing for the test task. Skipping.\")\n",
    "    else:\n",
    "        # 1. Generate patch for the single task\n",
    "        test_retrieved_context_str = \"\" # Memory disabled for this simple test\n",
    "        test_text_files_involved_str = \"Not available.\" # Simplified for test\n",
    "        if test_gold_patch_text:\n",
    "            try:\n",
    "                lines = test_gold_patch_text.split('\\n')\n",
    "                test_text_files_involved = list(set([line[len(\"--- a/\"):] for line in lines if line.startswith(\"--- a/\")] + \\\n",
    "                                           [line[len(\"+++ b/\"):] for line in lines if line.startswith(\"+++ b/\") and line[len(\"+++ b/\"):] != \"/dev/null\"]))\n",
    "                test_text_files_involved_str = \"\\n\".join(test_text_files_involved) if test_text_files_involved else \"No specific files identified.\"\n",
    "            except Exception: test_text_files_involved_str = \"Error parsing files.\"\n",
    "\n",
    "        test_input_data_for_llm = {\n",
    "            \"repo\": test_repo_name, \"base_commit\": test_base_commit, \"retrieved_context\": test_retrieved_context_str,\n",
    "            \"problem_statement\": test_problem_statement, \"hints_text\": test_hints_text or \"None\", \"text_files\": test_text_files_involved_str\n",
    "        }\n",
    "        \n",
    "        generated_patch_raw = \"\"\n",
    "        generated_patch_cleaned = \"\"\n",
    "        try:\n",
    "            formatted_prompt_content = prompt_template_str_with_memory.format(**test_input_data_for_llm)\n",
    "            messages = [HumanMessage(content=formatted_prompt_content)]\n",
    "            ai_response = test_llm_instance.invoke(messages)\n",
    "            generated_patch_raw = output_parser.invoke(ai_response)\n",
    "            generated_patch_cleaned = clean_llm_generated_patch(generated_patch_raw)\n",
    "            logger.info(f\"Quick Test: Generated patch for {test_instance_id}:\\n{generated_patch_cleaned}\")\n",
    "        except Exception as e_gen:\n",
    "            logger.error(f\"Quick Test: Error generating patch for {test_instance_id}: {e_gen}\")\n",
    "            generated_patch_cleaned = f\"Error: LLM failed. {e_gen}\"\n",
    "\n",
    "        # 2. Save this single prediction to a temporary file\n",
    "        debug_preds_dir = os.path.join(BASE_OUTPUT_DIR, \"debug_predictions\", EVALUATION_RUN_TIMESTAMP)\n",
    "        os.makedirs(debug_preds_dir, exist_ok=True)\n",
    "        # Make filename unique per task for debug to avoid issues if run multiple times\n",
    "        temp_debug_predictions_file = os.path.join(debug_preds_dir, f\"debug_pred_task_{test_instance_id.replace('/','_')}.jsonl\")\n",
    "        \n",
    "        with open(temp_debug_predictions_file, \"w\") as f:\n",
    "            swe_bench_formatted_pred = {\n",
    "                \"instance_id\": test_instance_id,\n",
    "                \"model_name_or_path\": test_model_name, # The model identifier for harness\n",
    "                \"model_patch\": generated_patch_cleaned\n",
    "            }\n",
    "            f.write(json.dumps(swe_bench_formatted_pred) + \"\\n\")\n",
    "        logger.info(f\"Quick Test: Saved single prediction for task {test_instance_id} to {temp_debug_predictions_file}\")\n",
    "\n",
    "        # 3. Run harness for this single prediction\n",
    "        # Use instance_id in run_id_suffix for unique log dir\n",
    "        debug_harness_results_file = run_swe_bench_harness(\n",
    "            temp_debug_predictions_file, \n",
    "            test_model_name, \n",
    "            \"no_memory_debug_condition\", \n",
    "            run_id_suffix=f\"debug_{test_instance_id.replace('/', '_')}\" # Unique suffix\n",
    "        )\n",
    "\n",
    "        # 4. Parse harness result\n",
    "        if debug_harness_results_file:\n",
    "            parsed_debug_results = parse_harness_results(debug_harness_results_file)\n",
    "            pass_status = parsed_debug_results.get(test_instance_id, False) # Default to False if not found\n",
    "            logger.info(f\"Quick Test: Harness result for task {test_instance_id}: {'PASSED' if pass_status else 'FAILED/ERROR'}\")\n",
    "            \n",
    "            # 5. (Optional for debug) Memory update\n",
    "            # if test_memory_system: # (memory is disabled in this test block currently)\n",
    "            #    test_memory_system.add_experience_to_memory(test_instance_id, test_sequence_data['id'], test_problem_statement, generated_patch_cleaned, pass_status)\n",
    "            #    logger.info(f\"Quick Test: (Simulated) memory update for {test_instance_id} with result: {pass_status}\")\n",
    "\n",
    "        else:\n",
    "            logger.error(f\"Quick Test: Harness run failed or results not found for task {test_instance_id}.\")\n",
    "        \n",
    "        # Clean up the temporary single prediction file\n",
    "        # if os.path.exists(temp_debug_predictions_file):\n",
    "        #     os.remove(temp_debug_predictions_file)\n",
    "\n",
    "    logger.info(\"=\"*20 + \" Quick Test Finished \" + \"=\"*20 + \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Utility Cell: Backfill Gold Patches into Existing State\n",
    "\n",
    "    This cell can be run once to update an existing `eval_state.json` file\n",
    "\n",
    "    to include the `gold_patch` for tasks that were processed before this field was added.\n",
    "\n",
    "    Make sure `SWE_BENCH_CL_DATASET_PATH` and `STATE_FILE_PATH` are correctly defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_BACKFILL_GOLD_PATCHES = True # Set to True to run this utility\n",
    "\n",
    "if RUN_BACKFILL_GOLD_PATCHES:\n",
    "    logger.info(\"Starting utility to backfill gold patches into eval_state.json...\")\n",
    "\n",
    "    # 1. Load the dataset to get gold patches\n",
    "    if not swe_bench_cl_full_data:\n",
    "        logger.error(\"SWE-Bench-CL dataset is not loaded. Cannot backfill gold patches.\")\n",
    "    else:\n",
    "        gold_patch_map = {}\n",
    "        for seq in swe_bench_cl_full_data.get(\"sequences\", []):\n",
    "            for task_detail in seq.get(\"tasks\", []):\n",
    "                instance_id = task_detail.get(\"metadata\", {}).get(\"instance_id\")\n",
    "                gold_patch = task_detail.get(\"evaluation\", {}).get(\"patch\")\n",
    "                if instance_id and gold_patch is not None: # Ensure gold_patch exists, even if empty string\n",
    "                    gold_patch_map[instance_id] = gold_patch\n",
    "        \n",
    "        logger.info(f\"Created gold patch map with {len(gold_patch_map)} entries.\")\n",
    "\n",
    "        # 2. Load the current state (or re-load to be sure)\n",
    "        # current_evaluation_state is already loaded globally, but for safety, one might reload.\n",
    "        # For this utility, we'll assume the global current_evaluation_state is what we want to modify.\n",
    "        if not current_evaluation_state or not os.path.exists(STATE_FILE_PATH):\n",
    "            logger.error(f\"Evaluation state not loaded or state file not found at {STATE_FILE_PATH}. Cannot backfill.\")\n",
    "        else:\n",
    "            updated_count = 0\n",
    "            missing_in_map_count = 0\n",
    "            \n",
    "            task_eval_progress = current_evaluation_state.get(\"task_eval_progress\", {})\n",
    "            \n",
    "            for model_name, conditions in task_eval_progress.items():\n",
    "                for condition_name, instances in conditions.items():\n",
    "                    # Typically, you'd be interested in \"memory_enabled\" and \"memory_disabled\"\n",
    "                    # if condition_name in [\"memory_enabled\", \"memory_disabled\"]: # Or iterate all\n",
    "                    for instance_id, task_data in instances.items():\n",
    "                        if isinstance(task_data, dict) and (task_data.get(\"gold_patch\") is None or task_data.get(\"gold_patch\") == \"\"): # Check if None or empty\n",
    "                            if instance_id in gold_patch_map:\n",
    "                                task_data[\"gold_patch\"] = gold_patch_map[instance_id]\n",
    "                                updated_count += 1\n",
    "                                logger.debug(f\"Added gold patch for {model_name}/{condition_name}/{instance_id}\")\n",
    "                            else:\n",
    "                                logger.warning(f\"Gold patch not found in map for {instance_id} (model: {model_name}, condition: {condition_name}). Skipping.\")\n",
    "                                missing_in_map_count +=1\n",
    "            \n",
    "            if updated_count > 0 or missing_in_map_count > 0 : # Save only if changes were made or warnings occurred\n",
    "                logger.info(f\"Backfill complete. Added/updated gold_patch for {updated_count} task entries.\")\n",
    "                if missing_in_map_count > 0:\n",
    "                    logger.warning(f\"Could not find gold patch in the dataset map for {missing_in_map_count} task entries in the state file.\")\n",
    "                \n",
    "                logger.info(\"Saving updated state...\")\n",
    "                save_state(current_evaluation_state)\n",
    "                logger.info(\"State saved successfully.\")\n",
    "            else:\n",
    "                logger.info(\"No task entries in the state file needed gold_patch backfilling.\")\n",
    "\n",
    "    logger.info(\"Gold patch backfill utility finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 9. Continual Learning Metrics Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(task_results_for_sequence: Dict[str, bool]) -> float:\n",
    "    if not task_results_for_sequence: return 0.0\n",
    "    return sum(1 for passed in task_results_for_sequence.values() if passed) / len(task_results_for_sequence)\n",
    "\n",
    "def calculate_aulc(task_results_for_sequence: Dict[str, bool], sequence_task_order: List[str]) -> float:\n",
    "    if not task_results_for_sequence or not sequence_task_order: return 0.0\n",
    "    N = len(sequence_task_order); cumulative_aulc_terms = 0\n",
    "    if N == 0: return 0.0\n",
    "    for i in range(1, N + 1):\n",
    "        current_i_tasks = sequence_task_order[:i]\n",
    "        sum_akk_for_current_i = sum(1 for task_id_k in current_i_tasks if task_results_for_sequence.get(task_id_k, False))\n",
    "        cumulative_aulc_terms += (1 / i) * sum_akk_for_current_i\n",
    "    return cumulative_aulc_terms / N\n",
    "\n",
    "def calculate_forward_transfer(task_results_mem_enabled: Dict[str, bool], task_results_no_mem: Dict[str, bool], sequence_task_order: List[str]) -> float:\n",
    "    if not task_results_mem_enabled or not task_results_no_mem or not sequence_task_order: return 0.0\n",
    "    N = len(sequence_task_order)\n",
    "    if N <= 1: return 0.0\n",
    "    ft_sum = 0\n",
    "    for i in range(N - 1):\n",
    "        task_i_plus_1_id = sequence_task_order[i+1]\n",
    "        s_mem_i_plus_1 = 1 if task_results_mem_enabled.get(task_i_plus_1_id, False) else 0\n",
    "        s_no_mem_i_plus_1 = 1 if task_results_no_mem.get(task_i_plus_1_id, False) else 0\n",
    "        ft_sum += (s_mem_i_plus_1 - s_no_mem_i_plus_1)\n",
    "    return ft_sum / (N - 1)\n",
    "\n",
    "def calculate_forgetting(task_results_initial_pass: Dict[str, bool], task_results_final_state: Dict[str, bool], sequence_task_order: List[str]) -> float:\n",
    "    if not task_results_initial_pass or not sequence_task_order: return 0.0\n",
    "    N = len(sequence_task_order)\n",
    "    if N <= 1: return 0.0\n",
    "    final_state_results_to_use = task_results_final_state if task_results_final_state else task_results_initial_pass\n",
    "    forgetting_sum = 0\n",
    "    for j_idx in range(N - 1):\n",
    "        task_j_id = sequence_task_order[j_idx]\n",
    "        max_perf_on_task_j = 1 if task_results_initial_pass.get(task_j_id, False) else 0\n",
    "        final_perf_on_task_j = 1 if final_state_results_to_use.get(task_j_id, False) else 0\n",
    "        forgetting_sum += (max_perf_on_task_j - final_perf_on_task_j)\n",
    "    return forgetting_sum / (N - 1) if (N-1) > 0 else 0.0\n",
    "\n",
    "def calculate_backward_transfer(task_results_initial_pass: Dict[str, bool], task_results_final_state: Dict[str, bool], sequence_task_order: List[str]) -> float:\n",
    "    if not task_results_initial_pass or not sequence_task_order: return 0.0\n",
    "    N = len(sequence_task_order)\n",
    "    if N <= 1: return 0.0\n",
    "    final_state_results_to_use = task_results_final_state if task_results_final_state else task_results_initial_pass\n",
    "    bwt_sum = 0\n",
    "    for i_idx in range(N - 1):\n",
    "        task_i_id = sequence_task_order[i_idx]\n",
    "        final_perf_on_task_i = 1 if final_state_results_to_use.get(task_i_id, False) else 0\n",
    "        initial_perf_on_task_i = 1 if task_results_initial_pass.get(task_i_id, False) else 0\n",
    "        bwt_sum += (final_perf_on_task_i - initial_perf_on_task_i)\n",
    "    return bwt_sum / (N - 1) if (N-1) > 0 else 0.0\n",
    "\n",
    "def calculate_cl_score(acc: float, f: float, ft: float, bwt: float, aulc: float, weights: Dict) -> float:\n",
    "    return (acc - weights[\"lambda_F\"] * f + weights[\"lambda_FT\"] * ft + weights[\"lambda_BWT\"] * bwt + weights[\"lambda_AULC\"] * aulc)\n",
    "\n",
    "def calculate_tool_use_efficiency(harness_log_dir: str) -> float:\n",
    "    logger.warning(\"Tool Use Efficiency (TUE) calculation is a placeholder.\")\n",
    "    return 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 10. Main Experiment Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load overall results list from state for appending\n",
    "overall_results_summary_list_from_state = current_evaluation_state.get(\"overall_results_list\", [])\n",
    "\n",
    "# This dictionary will map (model_name, sequence_id) to a list of already processed metric dicts\n",
    "# to avoid re-adding if script is resumed after metric calculation for some sequences.\n",
    "processed_metrics_tracker = {}\n",
    "for item in overall_results_summary_list_from_state:\n",
    "    key = (item[\"model_name\"], item[\"sequence_id\"])\n",
    "    if key not in processed_metrics_tracker:\n",
    "        processed_metrics_tracker[key] = []\n",
    "    processed_metrics_tracker[key].append(item)\n",
    "\n",
    "\n",
    "for model_name, llm_instance in tqdm(initialized_llms.items(), desc=\"Models\"):\n",
    "    logger.info(f\"\\n\\n{'='*20} Orchestrating for Model: {model_name} {'='*20}\")\n",
    "\n",
    "    # Ensure model entry exists in state dictionaries\n",
    "    current_evaluation_state.setdefault(\"task_eval_progress\", {}).setdefault(model_name, {})\n",
    "    current_evaluation_state[\"parsed_harness_results_data\"].setdefault(model_name, {})\n",
    "\n",
    "\n",
    "    # Store results for this model across conditions (loaded from state if available)\n",
    "    # These will be populated task-by-task now.\n",
    "    model_results_no_mem_from_state = current_evaluation_state[\"parsed_harness_results_data\"][model_name].get(\"memory_disabled\", {})\n",
    "    model_results_mem_enabled_from_state = current_evaluation_state[\"parsed_harness_results_data\"][model_name].get(\"memory_enabled\", {})\n",
    "    \n",
    "    for condition_name, condition_params in tqdm(EXPERIMENT_CONDITIONS.items(), desc=f\"Conditions for {model_name}\", leave=False):\n",
    "        is_memory_enabled_for_run = condition_params[\"memory_enabled\"]\n",
    "        logger.info(f\"\\n--- Running Condition: {condition_name} (Memory: {is_memory_enabled_for_run}) for Model: {model_name} ---\")\n",
    "\n",
    "        current_evaluation_state[\"task_eval_progress\"][model_name].setdefault(condition_name, {})\n",
    "        current_evaluation_state[\"parsed_harness_results_data\"][model_name].setdefault(condition_name, {})\n",
    "\n",
    "        # Initialize memory system for the current condition\n",
    "        current_memory_system = None\n",
    "        if is_memory_enabled_for_run and active_embedding_model:\n",
    "            semantic_mem_instance = SemanticMemory(active_embedding_model, EMBEDDING_MODEL_CONFIG[\"num_retrieved_memories\"])\n",
    "            current_memory_system = MemorySystem(semantic_mem_instance, EMBEDDING_MODEL_CONFIG[\"max_context_tokens_for_retrieval\"], tokenizer_for_counting)\n",
    "            current_memory_system.clear_memory() # Clear memory at the start of each condition\n",
    "            # Repopulate memory from historical successes/failures *within this condition* if resuming\n",
    "            # This requires iterating through already processed tasks *in order* for this condition\n",
    "            # and adding them to memory if they were evaluated.\n",
    "            logger.info(f\"Initializing memory for {model_name}/{condition_name}. Attempting to populate from prior tasks in this condition run.\")\n",
    "            \n",
    "            # Create a list of all tasks in their original order for this condition\n",
    "            ordered_tasks_for_memory_repopulation = []\n",
    "            for seq_data_mem_repop in swe_bench_cl_data_sequences:\n",
    "                seq_id_mem_repop = seq_data_mem_repop[\"id\"]\n",
    "                tasks_in_seq_mem_repop = seq_data_mem_repop.get(\"tasks\", [])\n",
    "                if TASKS_PER_SEQUENCE_LIMIT is not None:\n",
    "                    tasks_in_seq_mem_repop = tasks_in_seq_mem_repop[:TASKS_PER_SEQUENCE_LIMIT]\n",
    "                for task_detail_mem_repop in tasks_in_seq_mem_repop:\n",
    "                    instance_id_mem_repop = task_detail_mem_repop[\"metadata\"][\"instance_id\"]\n",
    "                    ordered_tasks_for_memory_repopulation.append({\n",
    "                        \"instance_id\": instance_id_mem_repop,\n",
    "                        \"sequence_id\": seq_id_mem_repop,\n",
    "                        \"problem_statement\": task_detail_mem_repop[\"task\"][\"problem_statement\"]\n",
    "                        # \"patch\" and \"harness_result\" will come from task_eval_progress\n",
    "                    })\n",
    "            \n",
    "            repopulated_memory_count = 0\n",
    "            for task_to_repop in ordered_tasks_for_memory_repopulation:\n",
    "                task_prog = current_evaluation_state[\"task_eval_progress\"][model_name][condition_name].get(task_to_repop[\"instance_id\"])\n",
    "                if task_prog and \"model_patch\" in task_prog and \"harness_result\" in task_prog:\n",
    "                    current_memory_system.add_experience_to_memory(\n",
    "                        task_to_repop[\"instance_id\"],\n",
    "                        task_to_repop[\"sequence_id\"],\n",
    "                        task_to_repop[\"problem_statement\"], # Need original problem statement\n",
    "                        task_prog[\"model_patch\"],\n",
    "                        task_prog[\"harness_result\"]\n",
    "                    )\n",
    "                    repopulated_memory_count += 1\n",
    "            if repopulated_memory_count > 0:\n",
    "                logger.info(f\"Repopulated memory with {repopulated_memory_count} experiences for {model_name}/{condition_name}.\")\n",
    "\n",
    "        elif is_memory_enabled_for_run and not active_embedding_model:\n",
    "            logger.warning(f\"Memory for {model_name}/{condition_name} requested, but no embedding model. Running effectively memory-disabled for RAG.\")\n",
    "\n",
    "        # PER-TASK PREDICTION, EVALUATION, AND MEMORY UPDATE LOOP\n",
    "        for sequence_data in tqdm(swe_bench_cl_data_sequences, desc=f\"Sequences for {model_name}/{condition_name}\", leave=False):\n",
    "            sequence_id = sequence_data[\"id\"]\n",
    "            repo_name = sequence_data[\"repo\"]\n",
    "            \n",
    "            current_evaluation_state[\"parsed_harness_results_data\"][model_name][condition_name].setdefault(sequence_id, {})\n",
    "\n",
    "            tasks_to_process_in_sequence = sequence_data.get(\"tasks\", [])\n",
    "            if TASKS_PER_SEQUENCE_LIMIT is not None:\n",
    "                tasks_to_process_in_sequence = tasks_to_process_in_sequence[:TASKS_PER_SEQUENCE_LIMIT]\n",
    "\n",
    "            # If memory is per-sequence (rather than per-condition), clear it here.\n",
    "            # Current setup: memory is per-condition, initialized and cleared once above.\n",
    "            # If per-sequence:\n",
    "            # if is_memory_enabled_for_run and current_memory_system:\n",
    "            #     current_memory_system.clear_memory()\n",
    "            #     logger.debug(f\"Memory cleared for new sequence {sequence_id} under {model_name}/{condition_name}\")\n",
    "\n",
    "\n",
    "            for task_idx, task_detail in tqdm(enumerate(tasks_to_process_in_sequence), desc=f\"Tasks in {sequence_id}\", total=len(tasks_to_process_in_sequence), leave=False):\n",
    "                task_meta = task_detail.get(\"metadata\", {})\n",
    "                instance_id = task_meta.get(\"instance_id\")\n",
    "\n",
    "                # --- Check if task already processed (prediction + evaluation) ---\n",
    "                task_progress_entry = current_evaluation_state[\"task_eval_progress\"][model_name][condition_name].get(instance_id)\n",
    "                if task_progress_entry: # Check if entry exists\n",
    "                    logger.info(f\"Skipping task {instance_id} in {sequence_id} ({model_name}/{condition_name}), already processed and evaluated.\")\n",
    "                    # Ensure its result is in parsed_harness_results_data if resuming after metrics\n",
    "                    current_evaluation_state[\"parsed_harness_results_data\"][model_name][condition_name][sequence_id][instance_id] = task_progress_entry.get(\"harness_result\", False)\n",
    "                    continue\n",
    "                \n",
    "                base_commit = task_meta.get(\"base_commit\")\n",
    "                task_content = task_detail.get(\"task\", {})\n",
    "                problem_statement = task_content.get(\"problem_statement\")\n",
    "                hints_text = task_content.get(\"hints_text\", \"No hints provided.\")\n",
    "                gold_patch_text = task_detail.get(\"evaluation\", {}).get(\"patch\", \"\") # For hints AND NOW FOR STORING\n",
    "\n",
    "                if not all([instance_id, problem_statement, repo_name, base_commit]):\n",
    "                    logger.warning(f\"Skipping task in {sequence_id} ({instance_id or 'Unknown ID'}) due to missing critical data.\")\n",
    "                    continue\n",
    "\n",
    "                # --- 1. Retrieve context from memory (if enabled) ---\n",
    "                retrieved_context_str = \"\"\n",
    "                if is_memory_enabled_for_run and current_memory_system:\n",
    "                    current_task_query = f\"Problem: {problem_statement}\\nHints: {hints_text}\"\n",
    "                    retrieved_context_str = current_memory_system.get_relevant_context_for_prompt(\n",
    "                        current_task_query, sequence_id, EMBEDDING_MODEL_CONFIG[\"num_retrieved_memories\"]\n",
    "                    )\n",
    "                \n",
    "                text_files_involved_str = \"Not available.\"\n",
    "                if gold_patch_text: # Use gold patch only for file hints, not for patch content\n",
    "                    try:\n",
    "                        lines = gold_patch_text.split('\\n')\n",
    "                        text_files_involved = list(set([line[len(\"--- a/\"):] for line in lines if line.startswith(\"--- a/\")] + \\\n",
    "                                                       [line[len(\"+++ b/\"):] for line in lines if line.startswith(\"+++ b/\") and line[len(\"+++ b/\"):] != \"/dev/null\"]))\n",
    "                        text_files_involved_str = \"\\n\".join(text_files_involved) if text_files_involved else \"No specific files identified.\"\n",
    "                    except Exception: text_files_involved_str = \"Error parsing files.\"\n",
    "\n",
    "                input_data_for_llm = {\n",
    "                    \"repo\": repo_name, \"base_commit\": base_commit, \"retrieved_context\": retrieved_context_str,\n",
    "                    \"problem_statement\": problem_statement, \"hints_text\": hints_text or \"None\", \"text_files\": text_files_involved_str\n",
    "                }\n",
    "                \n",
    "                # --- 2. Generate Patch ---\n",
    "                generated_patch_text_cleaned = \"\"\n",
    "                raw_llm_output_for_state = None\n",
    "                logger.debug(f\"Generating patch for Task: {instance_id} ({model_name}/{condition_name})\")\n",
    "                try:\n",
    "                    start_time_task_gen = time.time()\n",
    "                    formatted_prompt_content = prompt_template_str_with_memory.format(**input_data_for_llm)\n",
    "                    messages = [HumanMessage(content=formatted_prompt_content)]\n",
    "                    \n",
    "                    ai_response = llm_instance.invoke(messages)\n",
    "                    raw_llm_output_for_state = output_parser.invoke(ai_response) # Save raw output\n",
    "                    generated_patch_text_cleaned = clean_llm_generated_patch(raw_llm_output_for_state)\n",
    "                    \n",
    "                    duration_task_gen = time.time() - start_time_task_gen\n",
    "                    logger.debug(f\"LLM generated patch for {instance_id} in {duration_task_gen:.2f}s.\")\n",
    "                    logger.info(f\"Generated patch for {instance_id} (cleaned):\\n{generated_patch_text_cleaned[:500]}...\") # Log snippet\n",
    "                except Exception as e_gen:\n",
    "                    logger.error(f\"Error generating patch for {instance_id}: {e_gen}\")\n",
    "                    generated_patch_text_cleaned = f\"Error: LLM failed. {e_gen}\" # Store error as patch\n",
    "                    raw_llm_output_for_state = f\"Error: LLM failed. {e_gen}\"\n",
    "\n",
    "\n",
    "                # --- 3. Save Single Prediction to Temporary File ---\n",
    "                temp_preds_dir = os.path.join(BASE_OUTPUT_DIR, \"temp_predictions\", EVALUATION_RUN_TIMESTAMP, model_name.replace(\"/\",\"_\"), condition_name)\n",
    "                os.makedirs(temp_preds_dir, exist_ok=True)\n",
    "                temp_prediction_file_for_task = os.path.join(temp_preds_dir, f\"pred_task_{instance_id.replace('/','_')}.jsonl\")\n",
    "                \n",
    "                with open(temp_prediction_file_for_task, \"w\") as f_task_pred:\n",
    "                    pred_to_write = {\n",
    "                        \"instance_id\": instance_id,\n",
    "                        \"model_patch\": generated_patch_text_cleaned,\n",
    "                        \"model_name_or_path\": model_name # Harness needs this\n",
    "                    }\n",
    "                    f_task_pred.write(json.dumps(pred_to_write) + \"\\n\")\n",
    "\n",
    "                # --- 4. Run Harness for this Single Task ---\n",
    "                task_harness_result_file = None\n",
    "                task_pass_status = False # Default to False\n",
    "                if os.path.exists(temp_prediction_file_for_task):\n",
    "                    logger.info(f\"Running harness for single task {instance_id} using {temp_prediction_file_for_task}\")\n",
    "                    # Ensure run_id_suffix is unique for the task to avoid log collision\n",
    "                    task_run_id_suffix = f\"{instance_id.replace('/', '_')}\" \n",
    "                    task_harness_result_file = run_swe_bench_harness(\n",
    "                        temp_prediction_file_for_task, model_name, condition_name, task_run_id_suffix\n",
    "                    )\n",
    "                else:\n",
    "                    logger.error(f\"Temporary prediction file for task {instance_id} not found. Cannot run harness.\")\n",
    "\n",
    "                # --- 5. Parse Harness Result for this Task ---\n",
    "                if task_harness_result_file:\n",
    "                    parsed_single_task_result = parse_harness_results(task_harness_result_file)\n",
    "                    task_pass_status = parsed_single_task_result.get(instance_id, False)\n",
    "                    logger.info(f\"Harness result for task {instance_id}: {'PASSED' if task_pass_status else 'FAILED/ERROR'}\")\n",
    "                else:\n",
    "                    logger.error(f\"Harness run failed or no result file for task {instance_id}. Assuming failure for this task.\")\n",
    "                    task_pass_status = False # Explicitly set as False\n",
    "\n",
    "                # --- 6. Update Semantic Memory (if enabled) ---\n",
    "                if is_memory_enabled_for_run and current_memory_system:\n",
    "                    current_memory_system.add_experience_to_memory(\n",
    "                        instance_id, sequence_id, problem_statement, generated_patch_text_cleaned, task_pass_status\n",
    "                    )\n",
    "                    logger.debug(f\"Updated memory with experience from task {instance_id}, result: {task_pass_status}\")\n",
    "\n",
    "                # --- 7. Store Task Outcome in State ---\n",
    "                current_evaluation_state[\"task_eval_progress\"][model_name][condition_name][instance_id] = {\n",
    "                    \"model_patch\": generated_patch_text_cleaned,\n",
    "                    \"harness_result\": task_pass_status,\n",
    "                    \"raw_llm_output\": raw_llm_output_for_state, # Store the raw output\n",
    "                    \"gold_patch\": gold_patch_text, # Store the gold patch\n",
    "                    \"timestamp\": time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                }\n",
    "                current_evaluation_state[\"parsed_harness_results_data\"][model_name][condition_name].setdefault(sequence_id, {})[instance_id] = task_pass_status\n",
    "                \n",
    "                # --- 8. Save Overall State ---\n",
    "                save_state(current_evaluation_state)\n",
    "                logger.debug(f\"State saved after processing task {instance_id}\")\n",
    "\n",
    "                # --- 9. Clean up temporary prediction file for the task ---\n",
    "                if os.path.exists(temp_prediction_file_for_task):\n",
    "                    try:\n",
    "                        os.remove(temp_prediction_file_for_task)\n",
    "                    except Exception as e_remove:\n",
    "                        logger.warning(f\"Could not remove temporary prediction file {temp_prediction_file_for_task}: {e_remove}\")\n",
    "            \n",
    "            # End of tasks in a sequence\n",
    "            # If memory is per-sequence, clear it here\n",
    "            # if is_memory_enabled_for_run and current_memory_system and MEMORY_SCOPE == \"sequence\": # (Conceptual MEMORY_SCOPE)\n",
    "            #    current_memory_system.clear_memory()\n",
    "\n",
    "\n",
    "        # --- (End of all sequences for a condition) ---\n",
    "        # The old logic for writing a single predictions file for the whole condition is no longer needed.\n",
    "        # Also, the single harness run for the whole condition is replaced by per-task runs.\n",
    "        # Parsing of a single large harness result file is also replaced.\n",
    "        \n",
    "        # Update model_results_no_mem or model_results_mem_enabled for immediate use in metric calculation if needed\n",
    "        # This is now implicitly handled by populating current_evaluation_state[\"parsed_harness_results_data\"] directly\n",
    "        if is_memory_enabled_for_run:\n",
    "            model_results_mem_enabled_from_state = current_evaluation_state[\"parsed_harness_results_data\"][model_name].get(condition_name, {})\n",
    "        else:\n",
    "            model_results_no_mem_from_state = current_evaluation_state[\"parsed_harness_results_data\"][model_name].get(condition_name, {})\n",
    "\n",
    "\n",
    "    # --- 4. Calculate CL Metrics for the current model (after both conditions are processed) ---\n",
    "    logger.info(f\"\\n--- Calculating CL Metrics for Model: {model_name} ---\")\n",
    "    \n",
    "    # Use the results populated task-by-task during the run\n",
    "    model_results_no_mem = current_evaluation_state[\"parsed_harness_results_data\"][model_name].get(\"memory_disabled\", {})\n",
    "    model_results_mem_enabled = current_evaluation_state[\"parsed_harness_results_data\"][model_name].get(\"memory_enabled\", {})\n",
    "\n",
    "    if not model_results_no_mem and not model_results_mem_enabled:\n",
    "        logger.warning(f\"No harness results available for model {model_name} under any condition. Skipping metric calculation.\")\n",
    "        continue\n",
    "\n",
    "    # Determine sequence task orders once\n",
    "    sequence_task_orders_for_metrics = {}\n",
    "    for s_data in swe_bench_cl_data_sequences:\n",
    "        s_id = s_data[\"id\"]\n",
    "        s_tasks = [t[\"metadata\"][\"instance_id\"] for t in s_data.get(\"tasks\", [])]\n",
    "        if TASKS_PER_SEQUENCE_LIMIT: s_tasks = s_tasks[:TASKS_PER_SEQUENCE_LIMIT]\n",
    "        sequence_task_orders_for_metrics[s_id] = s_tasks\n",
    "\n",
    "    for seq_id in tqdm(sequence_task_orders_for_metrics.keys(), desc=f\"Sequences (Metrics) for {model_name}\", leave=False):\n",
    "        # Check if metrics for this model/sequence are already in overall_results_list_from_state\n",
    "        # This is a simple check; more robust would be to check based on a unique run ID for metrics.\n",
    "        # For now, if (model_name, seq_id) is in processed_metrics_tracker, assume it's done.\n",
    "        if (model_name, seq_id) in processed_metrics_tracker:\n",
    "            logger.info(f\"Metrics for {model_name}/{seq_id} already found in state. Skipping recalculation.\")\n",
    "            # Ensure these existing metrics are part of the final summary by keeping overall_results_summary_list_from_state as is.\n",
    "            continue \n",
    "\n",
    "        logger.info(f\"Calculating metrics for Sequence: {seq_id} of Model: {model_name}\")\n",
    "        seq_tasks_order = sequence_task_orders_for_metrics[seq_id]\n",
    "        \n",
    "        seq_res_no_mem = model_results_no_mem.get(seq_id, {})\n",
    "        seq_res_mem = model_results_mem_enabled.get(seq_id, {})\n",
    "        \n",
    "        # If one condition failed to produce results, use an empty dict for it\n",
    "        if not seq_res_no_mem and (model_results_no_mem or model_results_mem_enabled): # if model_results_no_mem is globally empty, this is fine\n",
    "            logger.warning(f\"No-memory results missing for {model_name}/{seq_id}. FT might be inaccurate.\")\n",
    "            seq_res_no_mem = {task_id: False for task_id in seq_tasks_order}\n",
    "        if not seq_res_mem and (model_results_no_mem or model_results_mem_enabled):\n",
    "            logger.warning(f\"Memory-enabled results missing for {model_name}/{seq_id}. Some metrics might be zero.\")\n",
    "            seq_res_mem = {task_id: False for task_id in seq_tasks_order}\n",
    "\n",
    "        acc_no_mem = calculate_accuracy(seq_res_no_mem)\n",
    "        aulc_no_mem = calculate_aulc(seq_res_no_mem, seq_tasks_order)\n",
    "        f_no_mem = calculate_forgetting(seq_res_no_mem, seq_res_no_mem, seq_tasks_order)\n",
    "        bwt_no_mem = calculate_backward_transfer(seq_res_no_mem, seq_res_no_mem, seq_tasks_order)\n",
    "        \n",
    "        acc_mem = calculate_accuracy(seq_res_mem)\n",
    "        aulc_mem = calculate_aulc(seq_res_mem, seq_tasks_order)\n",
    "        ft_mem = calculate_forward_transfer(seq_res_mem, seq_res_no_mem, seq_tasks_order)\n",
    "        f_mem = calculate_forgetting(seq_res_mem, seq_res_mem, seq_tasks_order)\n",
    "        bwt_mem = calculate_backward_transfer(seq_res_mem, seq_res_mem, seq_tasks_order)\n",
    "\n",
    "        cl_score_no_mem = calculate_cl_score(acc_no_mem, f_no_mem, 0, bwt_no_mem, aulc_no_mem, CL_SCORE_WEIGHTS)\n",
    "        cl_score_mem = calculate_cl_score(acc_mem, f_mem, ft_mem, bwt_mem, aulc_mem, CL_SCORE_WEIGHTS)\n",
    "        \n",
    "        tue_no_mem, tue_mem = 0.0, 0.0 # Placeholder\n",
    "\n",
    "        logger.info(f\"  {model_name}/{seq_id} - Memory Disabled: ACC={acc_no_mem:.3f}, AULC={aulc_no_mem:.3f}, F={f_no_mem:.3f}*, BWT={bwt_no_mem:.3f}*, CLS={cl_score_no_mem:.3f}\")\n",
    "        logger.info(f\"  {model_name}/{seq_id} - Memory Enabled:  ACC={acc_mem:.3f}, AULC={aulc_mem:.3f}, FT={ft_mem:.3f}, F={f_mem:.3f}*, BWT={bwt_mem:.3f}*, CLS={cl_score_mem:.3f}\")\n",
    "\n",
    "        current_metric_entry = {\n",
    "            \"model_name\": model_name, \"sequence_id\": seq_id, \"eval_timestamp\": EVALUATION_RUN_TIMESTAMP,\n",
    "            \"acc_no_mem\": acc_no_mem, \"aulc_no_mem\": aulc_no_mem, \"f_no_mem\": f_no_mem, \"bwt_no_mem\": bwt_no_mem, \"cl_score_no_mem\": cl_score_no_mem, \"tue_no_mem\": tue_no_mem,\n",
    "            \"acc_mem\": acc_mem, \"aulc_mem\": aulc_mem, \"ft_mem\": ft_mem, \"f_mem\": f_mem, \"bwt_mem\": bwt_mem, \"cl_score_mem\": cl_score_mem, \"tue_mem\": tue_mem,\n",
    "        }\n",
    "        # Add to overall_results_summary_list_from_state (which is current_evaluation_state[\"overall_results_list\"])\n",
    "        # Avoid duplicates if resuming after this point\n",
    "        found_in_state_list = False\n",
    "        for idx, item in enumerate(current_evaluation_state[\"overall_results_list\"]):\n",
    "            if item[\"model_name\"] == model_name and item[\"sequence_id\"] == seq_id and item.get(\"eval_timestamp\") == EVALUATION_RUN_TIMESTAMP:\n",
    "                current_evaluation_state[\"overall_results_list\"][idx] = current_metric_entry # Update if exists for this run\n",
    "                found_in_state_list = True\n",
    "                break\n",
    "        if not found_in_state_list:\n",
    "            current_evaluation_state[\"overall_results_list\"].append(current_metric_entry)\n",
    "        \n",
    "        save_state(current_evaluation_state) # Save state after each sequence's metrics are calculated\n",
    "\n",
    "    logger.warning(\"* F and BWT are calculated assuming no re-evaluation of tasks with final memory state. True values require a re-evaluation phase.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 11. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_evaluation_state[\"overall_results_list\"]:\n",
    "    # Use the list from the state, which has been appended to during the run\n",
    "    results_df = pd.DataFrame(current_evaluation_state[\"overall_results_list\"])\n",
    "    logger.info(\"\\n\\n======= Overall Continual Learning Metrics Summary =======\")\n",
    "    print(results_df.to_string())\n",
    "    \n",
    "    summary_file_path = os.path.join(BASE_OUTPUT_DIR, f\"cl_evaluation_summary_{EVALUATION_RUN_TIMESTAMP}.csv\")\n",
    "    results_df.to_csv(summary_file_path, index=False)\n",
    "    logger.info(f\"Overall summary saved to: {summary_file_path}\")\n",
    "    current_evaluation_state[\"overall_results_summary_df_path\"] = summary_file_path\n",
    "    save_state(current_evaluation_state) # Save final state with summary path\n",
    "else:\n",
    "    logger.info(\"No results to summarize.\")\n",
    "\n",
    "logger.info(f\"\\nEvaluation run {EVALUATION_RUN_TIMESTAMP} complete. Outputs are in {BASE_OUTPUT_DIR}\")\n",
    "logger.info(f\"State file is at: {STATE_FILE_PATH}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    --- End of Notebook ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
