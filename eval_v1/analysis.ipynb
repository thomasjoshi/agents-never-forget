{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Analysis of SWE-Bench-CL Evaluation Results\n",
    "\n",
    "\n",
    "\n",
    " This notebook analyzes the results stored in `eval_state.json` from the\n",
    "\n",
    " `eval_v1/eval_procedure.py` script. It focuses on comparing the performance\n",
    "\n",
    " of models under different conditions, primarily \"memory_enabled\" vs. \"memory_disabled\".\n",
    "\n",
    "\n",
    "\n",
    " Key analyses include:\n",
    "\n",
    " - Pass rate comparisons.\n",
    "\n",
    " - Patch similarity metrics (Levenshtein distance) between model-generated patches and gold patches.\n",
    "\n",
    " - Visualizations of these comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk # For Levenshtein distance and tokenization\n",
    "\n",
    "# Download punkt tokenizer data if not already present (needed for word_tokenize)\n",
    "# You only need to run this once.\n",
    "# try:\n",
    "#     nltk.data.find('tokenizers/punkt')\n",
    "# except nltk.downloader.DownloadError:\n",
    "#     nltk.download('punkt')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Configuration\n",
    "\n",
    "\n",
    "\n",
    " Update these paths to point to your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the evaluation state file\n",
    "# Assumes this script is in the project root, and eval_state.json is in eval_v1/eval_results/\n",
    "STATE_FILE_PATH = \"eval_v1/eval_results/eval_state.json\"\n",
    "\n",
    "# Optional: Path to the original dataset for additional metadata (e.g., sequence task order for learning curves)\n",
    "DATASET_FILE_PATH = \"data/SWE-Bench-CL-Curriculum.json\"\n",
    "LOAD_FULL_DATASET = True # Set to False if you don't need full dataset context\n",
    "\n",
    "# Models and conditions to focus on (if None, will try to process all found)\n",
    "MODELS_TO_ANALYZE = None # Example: [\"google/gemini-2.0-flash\"] or None for all\n",
    "CONDITIONS_TO_COMPARE = [\"memory_enabled\", \"memory_disabled\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(STATE_FILE_PATH):\n",
    "    logger.error(f\"State file not found at: {STATE_FILE_PATH}\")\n",
    "    raise FileNotFoundError(f\"State file not found at: {STATE_FILE_PATH}\")\n",
    "\n",
    "with open(STATE_FILE_PATH, 'r') as f:\n",
    "    eval_state = json.load(f)\n",
    "\n",
    "logger.info(f\"Successfully loaded evaluation state from {STATE_FILE_PATH}\")\n",
    "evaluation_run_timestamp = eval_state.get(\"evaluation_run_timestamp\", \"unknown_run\")\n",
    "\n",
    "swe_bench_cl_full_data = None\n",
    "if LOAD_FULL_DATASET and os.path.exists(DATASET_FILE_PATH):\n",
    "    try:\n",
    "        with open(DATASET_FILE_PATH, 'r') as f:\n",
    "            swe_bench_cl_full_data = json.load(f)\n",
    "        logger.info(f\"Successfully loaded full dataset from {DATASET_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not load full dataset from {DATASET_FILE_PATH}: {e}. Some contextual analyses might be limited.\")\n",
    "elif LOAD_FULL_DATASET:\n",
    "    logger.warning(f\"Dataset file not found at {DATASET_FILE_PATH}. Some contextual analyses might be limited.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Data Preparation\n",
    "\n",
    "\n",
    "\n",
    " Extract task evaluation progress and structure it into a Pandas DataFrame for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_task_results = []\n",
    "task_eval_progress = eval_state.get(\"task_eval_progress\", {})\n",
    "\n",
    "if not task_eval_progress:\n",
    "    logger.warning(\"No 'task_eval_progress' found in the state file. Cannot proceed with analysis.\")\n",
    "else:\n",
    "    for model_name, conditions_data in task_eval_progress.items():\n",
    "        if MODELS_TO_ANALYZE and model_name not in MODELS_TO_ANALYZE:\n",
    "            continue\n",
    "        for condition_name, instances_data in conditions_data.items():\n",
    "            if condition_name not in CONDITIONS_TO_COMPARE: # Focus on specified conditions\n",
    "                continue\n",
    "            for instance_id, task_data in instances_data.items():\n",
    "                if isinstance(task_data, dict): # Ensure task_data is a dictionary\n",
    "                    all_task_results.append({\n",
    "                        \"model_name\": model_name,\n",
    "                        \"condition\": condition_name,\n",
    "                        \"instance_id\": instance_id,\n",
    "                        \"harness_result\": task_data.get(\"harness_result\", False),\n",
    "                        \"model_patch\": task_data.get(\"model_patch\", \"\"),\n",
    "                        \"gold_patch\": task_data.get(\"gold_patch\", \"\"),\n",
    "                        \"raw_llm_output\": task_data.get(\"raw_llm_output\", \"\"),\n",
    "                        \"timestamp\": task_data.get(\"timestamp\")\n",
    "                    })\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping malformed task_data for {model_name}/{condition_name}/{instance_id}\")\n",
    "\n",
    "if not all_task_results:\n",
    "    logger.error(\"No valid task results found to analyze after filtering. Exiting.\")\n",
    "    # exit() # Or handle more gracefully\n",
    "\n",
    "results_df = pd.DataFrame(all_task_results)\n",
    "logger.info(f\"Created DataFrame with {len(results_df)} task results.\")\n",
    "if not results_df.empty:\n",
    "    logger.info(f\"DataFrame head:\\n{results_df.head()}\")\n",
    "    logger.info(f\"\\nModels found: {results_df['model_name'].unique()}\")\n",
    "    logger.info(f\"Conditions found: {results_df['condition'].unique()}\")\n",
    "else:\n",
    "    logger.error(\"Results DataFrame is empty. Please check your state file and configuration.\")\n",
    "\n",
    "\n",
    "# Add sequence information if dataset is loaded\n",
    "# This helps in per-sequence analysis and learning curves\n",
    "if swe_bench_cl_full_data and not results_df.empty:\n",
    "    instance_to_sequence_map = {}\n",
    "    instance_to_task_index_map = {}\n",
    "    sequence_task_orders = {}\n",
    "\n",
    "    for seq_idx, seq_data in enumerate(swe_bench_cl_full_data.get(\"sequences\", [])):\n",
    "        seq_id = seq_data.get(\"id\", f\"unknown_sequence_{seq_idx}\")\n",
    "        sequence_task_orders[seq_id] = []\n",
    "        for task_idx, task_detail in enumerate(seq_data.get(\"tasks\", [])):\n",
    "            instance_id = task_detail.get(\"metadata\", {}).get(\"instance_id\")\n",
    "            if instance_id:\n",
    "                instance_to_sequence_map[instance_id] = seq_id\n",
    "                instance_to_task_index_map[instance_id] = task_idx # 0-indexed task number in sequence\n",
    "                sequence_task_orders[seq_id].append(instance_id)\n",
    "\n",
    "    results_df[\"sequence_id\"] = results_df[\"instance_id\"].map(instance_to_sequence_map)\n",
    "    results_df[\"task_index_in_sequence\"] = results_df[\"instance_id\"].map(instance_to_task_index_map)\n",
    "    logger.info(\"Added sequence ID and task index to DataFrame.\")\n",
    "    logger.info(f\"DataFrame head with sequence info:\\n{results_df.head()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Calculate Patch Similarity Metrics\n",
    "\n",
    "\n",
    "\n",
    " We'll calculate Levenshtein distance at token and character levels.\n",
    "\n",
    " Patches starting with \"Error: LLM failed.\" will be assigned a very high distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein_distance(s1: str, s2: str, level=\"char\") -> int:\n",
    "    \"\"\"Calculates Levenshtein distance.\n",
    "    Level can be 'char' or 'token'.\n",
    "    Returns -1 if inputs are invalid (e.g., not strings).\n",
    "    \"\"\"\n",
    "    if not isinstance(s1, str) or not isinstance(s2, str):\n",
    "        return -1 # Or a very large number to indicate error/max distance\n",
    "\n",
    "    # Handle cases where patches indicate LLM failure\n",
    "    if s1.startswith(\"Error: LLM failed.\") or s2.startswith(\"Error: LLM failed.\"):\n",
    "        # Assign a very high distance if one is an error, scaled by typical patch length\n",
    "        # Or could return a specific sentinel value if preferred for filtering later\n",
    "        return max(len(s1), len(s2)) * 2 + 1000 # Arbitrarily large\n",
    "\n",
    "    if level == \"char\":\n",
    "        return nltk.edit_distance(s1, s2)\n",
    "    elif level == \"token\":\n",
    "        # Simple whitespace tokenization for patches\n",
    "        tokens1 = s1.split()\n",
    "        tokens2 = s2.split()\n",
    "        # Using NLTK's edit_distance which works on sequences (lists of tokens here)\n",
    "        return nltk.edit_distance(tokens1, tokens2)\n",
    "    else:\n",
    "        raise ValueError(\"Level must be 'char' or 'token'\")\n",
    "\n",
    "if not results_df.empty:\n",
    "    logger.info(\"Calculating patch similarity metrics (this may take a moment)...\")\n",
    "    # Ensure patches are strings\n",
    "    results_df[\"model_patch\"] = results_df[\"model_patch\"].astype(str)\n",
    "    results_df[\"gold_patch\"] = results_df[\"gold_patch\"].astype(str)\n",
    "\n",
    "    tqdm.pandas(desc=\"Char Levenshtein\")\n",
    "    results_df[\"char_levenshtein\"] = results_df.progress_apply(\n",
    "        lambda row: calculate_levenshtein_distance(row[\"model_patch\"], row[\"gold_patch\"], level=\"char\"), axis=1\n",
    "    )\n",
    "    tqdm.pandas(desc=\"Token Levenshtein\")\n",
    "    results_df[\"token_levenshtein\"] = results_df.progress_apply(\n",
    "        lambda row: calculate_levenshtein_distance(row[\"model_patch\"], row[\"gold_patch\"], level=\"token\"), axis=1\n",
    "    )\n",
    "    logger.info(\"Finished calculating Levenshtein distances.\")\n",
    "    logger.info(f\"DataFrame with similarity metrics:\\n{results_df[['instance_id', 'char_levenshtein', 'token_levenshtein']].head()}\")\n",
    "else:\n",
    "    logger.warning(\"Results DataFrame is empty. Skipping similarity metric calculation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Aggregate Results and Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.1. Pass Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    logger.info(\"\\n--- Pass Rate Analysis ---\")\n",
    "\n",
    "    # Overall pass rates per model and condition\n",
    "    overall_pass_rates = results_df.groupby([\"model_name\", \"condition\"])[\"harness_result\"].agg(['mean', 'sum', 'count']).rename(\n",
    "        columns={'mean': 'pass_rate', 'sum': 'passed_count', 'count': 'total_tasks'}\n",
    "    ).reset_index()\n",
    "    logger.info(f\"\\nOverall Pass Rates:\\n{overall_pass_rates.to_string()}\")\n",
    "\n",
    "    # Pass rates per sequence (if sequence_id is available)\n",
    "    if \"sequence_id\" in results_df.columns:\n",
    "        sequence_pass_rates = results_df.groupby([\"model_name\", \"condition\", \"sequence_id\"])[\"harness_result\"].agg(\n",
    "            ['mean', 'sum', 'count']\n",
    "        ).rename(\n",
    "            columns={'mean': 'pass_rate', 'sum': 'passed_count', 'count': 'total_tasks'}\n",
    "        ).reset_index()\n",
    "        logger.info(f\"\\nPass Rates per Sequence:\\n{sequence_pass_rates.to_string()}\")\n",
    "    else:\n",
    "        logger.warning(\"Sequence ID not available in DataFrame, skipping per-sequence pass rate analysis.\")\n",
    "else:\n",
    "    logger.warning(\"Results DataFrame is empty. Skipping pass rate analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.2. Patch Similarity Analysis\n",
    "\n",
    "\n",
    "\n",
    " We'll look at average similarity scores, especially for tasks that PASSED vs. FAILED.\n",
    "\n",
    " Lower Levenshtein distance is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and \"char_levenshtein\" in results_df.columns:\n",
    "    logger.info(\"\\n--- Patch Similarity Analysis (Levenshtein Distance) ---\")\n",
    "\n",
    "    # Filter out rows where distance calculation might have failed (e.g., returned -1 or the high error value)\n",
    "    # or where gold_patch was empty (can skew averages if model_patch is also empty)\n",
    "    valid_similarity_df = results_df[\n",
    "        (results_df[\"char_levenshtein\"] >= 0) & (results_df[\"char_levenshtein\"] < 1000) & # Exclude error sentinels\n",
    "        (results_df[\"token_levenshtein\"] >= 0) & (results_df[\"token_levenshtein\"] < 1000) &\n",
    "        (results_df[\"gold_patch\"].str.len() > 0) # Only consider if gold patch exists\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    if not valid_similarity_df.empty:\n",
    "        avg_similarity_scores = valid_similarity_df.groupby([\"model_name\", \"condition\", \"harness_result\"])[\n",
    "            [\"char_levenshtein\", \"token_levenshtein\"]\n",
    "        ].mean().reset_index()\n",
    "        logger.info(f\"\\nAverage Levenshtein Distances (lower is better):\\n{avg_similarity_scores.to_string()}\")\n",
    "\n",
    "        avg_similarity_overall = valid_similarity_df.groupby([\"model_name\", \"condition\"])[\n",
    "            [\"char_levenshtein\", \"token_levenshtein\"]\n",
    "        ].mean().reset_index()\n",
    "        logger.info(f\"\\nOverall Average Levenshtein Distances:\\n{avg_similarity_overall.to_string()}\")\n",
    "    else:\n",
    "        logger.warning(\"No valid similarity scores to analyze after filtering for errors or empty gold patches.\")\n",
    "else:\n",
    "    logger.warning(\"Similarity metrics not calculated or DataFrame empty. Skipping patch similarity analysis.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Visualizations\n",
    "\n",
    "\n",
    "\n",
    " Using Seaborn for aesthetically pleasing plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "FIG_OUTPUT_DIR = f\"analysis_plots_{evaluation_run_timestamp}\"\n",
    "os.makedirs(FIG_OUTPUT_DIR, exist_ok=True)\n",
    "logger.info(f\"Saving plots to {FIG_OUTPUT_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7.1. Pass Rate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and not overall_pass_rates.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=overall_pass_rates, x=\"model_name\", y=\"pass_rate\", hue=\"condition\")\n",
    "    plt.title(\"Overall Pass Rate by Model and Memory Condition\")\n",
    "    plt.ylabel(\"Pass Rate (Fraction)\")\n",
    "    plt.xlabel(\"Model Name\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_OUTPUT_DIR, \"overall_pass_rates.png\"))\n",
    "    logger.info(f\"Saved overall_pass_rates.png\")\n",
    "    plt.show()\n",
    "\n",
    "    if \"sequence_id\" in results_df.columns and not sequence_pass_rates.empty:\n",
    "        # Plot per-sequence pass rates for each model\n",
    "        for model in results_df['model_name'].unique():\n",
    "            model_seq_pass_rates = sequence_pass_rates[sequence_pass_rates['model_name'] == model]\n",
    "            if not model_seq_pass_rates.empty:\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                sns.barplot(data=model_seq_pass_rates, x=\"sequence_id\", y=\"pass_rate\", hue=\"condition\")\n",
    "                plt.title(f\"Pass Rate per Sequence for Model: {model}\\n(Comparison by Memory Condition)\")\n",
    "                plt.ylabel(\"Pass Rate (Fraction)\")\n",
    "                plt.xlabel(\"Sequence ID\")\n",
    "                plt.xticks(rotation=75, ha=\"right\")\n",
    "                plt.legend(title=\"Memory Condition\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(FIG_OUTPUT_DIR, f\"sequence_pass_rates_{model.replace('/','_')}.png\"))\n",
    "                logger.info(f\"Saved sequence_pass_rates_{model.replace('/','_')}.png\")\n",
    "                plt.show()\n",
    "else:\n",
    "    logger.warning(\"Pass rate data not available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Learning Curves (Cumulative Pass Rate)\n",
    "\n",
    " This requires task order within sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"sequence_id\" in results_df.columns and \"task_index_in_sequence\" in results_df.columns and not results_df.empty:\n",
    "    logger.info(\"Generating learning curves...\")\n",
    "    # Sort by task index to ensure correct cumulative sum\n",
    "    learning_df = results_df.sort_values(by=[\"model_name\", \"condition\", \"sequence_id\", \"task_index_in_sequence\"])\n",
    "    learning_df[\"cumulative_passed\"] = learning_df.groupby(\n",
    "        [\"model_name\", \"condition\", \"sequence_id\"]\n",
    "    )[\"harness_result\"].cumsum()\n",
    "    learning_df[\"cumulative_task_count\"] = learning_df.groupby(\n",
    "        [\"model_name\", \"condition\", \"sequence_id\"]\n",
    "    ).cumcount() + 1\n",
    "    learning_df[\"cumulative_pass_rate\"] = learning_df[\"cumulative_passed\"] / learning_df[\"cumulative_task_count\"]\n",
    "\n",
    "    unique_models = learning_df[\"model_name\"].unique()\n",
    "    unique_sequences = learning_df[\"sequence_id\"].dropna().unique()\n",
    "\n",
    "    for model in unique_models:\n",
    "        for seq_id in unique_sequences:\n",
    "            plot_data = learning_df[(learning_df[\"model_name\"] == model) & (learning_df[\"sequence_id\"] == seq_id)]\n",
    "            if not plot_data.empty:\n",
    "                plt.figure(figsize=(12, 7))\n",
    "                sns.lineplot(data=plot_data, x=\"cumulative_task_count\", y=\"cumulative_pass_rate\", hue=\"condition\", marker=\"o\")\n",
    "                plt.title(f\"Learning Curve for {model} on Sequence {seq_id}\")\n",
    "                plt.xlabel(\"Number of Tasks Seen in Sequence\")\n",
    "                plt.ylabel(\"Cumulative Pass Rate\")\n",
    "                plt.ylim(0, 1.05)\n",
    "                plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "                plt.legend(title=\"Memory Condition\")\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(FIG_OUTPUT_DIR, f\"learning_curve_{model.replace('/','_')}_{seq_id}.png\"))\n",
    "                logger.info(f\"Saved learning_curve_{model.replace('/','_')}_{seq_id}.png\")\n",
    "                plt.show()\n",
    "else:\n",
    "    logger.warning(\"Sequence or task index information not available. Skipping learning curve plots.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 7.2. Patch Similarity Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty and \"char_levenshtein\" in results_df.columns and not valid_similarity_df.empty:\n",
    "    # Character Levenshtein Distance\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(data=valid_similarity_df, x=\"model_name\", y=\"char_levenshtein\", hue=\"condition\")\n",
    "    plt.title(\"Character Levenshtein Distance (Model vs. Gold Patch)\\nLower is Better\")\n",
    "    plt.ylabel(\"Character Edit Distance\")\n",
    "    plt.xlabel(\"Model Name\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    # Consider y-axis scale if distances are very large for some\n",
    "    # plt.ylim(0, valid_similarity_df[\"char_levenshtein\"].quantile(0.95)) # Example: Cap at 95th percentile\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_OUTPUT_DIR, \"char_levenshtein_boxplot.png\"))\n",
    "    logger.info(f\"Saved char_levenshtein_boxplot.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Token Levenshtein Distance\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(data=valid_similarity_df, x=\"model_name\", y=\"token_levenshtein\", hue=\"condition\")\n",
    "    plt.title(\"Token Levenshtein Distance (Model vs. Gold Patch)\\nLower is Better\")\n",
    "    plt.ylabel(\"Token Edit Distance\")\n",
    "    plt.xlabel(\"Model Name\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    # plt.ylim(0, valid_similarity_df[\"token_levenshtein\"].quantile(0.95))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIG_OUTPUT_DIR, \"token_levenshtein_boxplot.png\"))\n",
    "    logger.info(f\"Saved token_levenshtein_boxplot.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Semantic Similarity (Placeholder) ---\n",
    "    # If you calculate semantic similarity (e.g., cosine similarity from embeddings)\n",
    "    # you would plot it here, likely with higher values being better.\n",
    "    # Example:\n",
    "    # if \"semantic_similarity\" in valid_similarity_df.columns:\n",
    "    #     plt.figure(figsize=(12, 7))\n",
    "    #     sns.boxplot(data=valid_similarity_df, x=\"model_name\", y=\"semantic_similarity\", hue=\"condition\")\n",
    "    #     plt.title(\"Semantic Similarity (Model vs. Gold Patch)\\nHigher is Better\")\n",
    "    #     plt.ylabel(\"Cosine Similarity\")\n",
    "    #     plt.xlabel(\"Model Name\")\n",
    "    #     plt.xticks(rotation=45, ha=\"right\")\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.savefig(os.path.join(FIG_OUTPUT_DIR, \"semantic_similarity_boxplot.png\"))\n",
    "    #     plt.show()\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Patch similarity data not available for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Further Analysis Ideas (Optional)\n",
    "\n",
    "\n",
    "\n",
    " - **Qualitative Analysis**: Examine specific instances where memory helped or hurt significantly. Look at the `raw_llm_output` or the patches themselves.\n",
    "\n",
    " - **Error Analysis**: Categorize types of errors made by the model patches when they fail.\n",
    "\n",
    " - **Patch Length Comparison**: Analyze if `memory_enabled` leads to longer/shorter patches and if this correlates with success or similarity.\n",
    "\n",
    " - **Statistical Significance**: Perform statistical tests (e.g., t-tests, ANOVA) to see if differences in pass rates or similarity scores are statistically significant.\n",
    "\n",
    " - **Impact of Retrieved Context**: If you stored the retrieved context in your `eval_state.json`, analyze its properties (e.g., length, relevance scores if available from memory system) and correlate with task success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Analysis script finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
