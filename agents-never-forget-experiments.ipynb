{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWE-Bench-CL Baseline Implementation\n",
    "# For Neural Networks and Deep Learning class at Columbia University\n",
    "\n",
    "!pip install transformers datasets sentence-transformers scikit-learn numpy pandas tqdm matplotlib anthropic\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import anthropic  # For Claude API access# Data Loading Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_swe_bench_cl(json_path):\n",
    "    \"\"\"Load and parse the SWE-Bench-CL dataset.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded SWE-Bench-CL v{data['metadata']['version']}\")\n",
    "    print(f\"Contains {data['metadata']['num_sequences']} sequences with {data['metadata']['total_tasks']} total tasks\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_sequence_by_repo(data, repo_name):\n",
    "    \"\"\"Get a specific sequence by repository name.\"\"\"\n",
    "    for sequence in data[\"sequences\"]:\n",
    "        if sequence[\"repository\"] == repo_name:\n",
    "            return sequence\n",
    "    return None\n",
    "\n",
    "def get_task_by_id(data, task_id):\n",
    "    \"\"\"Get a specific task by its ID.\"\"\"\n",
    "    for sequence in data[\"sequences\"]:\n",
    "        for task in sequence[\"tasks\"]:\n",
    "            if task[\"instance_id\"] == task_id:\n",
    "                return task\n",
    "    return None\n",
    "\n",
    "# Add a file upload widget\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Upload SWE-Bench-CL.json here\n",
    "\n",
    "# Load data\n",
    "data_path = next(iter(uploaded.keys()))\n",
    "data = load_swe_bench_cl(data_path)\n",
    "\n",
    "# Display available repositories\n",
    "repositories = [seq[\"repository\"] for seq in data[\"sequences\"]]\n",
    "print(f\"Available repositories: {repositories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86020406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude API Configuration\n",
    "CLAUDE_API_KEY = \"your_api_key_here\"  # Replace with your Claude API key\n",
    "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
    "\n",
    "def format_prompt(task):\n",
    "    \"\"\"Format a SWE-Bench-CL task into a Claude-compatible prompt.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    I need your help solving a software engineering task. Here are the details:\n",
    "    \n",
    "    REPOSITORY: {task.get('repository', '')}\n",
    "    PROBLEM: {task['problem_statement']}\n",
    "    \n",
    "    For context, the following files may be relevant:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add file context if available\n",
    "    for file in task.get('relevant_files', []):\n",
    "        prompt += f\"\\nFILE: {file['path']}\\nCONTENT:\\n{file['content']}\\n\"\n",
    "    \n",
    "    prompt += \"\\nPlease provide a solution to this problem. Your solution should include the code changes needed.\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def evaluate_zero_shot(task, model=\"claude-3-opus-20240229\"):\n",
    "    \"\"\"Evaluate a single task using Claude in zero-shot mode.\"\"\"\n",
    "    prompt = format_prompt(task)\n",
    "    \n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4000,\n",
    "            system=\"You are a skilled software engineer tasked with fixing bugs and implementing features in code. Provide clear, correct solutions.\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        solution = response.content[0].text\n",
    "        \n",
    "        # In a real implementation, you would validate the solution here\n",
    "        # For this example, we'll just record the response\n",
    "        \n",
    "        result = {\n",
    "            \"task_id\": task[\"instance_id\"],\n",
    "            \"success\": True,  # This would be determined by validation in practice\n",
    "            \"solution\": solution,\n",
    "            \"tokens_used\": response.usage.input_tokens + response.usage.output_tokens,\n",
    "            \"model\": model\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating task {task['instance_id']}: {str(e)}\")\n",
    "        return {\n",
    "            \"task_id\": task[\"instance_id\"],\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"model\": model\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "sample_task = get_task_by_id(data, data[\"sequences\"][0][\"tasks\"][0][\"instance_id\"])\n",
    "result = evaluate_zero_shot(sample_task)\n",
    "print(f\"Task evaluation complete: Success = {result['success']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolTracker:\n",
    "    \"\"\"Track and analyze tool usage patterns in model solutions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tool_types = [\n",
    "            \"code_search\", \"code_modification\", \"code_execution\",\n",
    "            \"debugging\", \"documentation\", \"version_control\"\n",
    "        ]\n",
    "        self.tool_usage = {tool: 0 for tool in self.tool_types}\n",
    "        self.task_tool_map = {}  # Maps task_id to tools used\n",
    "        \n",
    "    def analyze_response(self, task_id, response_text):\n",
    "        \"\"\"Analyze a model response to identify tool usage.\"\"\"\n",
    "        tools_used = set()\n",
    "        \n",
    "        # Simple keyword-based detection\n",
    "        if \"search\" in response_text.lower() or \"find\" in response_text.lower():\n",
    "            self.tool_usage[\"code_search\"] += 1\n",
    "            tools_used.add(\"code_search\")\n",
    "            \n",
    "        if \"change\" in response_text.lower() or \"modify\" in response_text.lower() or \"edit\" in response_text.lower():\n",
    "            self.tool_usage[\"code_modification\"] += 1\n",
    "            tools_used.add(\"code_modification\")\n",
    "            \n",
    "        if \"run\" in response_text.lower() or \"execute\" in response_text.lower() or \"test\" in response_text.lower():\n",
    "            self.tool_usage[\"code_execution\"] += 1\n",
    "            tools_used.add(\"code_execution\")\n",
    "            \n",
    "        if \"debug\" in response_text.lower() or \"error\" in response_text.lower():\n",
    "            self.tool_usage[\"debugging\"] += 1\n",
    "            tools_used.add(\"debugging\")\n",
    "            \n",
    "        if \"documentation\" in response_text.lower() or \"docs\" in response_text.lower():\n",
    "            self.tool_usage[\"documentation\"] += 1\n",
    "            tools_used.add(\"documentation\")\n",
    "            \n",
    "        if \"git\" in response_text.lower() or \"commit\" in response_text.lower() or \"branch\" in response_text.lower():\n",
    "            self.tool_usage[\"version_control\"] += 1\n",
    "            tools_used.add(\"version_control\")\n",
    "        \n",
    "        self.task_tool_map[task_id] = tools_used\n",
    "        return tools_used\n",
    "    \n",
    "    def visualize_usage(self):\n",
    "        \"\"\"Visualize tool usage distribution.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(self.tool_types, [self.tool_usage[tool] for tool in self.tool_types])\n",
    "        plt.title(\"Tool Usage Distribution\")\n",
    "        plt.xlabel(\"Tool Type\")\n",
    "        plt.ylabel(\"Usage Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "tool_tracker = ToolTracker()\n",
    "tools_used = tool_tracker.analyze_response(result[\"task_id\"], result[\"solution\"])\n",
    "print(f\"Tools used: {tools_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c18cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAugmenter:\n",
    "    \"\"\"Augment prompts with relevant context from previous tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"flax-sentence-embeddings/st-codesearch-t5-base\"):\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.memory = []  # Will store (task_embedding, task_content) pairs\n",
    "    \n",
    "    def add_to_memory(self, task, solution=None):\n",
    "        \"\"\"Add a task to memory with its embedding.\"\"\"\n",
    "        if solution is None and \"solution\" in task:\n",
    "            solution = task[\"solution\"]\n",
    "        \n",
    "        task_text = f\"{task['problem_statement']} {solution or ''}\"\n",
    "        embedding = self.embedding_model.encode(task_text)\n",
    "        \n",
    "        self.memory.append({\n",
    "            \"embedding\": embedding,\n",
    "            \"task_id\": task[\"instance_id\"],\n",
    "            \"problem\": task[\"problem_statement\"],\n",
    "            \"solution\": solution or \"\",\n",
    "            \"files_modified\": task.get(\"modified_files\", []),\n",
    "            \"repository\": task.get(\"repository\", \"\")\n",
    "        })\n",
    "    \n",
    "    def get_relevant_context(self, current_task, top_k=3):\n",
    "        \"\"\"Find most relevant previous tasks for context augmentation.\"\"\"\n",
    "        if not self.memory:\n",
    "            return \"\"\n",
    "            \n",
    "        current_text = current_task[\"problem_statement\"]\n",
    "        current_embedding = self.embedding_model.encode(current_text)\n",
    "        \n",
    "        # Calculate similarities with all memory items\n",
    "        similarities = []\n",
    "        for i, memory_item in enumerate(self.memory):\n",
    "            sim = cosine_similarity(\n",
    "                [current_embedding], \n",
    "                [memory_item[\"embedding\"]]\n",
    "            )[0][0]\n",
    "            similarities.append((i, sim))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Format relevant context from top_k similar tasks\n",
    "        context = \"Here are some similar tasks that have been solved before:\\n\\n\"\n",
    "        for i in range(min(top_k, len(similarities))):\n",
    "            idx, sim = similarities[i]\n",
    "            memory_item = self.memory[idx]\n",
    "            context += f\"TASK ID: {memory_item['task_id']}\\n\"\n",
    "            context += f\"SIMILARITY: {sim:.2f}\\n\"\n",
    "            context += f\"PROBLEM: {memory_item['problem']}\\n\"\n",
    "            context += f\"SOLUTION: {memory_item['solution']}\\n\\n\"\n",
    "            \n",
    "        return context\n",
    "    \n",
    "    def format_augmented_prompt(self, task, top_k=3):\n",
    "        \"\"\"Create a prompt augmented with relevant context.\"\"\"\n",
    "        relevant_context = self.get_relevant_context(task, top_k)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        I need your help solving a software engineering task. Here are the details:\n",
    "        \n",
    "        REPOSITORY: {task.get('repository', '')}\n",
    "        PROBLEM: {task['problem_statement']}\n",
    "        \n",
    "        {relevant_context}\n",
    "        \n",
    "        For context, the following files may be relevant:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add file context if available\n",
    "        for file in task.get('relevant_files', []):\n",
    "            prompt += f\"\\nFILE: {file['path']}\\nCONTENT:\\n{file['content']}\\n\"\n",
    "        \n",
    "        prompt += \"\\nPlease provide a solution to this problem. Your solution should include the code changes needed.\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "# Example usage\n",
    "augmenter = ContextAugmenter()\n",
    "augmenter.add_to_memory(sample_task, result[\"solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(data, repo_filter=None, sample_size=None):\n",
    "    \"\"\"Run comprehensive experiments on SWE-Bench-CL data.\"\"\"\n",
    "    \n",
    "    if repo_filter:\n",
    "        sequences = [seq for seq in data[\"sequences\"] if seq[\"repository\"] == repo_filter]\n",
    "    else:\n",
    "        sequences = data[\"sequences\"]\n",
    "    \n",
    "    results = {\n",
    "        \"zero_shot\": {},\n",
    "        \"augmented\": {},\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        repo = sequence[\"repository\"]\n",
    "        print(f\"\\nEvaluating repository: {repo}\")\n",
    "        \n",
    "        # Step 1: Zero-shot evaluation\n",
    "        print(\"Running zero-shot evaluation...\")\n",
    "        zero_shot_results = run_sequence_evaluation(sequence, sample_size=sample_size)\n",
    "        results[\"zero_shot\"][repo] = zero_shot_results\n",
    "        \n",
    "        # Step 2: Augmented evaluation\n",
    "        print(\"Running augmented evaluation...\")\n",
    "        augmenter = ContextAugmenter()\n",
    "        augmented_results = run_sequence_evaluation(sequence, augmenter, sample_size=sample_size)\n",
    "        results[\"augmented\"][repo] = augmented_results\n",
    "        \n",
    "        # Calculate metrics\n",
    "        zero_shot_metrics = calculate_metrics(zero_shot_results)\n",
    "        augmented_metrics = calculate_metrics(augmented_results)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = augmented_metrics[\"success_rate\"] - zero_shot_metrics[\"success_rate\"]\n",
    "        \n",
    "        results[\"metrics\"][repo] = {\n",
    "            \"zero_shot\": zero_shot_metrics,\n",
    "            \"augmented\": augmented_metrics,\n",
    "            \"improvement\": improvement\n",
    "        }\n",
    "        \n",
    "        print(f\"Zero-shot success rate: {zero_shot_metrics['success_rate']:.2f}\")\n",
    "        print(f\"Augmented success rate: {augmented_metrics['success_rate']:.2f}\")\n",
    "        print(f\"Improvement: {improvement:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Full experiment runner (commented out to avoid accidental execution)\n",
    "# experiment_results = run_experiments(data, repo_filter=\"django/django\", sample_size=5)\n",
    "\n",
    "# Save results\n",
    "def save_results(results, output_path=\"swe_bench_cl_results.json\"):\n",
    "    \"\"\"Save experiment results to a JSON file.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        # Convert NumPy arrays to lists for JSON serialization\n",
    "        import json\n",
    "        class NumpyEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "        json.dump(results, f, cls=NumpyEncoder, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Visualization functions\n",
    "def visualize_results(results):\n",
    "    \"\"\"Create visualizations from experiment results.\"\"\"\n",
    "    repos = list(results[\"metrics\"].keys())\n",
    "    zero_shot_rates = [results[\"metrics\"][repo][\"zero_shot\"][\"success_rate\"] for repo in repos]\n",
    "    augmented_rates = [results[\"metrics\"][repo][\"augmented\"][\"success_rate\"] for repo in repos]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(repos))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, zero_shot_rates, width, label='Zero-shot')\n",
    "    ax.bar(x + width/2, augmented_rates, width, label='Augmented')\n",
    "    \n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('Performance Comparison by Repository')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(repos, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
