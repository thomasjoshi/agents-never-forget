{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sentence-transformers scikit-learn numpy pandas tqdm matplotlib anthropic\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import anthropic  # For Claude API access# Data Loading Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_swe_bench_cl(json_path):\n",
    "    \"\"\"Load and parse the SWE-Bench-CL dataset.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded SWE-Bench-CL v{data['metadata']['version']}\")\n",
    "    print(f\"Contains {data['metadata']['num_sequences']} sequences with {data['metadata']['total_tasks']} total tasks\")\n",
    "    print(f\"Available repositories: {', '.join(data['metadata']['repositories'])}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_sequence_by_repo(data, repo_name):\n",
    "    \"\"\"Get a specific sequence by repository name.\"\"\"\n",
    "    for sequence in data[\"sequences\"]:\n",
    "        if sequence[\"id\"] == f\"{repo_name}_sequence\":\n",
    "            return sequence\n",
    "    return None\n",
    "\n",
    "def get_task_by_id(data, task_id):\n",
    "    \"\"\"Get a specific task by its ID.\"\"\"\n",
    "    for sequence in data[\"sequences\"]:\n",
    "        for task in sequence[\"tasks\"]:\n",
    "            if task[\"metadata\"][\"instance_id\"] == task_id:\n",
    "                return task\n",
    "    return None\n",
    "\n",
    "def calculate_metrics(sequence_results):\n",
    "    \"\"\"Calculate evaluation metrics for a sequence.\"\"\"\n",
    "    metrics = {\n",
    "        'success_rate': 0,\n",
    "        'forgetting_rate': 0,\n",
    "        'forward_transfer': 0,\n",
    "        'backward_transfer': 0,\n",
    "        'tool_use_efficiency': 0,\n",
    "        'cl_score': 0\n",
    "    }\n",
    "    \n",
    "    # Calculate success rate\n",
    "    total_tasks = len(sequence_results['tasks'])\n",
    "    successful_tasks = sum(1 for task in sequence_results['tasks'] \n",
    "                          if task.get('status') == 'success')\n",
    "    metrics['success_rate'] = successful_tasks / total_tasks\n",
    "    \n",
    "    # Calculate tool use efficiency\n",
    "    total_tool_calls = sum(len(task.get('tool_calls', [])) \n",
    "                          for task in sequence_results['tasks'])\n",
    "    successful_tool_calls = sum(len(task.get('successful_tool_calls', [])) \n",
    "                              for task in sequence_results['tasks'])\n",
    "    metrics['tool_use_efficiency'] = successful_tool_calls / (total_tool_calls or 1)\n",
    "    \n",
    "    # Calculate CL score\n",
    "    metrics['cl_score'] = (metrics['success_rate'] * \n",
    "                         (1 - metrics['forgetting_rate']) * \n",
    "                         (1 + 0.5 * metrics['forward_transfer'] + \n",
    "                          0.5 * metrics['backward_transfer']) * \n",
    "                         metrics['tool_use_efficiency'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate_sequence(sequence, model, memory_system=None):\n",
    "    \"\"\"Evaluate a sequence of tasks.\"\"\"\n",
    "    sequence_results = {\n",
    "        'repository': sequence['repo'],\n",
    "        'tasks': [],\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    for task in sequence['tasks']:\n",
    "        # Get task details\n",
    "        task_id = task['metadata']['instance_id']\n",
    "        problem_statement = task['task']['problem_statement']\n",
    "        \n",
    "        # Use memory system if available\n",
    "        if memory_system:\n",
    "            relevant_memory = memory_system.retrieve_relevant_memory(task)\n",
    "        \n",
    "        # Generate solution using model\n",
    "        solution = model.generate_solution(problem_statement)\n",
    "        \n",
    "        # Evaluate solution\n",
    "        success = evaluate_solution(solution, task['evaluation']['patch'])\n",
    "        \n",
    "        # Store results\n",
    "        task_result = {\n",
    "            'id': task_id,\n",
    "            'status': 'success' if success else 'failure',\n",
    "            'solution': solution,\n",
    "            'tool_calls': extract_tool_calls(solution),\n",
    "            'successful_tool_calls': extract_successful_tool_calls(solution)\n",
    "        }\n",
    "        \n",
    "        sequence_results['tasks'].append(task_result)\n",
    "        \n",
    "        # Update memory if available\n",
    "        if memory_system:\n",
    "            memory_system.update_memory(task, solution)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    sequence_results['metrics'] = calculate_metrics(sequence_results)\n",
    "    \n",
    "    return sequence_results\n",
    "\n",
    "# Add Google Drive mounting code\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load data from Google Drive\n",
    "data_path = '/content/drive/MyDrive/SWE-Bench-CL.json'\n",
    "data = load_swe_bench_cl(data_path)\n",
    "\n",
    "# Display repository statistics\n",
    "repositories = data['metadata']['repositories']\n",
    "print(f\"Available repositories: {repositories}\")\n",
    "print(f\"Generation date: {data['metadata']['generation_date']}\")\n",
    "print(f\"Total tasks: {data['metadata']['total_tasks']}\")\n",
    "print(f\"Number of sequences: {data['metadata']['num_sequences']}\")\n",
    "\n",
    "# Example: Get first sequence\n",
    "first_sequence = data['sequences'][0]\n",
    "print(f\"\\\\nFirst sequence details:\")\n",
    "print(f\"Repository: {first_sequence['repo']}\")\n",
    "print(f\"Number of tasks: {first_sequence['num_tasks']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86020406",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch accelerate bitsandbytes sentencepiece\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load CodeLlama model\n",
    "print(\"Loading CodeLlama model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"codellama/CodeLlama-7b-hf\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=True  # Quantization for memory efficiency\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "def format_prompt(task):\n",
    "    \"\"\"Format a SWE-Bench-CL task into a CodeLlama-compatible prompt.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a skilled software engineer tasked with fixing bugs and implementing features in code. \n",
    "    Please provide clear, correct solutions.\n",
    "    \n",
    "    Task Details:\n",
    "    REPOSITORY: {task['metadata']['repo']}\n",
    "    PROBLEM: {task['task']['problem_statement']}\n",
    "    \n",
    "    Please provide a solution to this problem. Your solution should include the code changes needed.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "def evaluate_zero_shot(task):\n",
    "    \"\"\"Evaluate a single task using CodeLlama in zero-shot mode.\"\"\"\n",
    "    prompt = format_prompt(task)\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=4096,\n",
    "                temperature=0.2,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        solution = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        result = {\n",
    "            \"task_id\": task[\"metadata\"][\"instance_id\"],\n",
    "            \"success\": True,  # This would be determined by validation in practice\n",
    "            \"solution\": solution,\n",
    "            \"model\": \"CodeLlama-7b\"\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating task {task['metadata']['instance_id']}: {str(e)}\")\n",
    "        return {\n",
    "            \"task_id\": task[\"metadata\"][\"instance_id\"],\n",
    "            \"success\": False,\n",
    "            \"error\": str(e),\n",
    "            \"model\": \"CodeLlama-7b\"\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "sample_task = get_task_by_id(data, data[\"sequences\"][0][\"tasks\"][0][\"metadata\"][\"instance_id\"])\n",
    "result = evaluate_zero_shot(sample_task)\n",
    "print(f\"Task evaluation complete: Success = {result['success']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolTracker:\n",
    "    \"\"\"Track and analyze tool usage patterns in model solutions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tool_types = [\n",
    "            \"code_search\", \"code_modification\", \"code_execution\",\n",
    "            \"debugging\", \"documentation\", \"version_control\"\n",
    "        ]\n",
    "        self.tool_usage = {tool: 0 for tool in self.tool_types}\n",
    "        self.task_tool_map = {}  # Maps task_id to tools used\n",
    "        \n",
    "    def analyze_response(self, task_id, response_text):\n",
    "        \"\"\"Analyze a model response to identify tool usage.\"\"\"\n",
    "        tools_used = set()\n",
    "        \n",
    "        # Simple keyword-based detection\n",
    "        if \"search\" in response_text.lower() or \"find\" in response_text.lower():\n",
    "            self.tool_usage[\"code_search\"] += 1\n",
    "            tools_used.add(\"code_search\")\n",
    "            \n",
    "        if \"change\" in response_text.lower() or \"modify\" in response_text.lower() or \"edit\" in response_text.lower():\n",
    "            self.tool_usage[\"code_modification\"] += 1\n",
    "            tools_used.add(\"code_modification\")\n",
    "            \n",
    "        if \"run\" in response_text.lower() or \"execute\" in response_text.lower() or \"test\" in response_text.lower():\n",
    "            self.tool_usage[\"code_execution\"] += 1\n",
    "            tools_used.add(\"code_execution\")\n",
    "            \n",
    "        if \"debug\" in response_text.lower() or \"error\" in response_text.lower():\n",
    "            self.tool_usage[\"debugging\"] += 1\n",
    "            tools_used.add(\"debugging\")\n",
    "            \n",
    "        if \"documentation\" in response_text.lower() or \"docs\" in response_text.lower():\n",
    "            self.tool_usage[\"documentation\"] += 1\n",
    "            tools_used.add(\"documentation\")\n",
    "            \n",
    "        if \"git\" in response_text.lower() or \"commit\" in response_text.lower() or \"branch\" in response_text.lower():\n",
    "            self.tool_usage[\"version_control\"] += 1\n",
    "            tools_used.add(\"version_control\")\n",
    "        \n",
    "        self.task_tool_map[task_id] = tools_used\n",
    "        return tools_used\n",
    "    \n",
    "    def visualize_usage(self):\n",
    "        \"\"\"Visualize tool usage distribution.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(self.tool_types, [self.tool_usage[tool] for tool in self.tool_types])\n",
    "        plt.title(\"Tool Usage Distribution\")\n",
    "        plt.xlabel(\"Tool Type\")\n",
    "        plt.ylabel(\"Usage Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "tool_tracker = ToolTracker()\n",
    "tools_used = tool_tracker.analyze_response(result[\"task_id\"], result[\"solution\"])\n",
    "print(f\"Tools used: {tools_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c18cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAugmenter:\n",
    "    \"\"\"Augment prompts with relevant context from previous tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"flax-sentence-embeddings/st-codesearch-t5-base\"):\n",
    "        self.embedding_model = SentenceTransformer(model_name)\n",
    "        self.memory = []  # Will store (task_embedding, task_content) pairs\n",
    "    \n",
    "    def add_to_memory(self, task, solution=None):\n",
    "        \"\"\"Add a task to memory with its embedding.\"\"\"\n",
    "        if solution is None and \"solution\" in task:\n",
    "            solution = task[\"solution\"]\n",
    "        \n",
    "        task_text = f\"{task['problem_statement']} {solution or ''}\"\n",
    "        embedding = self.embedding_model.encode(task_text)\n",
    "        \n",
    "        self.memory.append({\n",
    "            \"embedding\": embedding,\n",
    "            \"task_id\": task[\"instance_id\"],\n",
    "            \"problem\": task[\"problem_statement\"],\n",
    "            \"solution\": solution or \"\",\n",
    "            \"files_modified\": task.get(\"modified_files\", []),\n",
    "            \"repository\": task.get(\"repository\", \"\")\n",
    "        })\n",
    "    \n",
    "    def get_relevant_context(self, current_task, top_k=3):\n",
    "        \"\"\"Find most relevant previous tasks for context augmentation.\"\"\"\n",
    "        if not self.memory:\n",
    "            return \"\"\n",
    "            \n",
    "        current_text = current_task[\"problem_statement\"]\n",
    "        current_embedding = self.embedding_model.encode(current_text)\n",
    "        \n",
    "        # Calculate similarities with all memory items\n",
    "        similarities = []\n",
    "        for i, memory_item in enumerate(self.memory):\n",
    "            sim = cosine_similarity(\n",
    "                [current_embedding], \n",
    "                [memory_item[\"embedding\"]]\n",
    "            )[0][0]\n",
    "            similarities.append((i, sim))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Format relevant context from top_k similar tasks\n",
    "        context = \"Here are some similar tasks that have been solved before:\\n\\n\"\n",
    "        for i in range(min(top_k, len(similarities))):\n",
    "            idx, sim = similarities[i]\n",
    "            memory_item = self.memory[idx]\n",
    "            context += f\"TASK ID: {memory_item['task_id']}\\n\"\n",
    "            context += f\"SIMILARITY: {sim:.2f}\\n\"\n",
    "            context += f\"PROBLEM: {memory_item['problem']}\\n\"\n",
    "            context += f\"SOLUTION: {memory_item['solution']}\\n\\n\"\n",
    "            \n",
    "        return context\n",
    "    \n",
    "    def format_augmented_prompt(self, task, top_k=3):\n",
    "        \"\"\"Create a prompt augmented with relevant context.\"\"\"\n",
    "        relevant_context = self.get_relevant_context(task, top_k)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        I need your help solving a software engineering task. Here are the details:\n",
    "        \n",
    "        REPOSITORY: {task.get('repository', '')}\n",
    "        PROBLEM: {task['problem_statement']}\n",
    "        \n",
    "        {relevant_context}\n",
    "        \n",
    "        For context, the following files may be relevant:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add file context if available\n",
    "        for file in task.get('relevant_files', []):\n",
    "            prompt += f\"\\nFILE: {file['path']}\\nCONTENT:\\n{file['content']}\\n\"\n",
    "        \n",
    "        prompt += \"\\nPlease provide a solution to this problem. Your solution should include the code changes needed.\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "# Example usage\n",
    "augmenter = ContextAugmenter()\n",
    "augmenter.add_to_memory(sample_task, result[\"solution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(data, repo_filter=None, sample_size=None):\n",
    "    \"\"\"Run comprehensive experiments on SWE-Bench-CL data.\"\"\"\n",
    "    \n",
    "    if repo_filter:\n",
    "        sequences = [seq for seq in data[\"sequences\"] if seq[\"repository\"] == repo_filter]\n",
    "    else:\n",
    "        sequences = data[\"sequences\"]\n",
    "    \n",
    "    results = {\n",
    "        \"zero_shot\": {},\n",
    "        \"augmented\": {},\n",
    "        \"metrics\": {}\n",
    "    }\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        repo = sequence[\"repository\"]\n",
    "        print(f\"\\nEvaluating repository: {repo}\")\n",
    "        \n",
    "        # Step 1: Zero-shot evaluation\n",
    "        print(\"Running zero-shot evaluation...\")\n",
    "        zero_shot_results = run_sequence_evaluation(sequence, sample_size=sample_size)\n",
    "        results[\"zero_shot\"][repo] = zero_shot_results\n",
    "        \n",
    "        # Step 2: Augmented evaluation\n",
    "        print(\"Running augmented evaluation...\")\n",
    "        augmenter = ContextAugmenter()\n",
    "        augmented_results = run_sequence_evaluation(sequence, augmenter, sample_size=sample_size)\n",
    "        results[\"augmented\"][repo] = augmented_results\n",
    "        \n",
    "        # Calculate metrics\n",
    "        zero_shot_metrics = calculate_metrics(zero_shot_results)\n",
    "        augmented_metrics = calculate_metrics(augmented_results)\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = augmented_metrics[\"success_rate\"] - zero_shot_metrics[\"success_rate\"]\n",
    "        \n",
    "        results[\"metrics\"][repo] = {\n",
    "            \"zero_shot\": zero_shot_metrics,\n",
    "            \"augmented\": augmented_metrics,\n",
    "            \"improvement\": improvement\n",
    "        }\n",
    "        \n",
    "        print(f\"Zero-shot success rate: {zero_shot_metrics['success_rate']:.2f}\")\n",
    "        print(f\"Augmented success rate: {augmented_metrics['success_rate']:.2f}\")\n",
    "        print(f\"Improvement: {improvement:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Full experiment runner (commented out to avoid accidental execution)\n",
    "# experiment_results = run_experiments(data, repo_filter=\"django/django\", sample_size=5)\n",
    "\n",
    "# Save results\n",
    "def save_results(results, output_path=\"swe_bench_cl_results.json\"):\n",
    "    \"\"\"Save experiment results to a JSON file.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        # Convert NumPy arrays to lists for JSON serialization\n",
    "        import json\n",
    "        class NumpyEncoder(json.JSONEncoder):\n",
    "            def default(self, obj):\n",
    "                if isinstance(obj, np.ndarray):\n",
    "                    return obj.tolist()\n",
    "                return json.JSONEncoder.default(self, obj)\n",
    "        \n",
    "        json.dump(results, f, cls=NumpyEncoder, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Visualization functions\n",
    "def visualize_results(results):\n",
    "    \"\"\"Create visualizations from experiment results.\"\"\"\n",
    "    repos = list(results[\"metrics\"].keys())\n",
    "    zero_shot_rates = [results[\"metrics\"][repo][\"zero_shot\"][\"success_rate\"] for repo in repos]\n",
    "    augmented_rates = [results[\"metrics\"][repo][\"augmented\"][\"success_rate\"] for repo in repos]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    x = np.arange(len(repos))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, zero_shot_rates, width, label='Zero-shot')\n",
    "    ax.bar(x + width/2, augmented_rates, width, label='Augmented')\n",
    "    \n",
    "    ax.set_ylabel('Success Rate')\n",
    "    ax.set_title('Performance Comparison by Repository')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(repos, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
