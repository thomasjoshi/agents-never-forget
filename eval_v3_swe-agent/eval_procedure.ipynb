{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # SWE-Agent-CL Evaluation Framework (LangGraph + Semantic Memory)\n",
    "\n",
    "\n",
    "\n",
    " This notebook presents a comprehensive framework for evaluating large language models (LLMs) on SWE-Bench-CL, our continual learning adaptation of SWE-Bench. This framework integrates concepts from the **SWE-agent** project ([Yang et al., 2024](https://arxiv.org/abs/2405.15793)) with a novel **semantic memory** system to assess continual learning capabilities in software engineering tasks.\n",
    "\n",
    "\n",
    "\n",
    " **Key Features:**\n",
    "\n",
    "\n",
    "\n",
    " 1.  **Multi-Model Evaluation:** Supports various closed and open-source models (e.g., `Claude-3.7-Sonnet`, `GPT-4o`, `gemma3`, `llama4`, `Qwen`) via a flexible `get_llm` function.\n",
    "\n",
    " 2.  **SWE-agent Inspired ACI:** Implements an Agent-Computer Interface (ACI) based on SWE-agent principles using LangGraph. This includes:\n",
    "\n",
    "     *   LM-friendly tools for file navigation (`open`, `scroll_up`, `scroll_down`, `goto`), searching (`find_file`, `search_file`, `search_dir`), editing (`edit` with integrated linter), and execution (`run_tests`).\n",
    "\n",
    "     *   Concise, informative feedback mechanisms.\n",
    "\n",
    "     *   Prompts adapted from SWE-agent's design (System, Instance, Error handling).\n",
    "\n",
    " 3.  **Sophisticated Semantic Memory:** A novel memory system combining:\n",
    "\n",
    "     *   **Semantic Memory (RAG):** Stores and retrieves past task experiences (problem, solution, success status, rationale) using vector embeddings (FAISS). This allows the agent to learn from past successes and failures within a sequence.\n",
    "\n",
    "     *   **Context Management:** Integrates retrieved memories into the agent's prompt context.\n",
    "\n",
    " 4.  **Continual Learning Metrics:** Defines and enables the calculation of metrics crucial for CL evaluation (Success Rate, Tool Use Efficiency, Forward Transfer potential).\n",
    "\n",
    " 5.  **Experimental Design:** Facilitates experiments comparing performance with and without memory (0-shot vs. memory-augmented) across sequences.\n",
    "\n",
    " 6.  **Self-Contained & Runnable:** Uses standard Python libraries (`os`, `subprocess`, `pathlib`) and LangChain/LangGraph for a runnable evaluation setup.\n",
    "\n",
    "\n",
    "\n",
    " This framework aims to evaluate how effectively LLM agents can leverage past experiences (semantic memory) within a structured, LM-friendly environment (ACI) to solve evolving software engineering problems, mimicking a developer's learning process on a project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m19:34:16 - LiteLLM:WARNING\u001b[0m: utils.py:513 - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "2025-05-13 19:34:16,567 - WARNING - `litellm.set_verbose` is deprecated. Please set `os.environ['LITELLM_LOG'] = 'DEBUG'` for debug logs.\n",
      "\u001b[92m19:34:16 - LiteLLM:INFO\u001b[0m: utils.py:2869 - \n",
      "LiteLLM completion() model= gemma3:1b; provider = ollama\n",
      "2025-05-13 19:34:16,605 - INFO - \n",
      "LiteLLM completion() model= gemma3:1b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "Final returned optional params: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 19:34:17,281 - INFO - HTTP Request: POST http://localhost:11434/api/generate \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "JSONSchemaValidationError",
     "evalue": "litellm.JSONSchemaValidationError: model=, returned an invalid response=*   **Event:** Science Fair\n*   **Attendees:** Alice and Bob\n*   **Date:** Friday, for schema=*   **Event:** Science Fair\n*   **Attendees:** Alice and Bob\n*   **Date:** Friday.\nAccess raw response with `e.raw_response`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/litellm/litellm_core_utils/json_validation_rule.py:17\u001b[39m, in \u001b[36mvalidate_schema\u001b[39m\u001b[34m(schema, response)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     response_dict = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/json/__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/json/decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/json/decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONSchemaValidationError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[121]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m   date: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m     16\u001b[39m   participants: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m resp = \u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mollama/gemma3:1b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCalendarEvent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReceived=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(resp))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/litellm/utils.py:1279\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1276\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1277\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1278\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1279\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/litellm/utils.py:1195\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   1194\u001b[39m \u001b[38;5;66;03m### POST-CALL RULES ###\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m \u001b[43mpost_call_processing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[38;5;66;03m# [OPTIONAL] ADD TO CACHE\u001b[39;00m\n\u001b[32m   1202\u001b[39m _llm_caching_handler.sync_set_cache(\n\u001b[32m   1203\u001b[39m     result=result,\n\u001b[32m   1204\u001b[39m     args=args,\n\u001b[32m   1205\u001b[39m     kwargs=kwargs,\n\u001b[32m   1206\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/litellm/utils.py:995\u001b[39m, in \u001b[36mclient.<locals>.post_call_processing\u001b[39m\u001b[34m(original_response, model, optional_params)\u001b[39m\n\u001b[32m    987\u001b[39m                             litellm.litellm_core_utils.json_validation_rule.validate_schema(\n\u001b[32m    988\u001b[39m                                 schema=optional_params[\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m][\n\u001b[32m    989\u001b[39m                                     \u001b[33m\"\u001b[39m\u001b[33mresponse_schema\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    990\u001b[39m                                 ],\n\u001b[32m    991\u001b[39m                                 response=model_response,\n\u001b[32m    992\u001b[39m                             )\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/litellm/utils.py:954\u001b[39m, in \u001b[36mclient.<locals>.post_call_processing\u001b[39m\u001b[34m(original_response, model, optional_params)\u001b[39m\n\u001b[32m    946\u001b[39m             json_response_format = (\n\u001b[32m    947\u001b[39m                 type_to_response_format_param(\n\u001b[32m    948\u001b[39m                     response_format=optional_params[\n\u001b[32m   (...)\u001b[39m\u001b[32m    951\u001b[39m                 )\n\u001b[32m    952\u001b[39m             )\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m json_response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m             \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlitellm_core_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson_validation_rule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_response_format\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_schema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mschema\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    961\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/litellm/litellm_core_utils/json_validation_rule.py:19\u001b[39m, in \u001b[36mvalidate_schema\u001b[39m\u001b[34m(schema, response)\u001b[39m\n\u001b[32m     17\u001b[39m     response_dict = json.loads(response)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json.JSONDecodeError:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONSchemaValidationError(\n\u001b[32m     20\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, raw_response=response, schema=response\n\u001b[32m     21\u001b[39m     )\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     24\u001b[39m     validate(response_dict, schema=schema)\n",
      "\u001b[31mJSONSchemaValidationError\u001b[39m: litellm.JSONSchemaValidationError: model=, returned an invalid response=*   **Event:** Science Fair\n*   **Attendees:** Alice and Bob\n*   **Date:** Friday, for schema=*   **Event:** Science Fair\n*   **Attendees:** Alice and Bob\n*   **Date:** Friday.\nAccess raw response with `e.raw_response`"
     ]
    }
   ],
   "source": [
    "import litellm, os\n",
    "from litellm import completion \n",
    "from pydantic import BaseModel \n",
    "\n",
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ]\n",
    "\n",
    "litellm.enable_json_schema_validation = True\n",
    "litellm.set_verbose = True # see the raw request made by litellm\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "  name: str\n",
    "  date: str\n",
    "  participants: list[str]\n",
    "\n",
    "resp = completion(\n",
    "    model=\"ollama/gemma3:1b\",\n",
    "    messages=messages,\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "print(\"Received={}\".format(resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Load and Configure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:37,828 - INFO - Loaded SWE-Bench-CL dataset with 8 sequences and 273 tasks from ../data/SWE-Bench-CL-Curriculum.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repositories in dataset:\n",
      "- django/django\n",
      "- sympy/sympy\n",
      "- sphinx-doc/sphinx\n",
      "- matplotlib/matplotlib\n",
      "- scikit-learn/scikit-learn\n",
      "- astropy/astropy\n",
      "- pydata/xarray\n",
      "- pytest-dev/pytest\n",
      "\n",
      "First sequence: django_django_sequence\n",
      "Repository: django/django\n",
      "Number of tasks: 50\n",
      "Difficulty distribution: {'1': 50}\n",
      "Tasks with dependencies: 25 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# Load the SWE-Bench-CL dataset\n",
    "dataset_path = \"../data/SWE-Bench-CL-Curriculum.json\" # Adjust path as needed\n",
    "\n",
    "try:\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        swe_bench_cl = json.load(f)\n",
    "    logger.info(f\"Loaded SWE-Bench-CL dataset with {swe_bench_cl['metadata']['num_sequences']} sequences and {swe_bench_cl['metadata']['total_tasks']} tasks from {dataset_path}\")\n",
    "\n",
    "    # Display dataset metadata\n",
    "    print(\"\\nRepositories in dataset:\")\n",
    "    for repo in swe_bench_cl['metadata']['repositories']:\n",
    "        print(f\"- {repo}\")\n",
    "\n",
    "    # Examine the first sequence\n",
    "    first_sequence = swe_bench_cl['sequences'][0]\n",
    "    print(f\"\\nFirst sequence: {first_sequence['id']}\")\n",
    "    print(f\"Repository: {first_sequence.get('repo', 'N/A')}\") # Use .get for safety\n",
    "    print(f\"Number of tasks: {first_sequence['num_tasks']}\")\n",
    "    # Add checks for keys before accessing\n",
    "    if 'statistics' in first_sequence:\n",
    "        print(f\"Difficulty distribution: {first_sequence['statistics'].get('difficulty_distribution', 'N/A')}\")\n",
    "        print(f\"Tasks with dependencies: {first_sequence['statistics'].get('tasks_with_dependencies', 'N/A')} ({first_sequence['statistics'].get('dependency_rate', 'N/A')}%)\")\n",
    "    else:\n",
    "        print(\"Statistics not available for the first sequence.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Dataset file not found at {dataset_path}. Please ensure the file exists or run the dummy data generation cell.\")\n",
    "    swe_bench_cl = None # Ensure variable is None if loading fails\n",
    "except json.JSONDecodeError:\n",
    "    logger.error(f\"Error decoding JSON from {dataset_path}. The file might be corrupted.\")\n",
    "    swe_bench_cl = None\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred while loading the dataset: {e}\")\n",
    "    swe_bench_cl = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.5 Repository Management Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:37,839 - INFO - Repositories will be cloned/managed in: /Users/Shayan/Library/CloudStorage/GoogleDrive-sc4040@columbia.edu/My Drive/Academics/Spring 2025/COMS 4995 - Neural Nets & Deep Learning/NNDL Final Project/agents-never-forget/agent_v2/cloned_repos\n"
     ]
    }
   ],
   "source": [
    "# Repository Configuration\n",
    "REPOS_BASE_DIR = Path(\"./cloned_repos\") # Use a distinct directory\n",
    "REPOS_BASE_DIR.mkdir(exist_ok=True)\n",
    "logger.info(f\"Repositories will be cloned/managed in: {REPOS_BASE_DIR.resolve()}\")\n",
    "\n",
    "def setup_repository(\n",
    "    repo_identifier: str, # e.g., \"astropy/astropy\" or \"local/dummy_project\"\n",
    "    commit_hash: str,\n",
    "    base_clones_dir: Path,\n",
    "    dummy_files_setup: Optional[Callable[[Path], None]] = None # For dummy repos\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Ensures the specified repository is cloned and checked out to the given commit.\n",
    "    Returns the local path to the repository. Handles local/dummy setups.\n",
    "    Resets the repository state to avoid contamination between tasks.\n",
    "    \"\"\"\n",
    "    if repo_identifier.startswith(\"local/\"):\n",
    "        project_name = repo_identifier.split(\"/\", 1)[1]\n",
    "        local_repo_path = base_clones_dir / project_name\n",
    "        local_repo_path.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Setting up local/dummy repository at: {local_repo_path}\")\n",
    "        # Clean directory before setting up dummy files to ensure consistent state\n",
    "        for item in local_repo_path.iterdir():\n",
    "            if item.is_file(): item.unlink()\n",
    "            elif item.is_dir(): shutil.rmtree(item) # Requires import shutil\n",
    "        if dummy_files_setup:\n",
    "            dummy_files_setup(local_repo_path)\n",
    "        return local_repo_path.resolve()\n",
    "\n",
    "    # For actual git repositories\n",
    "    sanitized_repo_name = repo_identifier.replace(\"/\", \"__\")\n",
    "    local_repo_path = base_clones_dir / sanitized_repo_name\n",
    "\n",
    "    try:\n",
    "        if not local_repo_path.exists():\n",
    "            logger.info(f\"Repository {repo_identifier} not found locally. Cloning to {local_repo_path}...\")\n",
    "            local_repo_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            clone_url = f\"https://github.com/{repo_identifier}.git\"\n",
    "            # Clone with depth 1 initially if possible, then fetch specific commit if needed? No, need full history.\n",
    "            subprocess.run([\"git\", \"clone\", clone_url, str(local_repo_path)], check=True, timeout=600, capture_output=True)\n",
    "            logger.info(f\"Cloned {repo_identifier}.\")\n",
    "\n",
    "        # Ensure we are at the correct commit and the working directory is clean\n",
    "        logger.info(f\"Setting repository {repo_identifier} to commit {commit_hash} and cleaning...\")\n",
    "        # Fetch latest changes in case the commit is newer than the clone\n",
    "        subprocess.run([\"git\", \"fetch\"], cwd=local_repo_path, check=True, timeout=120, capture_output=True)\n",
    "        # Reset hard to the specific commit (discards local changes, index, and working tree changes)\n",
    "        subprocess.run([\"git\", \"reset\", \"--hard\", commit_hash], cwd=local_repo_path, check=True, timeout=60, capture_output=True)\n",
    "        # Clean untracked files and directories (-fdx option: files, directories, ignored files)\n",
    "        subprocess.run([\"git\", \"clean\", \"-fdx\"], cwd=local_repo_path, check=True, timeout=60, capture_output=True)\n",
    "        logger.info(f\"Repository {repo_identifier} set to commit {commit_hash} and cleaned.\")\n",
    "\n",
    "        return local_repo_path.resolve()\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        stderr = e.stderr.decode() if e.stderr else \"N/A\"\n",
    "        stdout = e.stdout.decode() if e.stdout else \"N/A\"\n",
    "        logger.error(f\"Git command failed for {repo_identifier} at {commit_hash}. Error: {e}\\nSTDOUT:\\n{stdout}\\nSTDERR:\\n{stderr}\")\n",
    "        raise\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        logger.error(f\"Git command timed out for {repo_identifier}: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error setting up repository {repo_identifier}: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_current_commit(repo_dir: Path) -> Optional[str]:\n",
    "    \"\"\"Gets the current commit hash of a git repository.\"\"\"\n",
    "    if not (repo_dir / \".git\").exists():\n",
    "        # logger.warning(f\"No .git directory found in {repo_dir}, cannot get commit hash.\")\n",
    "        return None # Expected for dummy repos\n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            [\"git\", \"rev-parse\", \"HEAD\"],\n",
    "            cwd=repo_dir,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        return process.stdout.strip()\n",
    "    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError) as e:\n",
    "        logger.error(f\"Error getting current commit for {repo_dir}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2.6 Dummy Data Setup (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:37,850 - WARNING - Creating/Overwriting dummy dataset for demonstration at SWE-Bench-CL-Curriculum_dummy.json\n",
      "2025-05-13 18:57:37,851 - INFO - Setting up local/dummy repository at: cloned_repos/dummy_math_project\n",
      "2025-05-13 18:57:37,853 - INFO - Dummy files created/reset in cloned_repos/dummy_math_project\n",
      "2025-05-13 18:57:37,853 - INFO - Initial setup for dummy repository 'local/dummy_math_project' complete.\n",
      "2025-05-13 18:57:37,870 - INFO - Successfully loaded dummy dataset from ../data/SWE-Bench-CL-Curriculum.json\n"
     ]
    }
   ],
   "source": [
    "# This cell creates the dummy dataset file if USE_DUMMY_DATA is True\n",
    "USE_DUMMY_DATA = True\n",
    "dummy_dataset_path = dataset_path.split(\"/\")[-1].split(\".\")[0] + \"_dummy.json\"\n",
    "\n",
    "if USE_DUMMY_DATA:\n",
    "    logger.warning(f\"Creating/Overwriting dummy dataset for demonstration at {dummy_dataset_path}\")\n",
    "    dummy_swe_bench_cl = {\n",
    "        \"metadata\": {\n",
    "            \"name\": \"SWE-Bench-CL-Dummy\",\n",
    "            \"description\": \"A dummy dataset for SWE-Bench-CL testing.\",\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"num_sequences\": 1,\n",
    "            \"total_tasks\": 1,\n",
    "            \"repositories\": [\"local/dummy_math_project\"],\n",
    "            \"generation_date\": \"2024-07-27T10:00:00Z\"\n",
    "        },\n",
    "        \"evaluation_metrics\": swe_bench_cl['evaluation_metrics'] if swe_bench_cl else {}, # Copy metrics if real dataset loaded\n",
    "        \"sequences\": [\n",
    "            {\n",
    "                \"id\": \"dummy_math_project_sequence\",\n",
    "                \"repo\": \"local/dummy_math_project\", # Sequence level repo identifier\n",
    "                \"num_tasks\": 1,\n",
    "                \"statistics\": { # Optional: Add dummy stats if needed by evaluator\n",
    "                    \"difficulty_distribution\": {\"easy\": 1},\n",
    "                    \"tasks_with_dependencies\": 0,\n",
    "                    \"dependency_rate\": 0.0\n",
    "                },\n",
    "                \"tasks\": [\n",
    "                    {\n",
    "                        \"metadata\": {\n",
    "                            \"instance_id\": \"local__dummy_math_project_task_1\",\n",
    "                            \"repo\": \"local/dummy_math_project\", # Task level repo identifier\n",
    "                            \"base_commit\": \"initial_state\", # Dummy commit hash\n",
    "                            \"created_at\": \"2024-01-01T12:00:00+00:00\",\n",
    "                            \"difficulty\": \"<15 min fix\" # Use CL difficulty levels\n",
    "                        },\n",
    "                        \"task\": {\n",
    "                            \"problem_statement\": \"The function `add(a, b)` in `math_utils.py` currently returns `a - b`. It should return `a + b`.\",\n",
    "                            \"hints_text\": \"Check the return statement in the add function. Basic arithmetic is needed.\"\n",
    "                        },\n",
    "                        \"evaluation\": {\n",
    "                            # Ground truth patch (useful for reference, not directly used by agent)\n",
    "                            \"patch\": \"diff --git a/math_utils.py b/math_utils.py\\n--- a/math_utils.py\\n+++ b/math_utils.py\\n@@ -1,2 +1,2 @@\\n def add(a, b):\\n-    return a - b\\n+    return a + b\",\n",
    "                            # Test patch to apply *before* running tests to check the fix\n",
    "                            \"test_patch\": \"diff --git a/test_math_utils.py b/test_math_utils.py\\n--- a/test_math_utils.py\\n+++ b/test_math_utils.py\\n@@ -1,5 +1,8 @@\\n import unittest\\n from math_utils import add\\n+\\n class TestMath(unittest.TestCase):\\n-    def test_initial_behavior(self):\\n-        self.assertEqual(add(2, 2), 0) # Current incorrect behavior\\n+    def test_addition(self):\\n+        # This test should fail initially and pass after the fix\\n+        self.assertEqual(add(2, 2), 4)\\n     def test_existing_behavior(self):\\n         self.assertTrue(True)\\n \\n\",\n",
    "                            # Tests that should FAIL before the fix and PASS after\n",
    "                            \"FAIL_TO_PASS\": [\"test_math_utils.TestMath.test_addition\"],\n",
    "                            # Tests that should PASS before and PASS after (regression check)\n",
    "                            \"PASS_TO_PASS\": [\"test_math_utils.TestMath.test_existing_behavior\"]\n",
    "                        },\n",
    "                        \"continual_learning\": {\n",
    "                            \"sequence_position\": 1,\n",
    "                            \"difficulty_score\": 1,\n",
    "                            \"dependencies\": [],\n",
    "                            \"modified_files\": [\"math_utils.py\"]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    with open(dummy_dataset_path, 'w') as f:\n",
    "        json.dump(dummy_swe_bench_cl, f, indent=2)\n",
    "\n",
    "    # Define the setup function for the dummy repository files\n",
    "    def dummy_files_setup_for_test(project_root_path: Path):\n",
    "        project_root_path.mkdir(parents=True, exist_ok=True)\n",
    "        # Create the initial buggy file\n",
    "        with open(project_root_path / \"math_utils.py\", \"w\") as f:\n",
    "            f.write(\"def add(a, b):\\n    return a - b\\n\")\n",
    "        # Create the initial test file (before test_patch is applied)\n",
    "        with open(project_root_path / \"test_math_utils.py\", \"w\") as f:\n",
    "            f.write(\"import unittest\\nfrom math_utils import add\\n\\nclass TestMath(unittest.TestCase):\\n    def test_initial_behavior(self):\\n        # This test reflects the initial incorrect state\\n        self.assertEqual(add(2, 2), 0)\\n    def test_existing_behavior(self):\\n        # This test should always pass\\n        self.assertTrue(True)\\n\\nif __name__ == '__main__':\\n    unittest.main()\\n\")\n",
    "        logger.info(f\"Dummy files created/reset in {project_root_path}\")\n",
    "\n",
    "    # Perform initial setup using the utility function\n",
    "    dummy_repo_id = dummy_swe_bench_cl[\"sequences\"][0][\"tasks\"][0][\"metadata\"][\"repo\"]\n",
    "    dummy_commit = dummy_swe_bench_cl[\"sequences\"][0][\"tasks\"][0][\"metadata\"][\"base_commit\"]\n",
    "    try:\n",
    "        setup_repository(dummy_repo_id, dummy_commit, REPOS_BASE_DIR, dummy_files_setup=dummy_files_setup_for_test)\n",
    "        logger.info(f\"Initial setup for dummy repository '{dummy_repo_id}' complete.\")\n",
    "    except Exception as e_dummy_setup:\n",
    "        logger.error(f\"Failed initial setup of dummy repository: {e_dummy_setup}\")\n",
    "\n",
    "\n",
    "# Reload swe_bench_cl if we created the dummy file\n",
    "if USE_DUMMY_DATA and dummy_swe_bench_cl:\n",
    "     try:\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            swe_bench_cl = json.load(f)\n",
    "        logger.info(f\"Successfully loaded dummy dataset from {dataset_path}\")\n",
    "     except Exception as e:\n",
    "         logger.error(f\"Failed to reload dummy dataset: {e}\")\n",
    "         # Execution should probably stop if dataset isn't loaded\n",
    "         raise RuntimeError(\"Dataset could not be loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:37,885 - INFO - Successfully initialized models: ['google/gemini-2.0-flash']\n"
     ]
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/Shayan/Library/CloudStorage/GoogleDrive-sc4040@columbia.edu/My Drive/Academics/Spring 2025/COMS 4995 - Neural Nets & Deep Learning/NNDL Final Project/agents-never-forget/.env')\n",
    "\n",
    "# Model configuration\n",
    "### IMPORTANT: Naming format for models is `provider/model_name`\n",
    "MODELS = [\n",
    "    ## Closed-source models\n",
    "    # \"google/gemini-2.5-pro-preview-05-06\",\n",
    "    # \"google/gemini-2.5-flash-preview-04-17\",\n",
    "    \"google/gemini-2.0-flash\",\n",
    "    # \"anthropic/claude-3-7-sonnet\",\n",
    "    # \"openai/gpt-4o\",\n",
    "    ## Open-source models\n",
    "    # \"ollama/llama3.1:8b\"\n",
    "    # \"ollama/llama4:scout\", # 17B\n",
    "    # \"ollama/gemma3:27b\",\n",
    "    # \"ollama/qwen-3:14b\",\n",
    "    # \"ollama/deepseek-r1:14b\",\n",
    "]\n",
    "TEMPERATURE = 0.2\n",
    "MAX_TOKENS = 4096 # Max tokens for model output, not context window\n",
    "\n",
    "# Function to initialize model based on provider\n",
    "def get_llm(model):\n",
    "    provider = model.split(\"/\")[0]\n",
    "    model_name = model.split(\"/\")[1]\n",
    "    \n",
    "    if provider == \"anthropic\":\n",
    "        return ChatAnthropic(\n",
    "            model=model_name,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "        )\n",
    "    elif provider == \"openai\":\n",
    "        return ChatOpenAI(\n",
    "            model=model_name,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        )\n",
    "    elif provider == \"google\":\n",
    "        return ChatGoogleGenerativeAI(\n",
    "            model=model_name,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            google_api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    "        )\n",
    "    elif provider == \"ollama\":\n",
    "        return ChatOllama(\n",
    "            model=model_name,\n",
    "            temperature=TEMPERATURE,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "# Initialize models that are configured and have API keys\n",
    "initialized_models = {}\n",
    "for name in MODELS:\n",
    "    llm = get_llm(name)\n",
    "    if llm: initialized_models[name] = llm\n",
    "    else: logger.warning(f\"Could not initialize model: {name}. It will be skipped.\")\n",
    "\n",
    "MODELS = list(initialized_models.keys()) # Update MODELS to only include successfully initialized ones\n",
    "if not initialized_models: logger.error(\"No models were successfully initialized. Please check your API keys and model names.\")\n",
    "else: logger.info(f\"Successfully initialized models: {list(initialized_models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load API keys from .env file\n",
    "# from dotenv import load_dotenv\n",
    "# # Specify the path to your .env file explicitly if it's not in the default location\n",
    "# dotenv_path = Path('/Users/Shayan/Library/CloudStorage/GoogleDrive-sc4040@columbia.edu/My Drive/Academics/Spring 2025/COMS 4995 - Neural Nets & Deep Learning/NNDL Final Project/agents-never-forget/.env') # Adjust this path\n",
    "# if dotenv_path.exists():\n",
    "#     load_dotenv(dotenv_path=dotenv_path)\n",
    "#     logger.info(f\"Loaded environment variables from {dotenv_path}\")\n",
    "# else:\n",
    "#     logger.warning(f\".env file not found at {dotenv_path}. Relying on environment variables.\")\n",
    "\n",
    "\n",
    "# # Model configuration\n",
    "# ### IMPORTANT: Naming format for models is `provider/model_name`\n",
    "# MODELS = [\n",
    "#     # Closed-source models\n",
    "#     # \"anthropic/claude-3-opus-20240229\",\n",
    "#     # \"anthropic/claude-3-sonnet-20240229\",\n",
    "#     # \"anthropic/claude-3-haiku-20240307\",\n",
    "#     # \"openai/gpt-4o\",\n",
    "#     # \"openai/gpt-4-turbo\",\n",
    "#     # \"openai/gpt-3.5-turbo\",\n",
    "#     \"google/gemini-1.5-pro-latest\",\n",
    "#     # \"google/gemini-1.5-flash-latest\",\n",
    "\n",
    "#     # Open-source models (via Ollama) - Ensure Ollama server is running with these models pulled\n",
    "#     # \"ollama/llama3\", # Default llama3 model\n",
    "#     # \"ollama/codellama\",\n",
    "#     # \"ollama/mistral\",\n",
    "# ]\n",
    "# TEMPERATURE = 0.1 # Lower temperature for more deterministic behavior in SWE tasks\n",
    "# MAX_TOKENS = 4096 # Max tokens for model *output*, not context window\n",
    "\n",
    "# # Function to initialize model based on provider\n",
    "# def get_llm(model_id: str):\n",
    "#     \"\"\"Initializes LangChain ChatModel based on model_id string.\"\"\"\n",
    "#     if not isinstance(model_id, str) or \"/\" not in model_id:\n",
    "#          raise ValueError(f\"Invalid model_id format: {model_id}. Expected 'provider/model_name'.\")\n",
    "\n",
    "#     provider, model_name = model_id.split(\"/\", 1)\n",
    "\n",
    "#     try:\n",
    "#         if provider == \"anthropic\":\n",
    "#             api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "#             if not api_key: raise ValueError(\"ANTHROPIC_API_KEY not found in environment.\")\n",
    "#             return ChatAnthropic(\n",
    "#                 model=model_name,\n",
    "#                 temperature=TEMPERATURE,\n",
    "#                 max_tokens=MAX_TOKENS,\n",
    "#                 api_key=api_key,\n",
    "#             )\n",
    "#         elif provider == \"openai\":\n",
    "#             api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#             if not api_key: raise ValueError(\"OPENAI_API_KEY not found in environment.\")\n",
    "#             return ChatOpenAI(\n",
    "#                 model=model_name,\n",
    "#                 temperature=TEMPERATURE,\n",
    "#                 max_tokens=MAX_TOKENS,\n",
    "#                 api_key=api_key,\n",
    "#             )\n",
    "#         elif provider == \"google\":\n",
    "#             api_key = os.getenv(\"GOOGLE_API_KEY\") # Often named GOOGLE_API_KEY or GEMINI_API_KEY\n",
    "#             if not api_key: raise ValueError(\"GOOGLE_API_KEY (or GEMINI_API_KEY) not found in environment.\")\n",
    "#             # Ensure the correct model name format for Google (e.g., 'gemini-1.5-pro-latest')\n",
    "#             # The split already gives us the correct model_name part\n",
    "#             return ChatGoogleGenerativeAI(\n",
    "#                 model=model_name, # Use the part after 'google/'\n",
    "#                 temperature=TEMPERATURE,\n",
    "#                 max_tokens=MAX_TOKENS,\n",
    "#                 google_api_key=api_key,\n",
    "#                 # convert_system_message_to_human=True # Sometimes needed for older models/versions\n",
    "#             )\n",
    "#         elif provider == \"ollama\":\n",
    "#             # Assumes Ollama server is running at default http://localhost:11434\n",
    "#             # Ensure the model_name (e.g., 'llama3') is pulled in Ollama\n",
    "#             return ChatOllama(\n",
    "#                 model=model_name,\n",
    "#                 temperature=TEMPERATURE,\n",
    "#                 # max_tokens might not be directly supported or needed for Ollama Chat models in the same way\n",
    "#             )\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Failed to initialize model {model_id}: {e}\")\n",
    "#         return None # Return None on failure\n",
    "\n",
    "# # Initialize models that are configured and have API keys/Ollama setup\n",
    "# initialized_models = {}\n",
    "# for name in MODELS:\n",
    "#     llm = get_llm(name)\n",
    "#     if llm:\n",
    "#         initialized_models[name] = llm\n",
    "#         logger.info(f\"Successfully initialized model: {name}\")\n",
    "#     else:\n",
    "#         logger.warning(f\"Could not initialize model: {name}. It will be skipped.\")\n",
    "\n",
    "# MODELS = list(initialized_models.keys()) # Update MODELS to only include successfully initialized ones\n",
    "# if not initialized_models:\n",
    "#     logger.error(\"No models were successfully initialized. Please check API keys, .env path, Ollama server, and model names.\")\n",
    "#     # Optionally raise an error if no models are essential\n",
    "#     # raise RuntimeError(\"Model initialization failed. Cannot proceed.\")\n",
    "# else:\n",
    "#     logger.info(f\"Successfully initialized models available for evaluation: {MODELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:37,897 - INFO - Testing model: google/gemini-2.0-flash with prompt: 'Explain the concept of an Agent-Computer Interface (ACI) in 50 words.'\n",
      "2025-05-13 18:57:40,826 - INFO - Test response from google/gemini-2.0-flash: An Agent-Computer Interface (ACI) facilitates communication between a software agent and a computer system. It defines protocols and methods allowing the agent to perceive the environment, perform act...\n"
     ]
    }
   ],
   "source": [
    "# Test an initialized model (if any exist)\n",
    "if initialized_models:\n",
    "    try:\n",
    "        test_model_id = next(iter(initialized_models))\n",
    "        test_llm = initialized_models[test_model_id]\n",
    "        test_prompt = \"Explain the concept of an Agent-Computer Interface (ACI) in 50 words.\"\n",
    "        logger.info(f\"Testing model: {test_model_id} with prompt: '{test_prompt}'\")\n",
    "        response = test_llm.invoke([HumanMessage(content=test_prompt)])\n",
    "        logger.info(f\"Test response from {test_model_id}: {response.content[:200]}...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error testing model {test_model_id}: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"Skipping model test as no models were initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. SWE-agent Tools Implementation\n",
    "\n",
    "\n",
    "\n",
    " Implement the LM-friendly tools described in the SWE-agent paper for file navigation, searching, editing (with linting), and execution. These tools operate on the `repo_path` provided in the agent's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tool Schemas (Pydantic Models) ---\n",
    "\n",
    "class FindFileSchema(BaseModel):\n",
    "    \"\"\"Search for files by name within the repository.\"\"\"\n",
    "    filename: str = Field(description=\"The name or pattern of the file to find (e.g., 'test_*.py', 'settings.py').\")\n",
    "    repo_path: str = Field(description=\"The local filesystem path to the root of the repository.\")\n",
    "    # directory: Optional[str] = Field(default=\".\", description=\"Directory relative to repo_path to start the search.\") # Simplified: always search from root\n",
    "\n",
    "class SearchSchema(BaseModel):\n",
    "    \"\"\"Search for a string pattern within files or directories.\"\"\"\n",
    "    query: str = Field(description=\"The string or regex pattern to search for.\")\n",
    "    repo_path: str = Field(description=\"The local filesystem path to the root of the repository.\")\n",
    "    target: Optional[str] = Field(default=None, description=\"Optional: Path to a specific file or directory relative to repo_path to search within. If None, searches the current open file (if any) or the whole repo.\")\n",
    "    search_type: str = Field(description=\"Must be 'file' or 'dir'. Specifies whether to search a single file or a directory.\")\n",
    "\n",
    "class FileViewerSchema(BaseModel):\n",
    "    \"\"\"Interact with the file viewer: open a file, scroll, or go to a line.\"\"\"\n",
    "    action: str = Field(description=\"Action to perform: 'open', 'scroll_up', 'scroll_down', 'goto'.\")\n",
    "    repo_path: str = Field(description=\"The local filesystem path to the root of the repository.\")\n",
    "    path: Optional[str] = Field(default=None, description=\"Path to the file relative to repo_path (required for 'open').\")\n",
    "    line_number: Optional[int] = Field(default=None, description=\"Target line number (required for 'goto', optional for 'open').\")\n",
    "\n",
    "class EditSchema(BaseModel):\n",
    "    \"\"\"Edit the currently open file by replacing a range of lines.\"\"\"\n",
    "    start_line: int = Field(description=\"The 1-indexed starting line number of the range to replace (inclusive).\")\n",
    "    end_line: int = Field(description=\"The 1-indexed ending line number of the range to replace (inclusive). To insert before line N, use start_line=N, end_line=N-1. To delete lines N-M, provide empty replacement_text.\")\n",
    "    replacement_text: str = Field(description=\"The new text to insert. Use '\\\\n' for newlines. Must include correct indentation.\")\n",
    "    repo_path: str = Field(description=\"The local filesystem path to the root of the repository (used for context, edit happens on open file state).\")\n",
    "    # current_open_file is implicitly taken from agent state\n",
    "\n",
    "class RunTestsSchema(BaseModel):\n",
    "    \"\"\"Run a shell command, typically for executing tests.\"\"\"\n",
    "    command: str = Field(description=\"The shell command to execute (e.g., 'python -m pytest', 'make test').\")\n",
    "    repo_path: str = Field(description=\"The local filesystem path to the root of the repository where the command should be run.\")\n",
    "\n",
    "# --- Tool Implementations ---\n",
    "\n",
    "# Constants for tools\n",
    "FILE_VIEWER_WINDOW_SIZE = 100\n",
    "MAX_SEARCH_RESULTS = 50 # As per SWE-agent paper\n",
    "\n",
    "def format_file_viewer_output(file_path_rel: str, lines: List[str], start_line_idx: int, window_size: int, total_lines: int) -> str:\n",
    "    \"\"\"Formats the file content for the agent's view.\"\"\"\n",
    "    end_line_idx = min(start_line_idx + window_size, total_lines)\n",
    "    window_lines = lines[start_line_idx:end_line_idx]\n",
    "\n",
    "    output = f\"[File: {file_path_rel} ({total_lines} lines total)]\\n\"\n",
    "    if start_line_idx > 0:\n",
    "        output += f\"({start_line_idx} lines above)\\n\"\n",
    "\n",
    "    for i, line in enumerate(window_lines):\n",
    "        output += f\"{start_line_idx + 1 + i}: {line.rstrip()}\\n\" # Display 1-indexed line numbers\n",
    "\n",
    "    lines_below = total_lines - end_line_idx\n",
    "    if lines_below > 0:\n",
    "        output += f\"({lines_below} lines below)\\n\"\n",
    "    return output\n",
    "\n",
    "@tool(\"find_file\", args_schema=FindFileSchema)\n",
    "def find_file(filename: str, repo_path: str) -> str:\n",
    "    \"\"\"Finds files matching the filename pattern within the repository.\"\"\"\n",
    "    repo_abs_path = Path(repo_path).resolve()\n",
    "    if not repo_abs_path.is_dir():\n",
    "        return f\"Error: Repository path {repo_path} does not exist.\"\n",
    "\n",
    "    try:\n",
    "        # Use git ls-files for potentially faster searching in git repos, fallback to find\n",
    "        cmd_git = [\"git\", \"ls-files\", f\"*{filename}*\"] # Use wildcards for broader matching\n",
    "        process_git = subprocess.run(cmd_git, cwd=repo_abs_path, capture_output=True, text=True, timeout=30)\n",
    "\n",
    "        if process_git.returncode == 0 and process_git.stdout:\n",
    "            results = process_git.stdout.strip().splitlines()\n",
    "        else:\n",
    "            # Fallback to 'find' command\n",
    "            cmd_find = [\"find\", \".\", \"-name\", filename, \"-type\", \"f\"]\n",
    "            process_find = subprocess.run(cmd_find, cwd=repo_abs_path, capture_output=True, text=True, timeout=30)\n",
    "            if process_find.returncode == 0:\n",
    "                 # Strip './' prefix from find results for cleaner relative paths\n",
    "                results = [p[2:] if p.startswith('./') else p for p in process_find.stdout.strip().splitlines()]\n",
    "            else:\n",
    "                logger.warning(f\"'git ls-files' and 'find' failed for {filename} in {repo_path}. Error: {process_find.stderr or process_git.stderr}\")\n",
    "                results = []\n",
    "\n",
    "        if not results:\n",
    "            return f\"No files found matching '{filename}' in {repo_path}.\"\n",
    "\n",
    "        if len(results) > MAX_SEARCH_RESULTS:\n",
    "             return f\"Found {len(results)} files matching '{filename}'. Please refine your search. Showing first {MAX_SEARCH_RESULTS}:\\n\" + \"\\n\".join(results[:MAX_SEARCH_RESULTS])\n",
    "        else:\n",
    "             return f\"Found {len(results)} files matching '{filename}':\\n\" + \"\\n\".join(results)\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return f\"Error: File search command timed out for '{filename}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred during find_file: {str(e)}\"\n",
    "\n",
    "@tool(\"search\", args_schema=SearchSchema)\n",
    "def search(query: str, repo_path: str, search_type: str, target: Optional[str] = None) -> str:\n",
    "    \"\"\"Searches for a query string in a specific file or directory using ripgrep (rg) or grep.\"\"\"\n",
    "    repo_abs_path = Path(repo_path).resolve()\n",
    "    if not repo_abs_path.is_dir():\n",
    "        return f\"Error: Repository path {repo_path} does not exist.\"\n",
    "\n",
    "    search_path = \".\" # Default to searching the whole repo relative path\n",
    "    if target:\n",
    "        target_abs_path = (repo_abs_path / target).resolve()\n",
    "        # Security check: ensure target is within repo\n",
    "        if repo_abs_path not in target_abs_path.parents and target_abs_path != repo_abs_path:\n",
    "             return f\"Error: Target path '{target}' is outside the repository '{repo_path}'.\"\n",
    "        if not target_abs_path.exists():\n",
    "            return f\"Error: Target path '{target}' does not exist.\"\n",
    "        # Use relative path from repo root for search command\n",
    "        search_path = str(target_abs_path.relative_to(repo_abs_path))\n",
    "\n",
    "    if search_type == 'file' and target and not target_abs_path.is_file():\n",
    "        return f\"Error: Target '{target}' is not a file, but search_type is 'file'.\"\n",
    "    if search_type == 'dir' and target and not target_abs_path.is_dir():\n",
    "         return f\"Error: Target '{target}' is not a directory, but search_type is 'dir'.\"\n",
    "    if search_type == 'file' and not target:\n",
    "        # TODO: Integrate with file viewer state - search current open file\n",
    "        return \"Error: Searching current open file not yet implemented via this tool. Provide a specific file path for 'target'.\"\n",
    "\n",
    "    try:\n",
    "        # Use ripgrep (rg) if available, fallback to grep\n",
    "        cmd = [\"rg\", \"-n\", \"--glob\", (\"*\" if search_type == 'dir' else target) if target else \"*\", query, search_path]\n",
    "        # Simpler command structure: rg -n query [path]\n",
    "        cmd = [\"rg\", \"-n\", query, search_path]\n",
    "\n",
    "        process = subprocess.run(cmd, cwd=repo_abs_path, capture_output=True, text=True, timeout=30)\n",
    "\n",
    "        if process.returncode == 0: # Found matches\n",
    "            results = process.stdout.strip().splitlines()\n",
    "            output = f\"Found {len(results)} matches for '{query}' in '{search_path}':\\n\"\n",
    "            if len(results) > MAX_SEARCH_RESULTS:\n",
    "                output += f\"(Showing first {MAX_SEARCH_RESULTS})\\n\"\n",
    "                results = results[:MAX_SEARCH_RESULTS]\n",
    "            # Format results: file:line:match\n",
    "            formatted_results = [f\"- {line}\" for line in results]\n",
    "            return output + \"\\n\".join(formatted_results)\n",
    "        elif process.returncode == 1: # No matches found\n",
    "            return f\"No results found for '{query}' in '{search_path}'.\"\n",
    "        else: # Error occurred\n",
    "            logger.warning(f\"ripgrep failed (code {process.returncode}): {process.stderr}. Trying grep...\")\n",
    "            # Fallback to grep - simpler grep command\n",
    "            grep_cmd_str = f\"grep -rnH -E '{query}' {search_path}\"\n",
    "            process_grep = subprocess.run(grep_cmd_str, shell=True, cwd=repo_abs_path, capture_output=True, text=True, timeout=30)\n",
    "\n",
    "            if process_grep.returncode == 0:\n",
    "                results = process_grep.stdout.strip().splitlines()\n",
    "                output = f\"Found {len(results)} matches for '{query}' in '{search_path}' (using grep):\\n\"\n",
    "                if len(results) > MAX_SEARCH_RESULTS:\n",
    "                    output += f\"(Showing first {MAX_SEARCH_RESULTS})\\n\"\n",
    "                    results = results[:MAX_SEARCH_RESULTS]\n",
    "                formatted_results = [f\"- {line}\" for line in results]\n",
    "                return output + \"\\n\".join(formatted_results)\n",
    "            elif process_grep.returncode == 1:\n",
    "                return f\"No results found for '{query}' in '{search_path}' (using grep).\"\n",
    "            else:\n",
    "                return f\"Error using grep (code {process_grep.returncode}): {process_grep.stderr}\"\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        return \"Error: ripgrep (rg) command not found. Please ensure it's installed and in PATH for efficient search.\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return f\"Error: Search command timed out for '{query}'.\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred during search: {str(e)}\"\n",
    "\n",
    "# Note: The file viewer tool needs access to the agent's state (current file, lines, window position).\n",
    "# LangGraph tools are typically stateless. We handle this by passing the relevant state parts\n",
    "# *from* the AgentState *into* the tool call within the graph execution logic.\n",
    "# The tool function itself will receive these as arguments.\n",
    "# The *return* value of the tool should include the updated state parts.\n",
    "\n",
    "@tool(\"file_viewer\", args_schema=FileViewerSchema)\n",
    "def file_viewer(action: str, repo_path: str, path: Optional[str] = None, line_number: Optional[int] = None,\n",
    "                # State passed from AgentState\n",
    "                current_open_file: Optional[str] = None,\n",
    "                current_file_lines: Optional[List[str]] = None,\n",
    "                current_window_start_line: int = 0\n",
    "                ) -> Dict[str, Any]:\n",
    "    \"\"\"Opens, scrolls, or jumps within a file viewer. Returns new view and updated state.\"\"\"\n",
    "    repo_abs_path = Path(repo_path).resolve()\n",
    "    output_state = { # Dictionary to return updated state fields\n",
    "        \"current_open_file\": current_open_file,\n",
    "        \"current_file_lines\": current_file_lines,\n",
    "        \"current_window_start_line\": current_window_start_line,\n",
    "        \"viewer_output\": \"\" # The formatted string to show the agent\n",
    "    }\n",
    "\n",
    "    if action == \"open\":\n",
    "        if not path: return {\"viewer_output\": \"Error: 'path' is required for 'open' action.\"}\n",
    "        target_abs_path = (repo_abs_path / path).resolve()\n",
    "        if repo_abs_path not in target_abs_path.parents and target_abs_path != repo_abs_path:\n",
    "            return {\"viewer_output\": f\"Error: Path '{path}' is outside the repository.\"}\n",
    "        if not target_abs_path.is_file():\n",
    "            return {\"viewer_output\": f\"Error: Path '{path}' is not a file.\"}\n",
    "\n",
    "        try:\n",
    "            lines = target_abs_path.read_text(encoding='utf-8', errors='ignore').splitlines(True) # Keep newlines\n",
    "            total_lines = len(lines)\n",
    "            start_line_idx = 0\n",
    "            if line_number:\n",
    "                start_line_idx = max(0, min(line_number - 1, total_lines - 1)) # Go to specific line (0-indexed)\n",
    "                # Center window if possible\n",
    "                start_line_idx = max(0, start_line_idx - FILE_VIEWER_WINDOW_SIZE // 2)\n",
    "\n",
    "            output_state[\"current_open_file\"] = path # Store relative path\n",
    "            output_state[\"current_file_lines\"] = lines\n",
    "            output_state[\"current_window_start_line\"] = start_line_idx\n",
    "            output_state[\"viewer_output\"] = format_file_viewer_output(path, lines, start_line_idx, FILE_VIEWER_WINDOW_SIZE, total_lines)\n",
    "            return output_state\n",
    "        except Exception as e:\n",
    "            return {\"viewer_output\": f\"Error opening file '{path}': {str(e)}\"}\n",
    "\n",
    "    # Actions requiring an open file\n",
    "    if not current_open_file or current_file_lines is None:\n",
    "        return {\"viewer_output\": \"Error: No file is currently open. Use 'open' first.\"}\n",
    "\n",
    "    total_lines = len(current_file_lines)\n",
    "    start_line_idx = current_window_start_line\n",
    "\n",
    "    if action == \"scroll_down\":\n",
    "        start_line_idx = min(current_window_start_line + FILE_VIEWER_WINDOW_SIZE, total_lines - 1)\n",
    "    elif action == \"scroll_up\":\n",
    "        start_line_idx = max(0, current_window_start_line - FILE_VIEWER_WINDOW_SIZE)\n",
    "    elif action == \"goto\":\n",
    "        if not line_number: return {\"viewer_output\": \"Error: 'line_number' is required for 'goto' action.\"}\n",
    "        target_line_idx = max(0, min(line_number - 1, total_lines - 1))\n",
    "        # Center window if possible\n",
    "        start_line_idx = max(0, target_line_idx - FILE_VIEWER_WINDOW_SIZE // 2)\n",
    "    else:\n",
    "        return {\"viewer_output\": f\"Error: Invalid file_viewer action '{action}'.\"}\n",
    "\n",
    "    output_state[\"current_window_start_line\"] = start_line_idx\n",
    "    output_state[\"viewer_output\"] = format_file_viewer_output(current_open_file, current_file_lines, start_line_idx, FILE_VIEWER_WINDOW_SIZE, total_lines)\n",
    "    return output_state\n",
    "\n",
    "\n",
    "def run_linter(file_path: Path, file_content: str) -> Tuple[bool, str]:\n",
    "    \"\"\"Runs flake8 linter on file content and returns (has_errors, error_message).\"\"\"\n",
    "    if not FLAKE8_AVAILABLE:\n",
    "        return False, \"Linter check skipped: flake8 not installed.\"\n",
    "\n",
    "    # Flake8 needs a file to operate on. Write temporary content.\n",
    "    # Consider security implications if file_path is user-controlled (though it should be within repo)\n",
    "    temp_file_path = file_path.parent / f\".~{file_path.name}.tmp\"\n",
    "    try:\n",
    "        with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(file_content)\n",
    "\n",
    "        # Configure flake8 engine\n",
    "        # Select specific error codes relevant to SWE-agent paper (syntax, undefined names)\n",
    "        # E111-E113: Indentation errors\n",
    "        # E999: SyntaxError\n",
    "        # F821: undefined name\n",
    "        # F822: undefined name in __all__\n",
    "        # F831: duplicate argument name\n",
    "        # E902: IOError (e.g., file cannot be read - less likely here)\n",
    "        select_codes = [\"E111\", \"E112\", \"E113\", \"E999\", \"F821\", \"F822\", \"F831\"]\n",
    "        style_guide = flake8_api.get_style_guide(select=select_codes)\n",
    "\n",
    "        report = style_guide.check_files([str(temp_file_path)])\n",
    "\n",
    "        errors = report.get_statistics('E') + report.get_statistics('F') # Combine Error and Failure codes\n",
    "        if errors:\n",
    "            error_messages = []\n",
    "            # report.print_errors() doesn't return string easily, parse manually if needed\n",
    "            # For simplicity, just report the codes found\n",
    "            # A more robust parser could extract line numbers and messages\n",
    "            for error_code in errors:\n",
    "                 error_messages.append(f\"Linter Error Code: {error_code}\")\n",
    "\n",
    "            # Try to get detailed messages (may need more robust parsing)\n",
    "            # This part is tricky with the legacy API. A direct subprocess call might be easier.\n",
    "            cmd_flake8 = [\"flake8\", \"--select=\" + \",\".join(select_codes), str(temp_file_path)]\n",
    "            proc = subprocess.run(cmd_flake8, capture_output=True, text=True, timeout=10)\n",
    "            if proc.stdout:\n",
    "                error_messages = proc.stdout.strip().splitlines()\n",
    "\n",
    "\n",
    "            return True, \"Linting Errors Found:\\n\" + \"\\n\".join(error_messages)\n",
    "        else:\n",
    "            return False, \"No critical linting errors found.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during linting: {e}\")\n",
    "        return False, f\"Linter check failed with exception: {e}\"\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        if temp_file_path.exists():\n",
    "            try:\n",
    "                temp_file_path.unlink()\n",
    "            except OSError:\n",
    "                logger.warning(f\"Could not remove temporary lint file: {temp_file_path}\")\n",
    "\n",
    "\n",
    "@tool(\"edit\", args_schema=EditSchema)\n",
    "def edit(start_line: int, end_line: int, replacement_text: str, repo_path: str,\n",
    "         # State passed from AgentState\n",
    "         current_open_file: Optional[str] = None,\n",
    "         current_file_lines: Optional[List[str]] = None,\n",
    "         current_window_start_line: int = 0\n",
    "         ) -> Dict[str, Any]:\n",
    "    \"\"\"Edits the currently open file, runs linter, and returns new view/state or error.\"\"\"\n",
    "    output_state = {\n",
    "        \"current_open_file\": current_open_file,\n",
    "        \"current_file_lines\": current_file_lines,\n",
    "        \"current_window_start_line\": current_window_start_line,\n",
    "        \"viewer_output\": \"\"\n",
    "    }\n",
    "\n",
    "    if not current_open_file or current_file_lines is None:\n",
    "        output_state[\"viewer_output\"] = \"Error: No file is currently open. Use 'open' first.\"\n",
    "        return output_state\n",
    "\n",
    "    repo_abs_path = Path(repo_path).resolve()\n",
    "    target_file_abs_path = (repo_abs_path / current_open_file).resolve()\n",
    "    if repo_abs_path not in target_file_abs_path.parents and target_file_abs_path != repo_abs_path:\n",
    "         output_state[\"viewer_output\"] = f\"Error: File path '{current_open_file}' seems outside the repository '{repo_path}'.\"\n",
    "         return output_state\n",
    "    if not target_file_abs_path.is_file(): # Should exist if open\n",
    "         output_state[\"viewer_output\"] = f\"Error: Open file '{current_open_file}' not found or is not a file.\"\n",
    "         return output_state\n",
    "\n",
    "    lines = list(current_file_lines) # Make a mutable copy\n",
    "    total_lines = len(lines)\n",
    "\n",
    "    # Validate line numbers (1-indexed input)\n",
    "    # Adjust for 0-indexed list access\n",
    "    start_idx = start_line - 1\n",
    "    # end_idx is the index *after* the last line to remove/replace (exclusive)\n",
    "    # For insertion (end_line = start_line - 1), end_idx = start_idx\n",
    "    end_idx = end_line # If replacing line N, end_line=N, end_idx=N (slices up to N)\n",
    "\n",
    "    if start_line < 1:\n",
    "        output_state[\"viewer_output\"] = f\"Error: start_line ({start_line}) must be 1 or greater.\"\n",
    "        return output_state\n",
    "    # Allow insertion at end: start_line = total_lines + 1, end_line = total_lines\n",
    "    if start_line > total_lines + 1:\n",
    "         output_state[\"viewer_output\"] = f\"Error: start_line ({start_line}) is out of bounds for file with {total_lines} lines.\"\n",
    "         return output_state\n",
    "    # Insertion check: end_line == start_line - 1 is valid\n",
    "    if end_line < start_line - 1:\n",
    "         output_state[\"viewer_output\"] = f\"Error: end_line ({end_line}) cannot be less than start_line - 1 ({start_line - 1}).\"\n",
    "         return output_state\n",
    "    if end_line > total_lines:\n",
    "         output_state[\"viewer_output\"] = f\"Error: end_line ({end_line}) is out of bounds for file with {total_lines} lines.\"\n",
    "         return output_state\n",
    "\n",
    "    # Perform the edit in memory\n",
    "    new_lines_list = replacement_text.splitlines(True) # Keep trailing newlines\n",
    "    try:\n",
    "        modified_lines = lines[:start_idx] + new_lines_list + lines[end_idx:]\n",
    "        modified_content = \"\".join(modified_lines)\n",
    "    except IndexError:\n",
    "         output_state[\"viewer_output\"] = f\"Error: Line index calculation failed for range {start_line}-{end_line}.\"\n",
    "         return output_state\n",
    "\n",
    "    # Run linter on the modified content\n",
    "    has_errors, lint_message = run_linter(target_file_abs_path, modified_content)\n",
    "\n",
    "    if has_errors:\n",
    "        # Format error message as per SWE-agent Appendix Figure 11\n",
    "        original_snippet = \"\".join(lines[max(0, start_idx-2):min(total_lines, end_idx+2)]) # Context around edit\n",
    "        proposed_snippet = \"\".join(modified_lines[max(0, start_idx-2):min(len(modified_lines), start_idx+len(new_lines_list)+2)])\n",
    "\n",
    "        error_output = (\n",
    "            f\"Your proposed edit has introduced new syntax error(s). Please understand the fixes and retry your edit command.\\n\"\n",
    "            f\"ERRORS:\\n{lint_message}\\n\\n\"\n",
    "            f\"This is how your edit would have looked if applied (showing context):\\n---\\n{proposed_snippet}---\\n\\n\"\n",
    "            f\"This is the original code before your edit (showing context):\\n---\\n{original_snippet}---\\n\\n\"\n",
    "            f\"Your changes have NOT been applied. Please fix your edit command and try again.\\n\"\n",
    "            f\"DO NOT re-run the same failed edit command. Running it again will lead to the same error.\"\n",
    "        )\n",
    "        output_state[\"viewer_output\"] = error_output\n",
    "        # State remains unchanged as edit was rejected\n",
    "        return output_state\n",
    "    else:\n",
    "        # Linter passed, apply changes to file and state\n",
    "        try:\n",
    "            with open(target_file_abs_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(modified_content)\n",
    "\n",
    "            output_state[\"current_file_lines\"] = modified_lines\n",
    "            # Keep window start line the same, let user navigate if needed\n",
    "            new_total_lines = len(modified_lines)\n",
    "            output_state[\"viewer_output\"] = f\"Edit successful. {lint_message}\\n\" + \\\n",
    "                                            format_file_viewer_output(current_open_file, modified_lines, current_window_start_line, FILE_VIEWER_WINDOW_SIZE, new_total_lines)\n",
    "            return output_state\n",
    "        except Exception as e:\n",
    "            output_state[\"viewer_output\"] = f\"Edit applied in memory but failed to write file '{current_open_file}': {str(e)}\"\n",
    "            # Revert state if write fails? Or keep in-memory state? Let's keep state for now.\n",
    "            output_state[\"current_file_lines\"] = modified_lines\n",
    "            return output_state\n",
    "\n",
    "\n",
    "@tool(\"run_tests\", args_schema=RunTestsSchema)\n",
    "def run_tests(command: str, repo_path: str) -> str:\n",
    "    \"\"\"Runs a shell command (usually tests) in the repository path.\"\"\"\n",
    "    repo_abs_path = Path(repo_path).resolve()\n",
    "    if not repo_abs_path.is_dir():\n",
    "        return f\"Error: Repository path {repo_path} does not exist.\"\n",
    "\n",
    "    try:\n",
    "        # Use shell=True carefully, ensure command isn't directly from LLM without validation if possible\n",
    "        # For SWE-Bench, the test commands are usually predefined or simple patterns.\n",
    "        process = subprocess.run(command, shell=True, cwd=repo_abs_path, capture_output=True, text=True, timeout=600) # 10 min timeout for tests\n",
    "        \n",
    "        # Provide concise feedback\n",
    "        output = f\"Command: {command}\\nExit Code: {process.returncode}\\n\"\n",
    "        stdout_snippet = process.stdout.strip()[-2000:] # Last 2000 chars of stdout\n",
    "        stderr_snippet = process.stderr.strip()[-2000:] # Last 2000 chars of stderr\n",
    "\n",
    "        if stdout_snippet:\n",
    "            output += f\"--- STDOUT (last 2000 chars) ---\\n{stdout_snippet}\\n\"\n",
    "        if stderr_snippet:\n",
    "             output += f\"--- STDERR (last 2000 chars) ---\\n{stderr_snippet}\\n\"\n",
    "\n",
    "        if not stdout_snippet and not stderr_snippet:\n",
    "             output += \"(No output produced)\\n\"\n",
    "\n",
    "        # Simple success indication based on exit code\n",
    "        if process.returncode == 0:\n",
    "            output += \"Command executed successfully (exit code 0).\"\n",
    "        else:\n",
    "            output += \"Command failed or produced errors (non-zero exit code).\"\n",
    "\n",
    "        return output\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return f\"Error: Command '{command}' timed out after 10 minutes.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred running command '{command}': {str(e)}\"\n",
    "\n",
    "# List of all SWE-agent style tools\n",
    "swe_agent_tools = [find_file, search, file_viewer, edit, run_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Semantic Memory System Integration\n",
    "\n",
    "\n",
    "\n",
    " Implement the semantic memory system using FAISS for RAG and integrate it into the agent workflow. This memory stores past task solutions and experiences, allowing the agent to retrieve relevant context for new tasks, especially within the same sequence (simulating learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:40,988 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2025-05-13 18:57:40,989 - INFO - Ollama embedding model 'nomic-embed-text' seems available.\n",
      "2025-05-13 18:57:40,989 - INFO - Memory system initialized with ollama/nomic-embed-text.\n"
     ]
    }
   ],
   "source": [
    "class SemanticMemory:\n",
    "    \"\"\"Stores and retrieves task experiences using vector embeddings.\"\"\"\n",
    "    def __init__(self, embedding_model: Any, k_results: int = 3):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.k_results = k_results\n",
    "        self.index = None\n",
    "        self.documents: List[Document] = []\n",
    "        self.doc_counter = 0 # Simple ID for documents\n",
    "\n",
    "    def add_entry(self, task_id: str, sequence_id: str, content: str, success: bool, metadata: Optional[Dict] = None):\n",
    "        \"\"\"Adds a task experience to the memory.\"\"\"\n",
    "        meta = metadata or {}\n",
    "        meta[\"task_id\"] = task_id\n",
    "        meta[\"sequence_id\"] = sequence_id\n",
    "        meta[\"success\"] = success\n",
    "        meta[\"doc_id\"] = self.doc_counter\n",
    "        self.doc_counter += 1\n",
    "\n",
    "        # Prepend status to content for better retrieval signal\n",
    "        status_prefix = \"[SUCCESSFUL SOLUTION]\" if success else \"[ATTEMPTED SOLUTION (Failed)]\"\n",
    "        full_content = f\"{status_prefix} for Task {task_id} in Sequence {sequence_id}:\\n{content}\"\n",
    "\n",
    "        doc = Document(page_content=full_content, metadata=meta)\n",
    "        self.documents.append(doc)\n",
    "\n",
    "        # Update FAISS index incrementally (or rebuild)\n",
    "        if self.documents:\n",
    "            try:\n",
    "                if self.index:\n",
    "                    # FAISS doesn't easily support incremental additions without saving/loading\n",
    "                    # Rebuilding is simpler for this scale. For larger scales, consider alternatives.\n",
    "                    self.index = FAISS.from_documents(self.documents, self.embedding_model)\n",
    "                else:\n",
    "                    self.index = FAISS.from_documents(self.documents, self.embedding_model)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error updating FAISS index: {e}\")\n",
    "                # Optionally remove the last added document if index update fails?\n",
    "                # self.documents.pop()\n",
    "                # self.doc_counter -=1\n",
    "\n",
    "    def retrieve_relevant(self, query: str, sequence_id_filter: Optional[str] = None, num_results: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"Retrieves relevant experiences, optionally filtering by sequence.\"\"\"\n",
    "        if not self.index:\n",
    "            logger.warning(\"Semantic memory index not initialized or empty.\")\n",
    "            return []\n",
    "\n",
    "        k = num_results if num_results is not None else self.k_results\n",
    "\n",
    "        try:\n",
    "            # FAISS basic search doesn't support metadata filtering directly in similarity_search.\n",
    "            # We retrieve more results and filter afterwards.\n",
    "            # Retrieve more candidates, e.g., k * 5 or a fixed larger number\n",
    "            candidate_k = k * 5\n",
    "            results_with_scores = self.index.similarity_search_with_score(query, k=max(candidate_k, len(self.documents)))\n",
    "\n",
    "            filtered_results = []\n",
    "            seen_task_ids = set() # Avoid duplicate task entries if content is similar\n",
    "            for doc, score in results_with_scores:\n",
    "                # Apply sequence filter if provided\n",
    "                if sequence_id_filter and doc.metadata.get(\"sequence_id\") != sequence_id_filter:\n",
    "                    continue\n",
    "                # Avoid duplicates\n",
    "                task_id = doc.metadata.get(\"task_id\", \"unknown\")\n",
    "                if task_id in seen_task_ids:\n",
    "                    continue\n",
    "\n",
    "                formatted = {\n",
    "                    \"task_id\": task_id,\n",
    "                    \"sequence_id\": doc.metadata.get(\"sequence_id\", \"unknown\"),\n",
    "                    \"content\": doc.page_content, # Already includes status prefix\n",
    "                    \"success\": doc.metadata.get(\"success\", False),\n",
    "                    \"score\": float(score) # Lower score is better in FAISS L2 distance\n",
    "                }\n",
    "                filtered_results.append(formatted)\n",
    "                seen_task_ids.add(task_id)\n",
    "\n",
    "                if len(filtered_results) >= k:\n",
    "                    break # Stop once we have enough filtered results\n",
    "\n",
    "            # Sort by score (ascending for L2 distance)\n",
    "            filtered_results.sort(key=lambda x: x[\"score\"])\n",
    "            return filtered_results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during similarity search: {e}\")\n",
    "            return []\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"Clears the memory.\"\"\"\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.doc_counter = 0\n",
    "        logger.info(\"Semantic memory cleared.\")\n",
    "\n",
    "class MemorySystem:\n",
    "    \"\"\"Manages semantic memory and context building for the agent.\"\"\"\n",
    "    def __init__(self, semantic_memory: SemanticMemory, max_context_tokens: int = 8000):\n",
    "        self.semantic_memory = semantic_memory\n",
    "        self.max_context_tokens = max_context_tokens # Rough token limit for context\n",
    "\n",
    "    def add_experience_to_memory(self, task_id: str, sequence_id: str, solution_data: Dict):\n",
    "        \"\"\"Adds a completed task experience to semantic memory.\"\"\"\n",
    "        summary = solution_data.get(\"solution_summary\", \"N/A\")\n",
    "        rationale = solution_data.get(\"final_rationale\", \"N/A\")\n",
    "        # Include tool usage info?\n",
    "        tool_calls = solution_data.get(\"tool_calls_count\", 0)\n",
    "        success = solution_data.get(\"tests_passed\", False) # Expecting a boolean\n",
    "\n",
    "        content_to_store = (\n",
    "            f\"Problem Summary (Task {task_id}): {solution_data.get('problem_statement', 'N/A')[:200]}...\\n\"\n",
    "            f\"Solution Summary: {summary}\\n\"\n",
    "            f\"Rationale: {rationale}\\n\"\n",
    "            f\"Tool Calls: {tool_calls}\\n\"\n",
    "            f\"Outcome: {'Success' if success else 'Failure'}\"\n",
    "        )\n",
    "\n",
    "        self.semantic_memory.add_entry(task_id, sequence_id, content_to_store, success=success)\n",
    "        logger.info(f\"Added {'successful' if success else 'failed'} experience for task {task_id} (Seq: {sequence_id}) to semantic memory.\")\n",
    "\n",
    "    def get_relevant_context_for_prompt(self, current_task_prompt: str, current_sequence_id: str, num_memories: int = 3) -> str:\n",
    "        \"\"\"Builds a context string including relevant memories.\"\"\"\n",
    "        # Retrieve memories, prioritizing those from the same sequence\n",
    "        retrieved_memories = self.semantic_memory.retrieve_relevant(\n",
    "            current_task_prompt,\n",
    "            sequence_id_filter=current_sequence_id,\n",
    "            num_results=num_memories\n",
    "        )\n",
    "\n",
    "        # If not enough memories from the same sequence, retrieve globally (optional)\n",
    "        # if len(retrieved_memories) < num_memories:\n",
    "        #     global_memories = self.semantic_memory.retrieve_relevant(\n",
    "        #         current_task_prompt,\n",
    "        #         num_results=num_memories - len(retrieved_memories)\n",
    "        #     )\n",
    "        #     # Add global memories if they are not already included\n",
    "        #     existing_ids = {mem['task_id'] for mem in retrieved_memories}\n",
    "        #     for mem in global_memories:\n",
    "        #         if mem['task_id'] not in existing_ids:\n",
    "        #             retrieved_memories.append(mem)\n",
    "        #     retrieved_memories.sort(key=lambda x: x[\"score\"]) # Re-sort after adding global\n",
    "\n",
    "        context_str = \"\"\n",
    "        if retrieved_memories:\n",
    "            context_str += \"\\n\\n--- Relevant Past Experiences (from Semantic Memory) ---\\n\"\n",
    "            # Simple heuristic for token counting (split by space) - use a proper tokenizer in production\n",
    "            current_token_count = 0\n",
    "            base_prompt_token_count = len(current_task_prompt.split())\n",
    "\n",
    "            for mem in retrieved_memories:\n",
    "                # Content already includes status prefix\n",
    "                mem_text = f\"Experience (Score: {mem['score']:.2f}):\\n{mem['content']}\\n---\\n\"\n",
    "                mem_token_count = len(mem_text.split())\n",
    "\n",
    "                # Check token limit (approximate)\n",
    "                # Need to account for base prompt tokens as well\n",
    "                if base_prompt_token_count + current_token_count + mem_token_count <= self.max_context_tokens:\n",
    "                    context_str += mem_text\n",
    "                    current_token_count += mem_token_count\n",
    "                else:\n",
    "                    logger.info(f\"Memory context limit reached ({self.max_context_tokens} tokens), truncating memories.\")\n",
    "                    break\n",
    "            context_str += \"--- End of Past Experiences ---\\n\"\n",
    "        return context_str\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clears the underlying semantic memory.\"\"\"\n",
    "        self.semantic_memory.clear()\n",
    "\n",
    "# --- Initialize Memory System ---\n",
    "# Choose embedding model\n",
    "# embedding_model_name = \"openai/text-embedding-3-small\"\n",
    "embedding_model_name = \"ollama/nomic-embed-text\" # Example for local embeddings via Ollama\n",
    "\n",
    "active_embedding_model = None\n",
    "memory_system = None\n",
    "\n",
    "try:\n",
    "    if embedding_model_name.startswith(\"openai/\"):\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key: logger.warning(\"OpenAI API key not found for embeddings.\")\n",
    "        else: active_embedding_model = OpenAIEmbeddings(model=embedding_model_name.split(\"/\")[1], openai_api_key=api_key)\n",
    "    elif embedding_model_name.startswith(\"ollama/\"):\n",
    "        # Ensure Ollama server is running and has the embedding model\n",
    "        try:\n",
    "            active_embedding_model = OllamaEmbeddings(model=embedding_model_name.split(\"/\")[1])\n",
    "            # Test connection\n",
    "            active_embedding_model.embed_query(\"test\")\n",
    "            logger.info(f\"Ollama embedding model '{embedding_model_name.split('/')[1]}' seems available.\")\n",
    "        except Exception as ollama_err:\n",
    "             logger.error(f\"Failed to connect or use Ollama embedding model '{embedding_model_name.split('/')[1]}'. Is Ollama running and the model pulled? Error: {ollama_err}\")\n",
    "             active_embedding_model = None\n",
    "    else:\n",
    "        logger.error(f\"Unsupported embedding model provider for: {embedding_model_name}\")\n",
    "\n",
    "    if active_embedding_model:\n",
    "        semantic_memory_instance = SemanticMemory(embedding_model=active_embedding_model)\n",
    "        memory_system = MemorySystem(semantic_memory_instance)\n",
    "        logger.info(f\"Memory system initialized with {embedding_model_name}.\")\n",
    "    else:\n",
    "        logger.error(\"Failed to initialize embedding model. Semantic memory system will be disabled.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing memory system: {e}. Memory system will be disabled.\")\n",
    "    memory_system = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Agentic Workflow Implementation (LangGraph + SWE-agent Principles)\n",
    "\n",
    "\n",
    "\n",
    " Define the agent state, adapt prompts from SWE-agent, and build the LangGraph workflow incorporating the new tools and semantic memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:41,037 - INFO - SWE-Agent-CL workflow compiled.\n"
     ]
    }
   ],
   "source": [
    "# --- Agent State ---\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"State for the SWE-Agent-CL.\"\"\"\n",
    "    # Task specific\n",
    "    problem_statement: str = Field(description=\"The problem description from the dataset\")\n",
    "    hints: Optional[str] = Field(default=None, description=\"Hints provided for the task\")\n",
    "    repo_path: str = Field(description=\"Absolute path to the locally cloned repository\")\n",
    "    task_id: str = Field(description=\"Unique identifier for the current task\")\n",
    "    sequence_id: str = Field(description=\"Identifier for the sequence the task belongs to\")\n",
    "    task_data: Dict = Field(description=\"Full data dictionary for the current task\") # Includes evaluation details\n",
    "\n",
    "    # Model and Memory\n",
    "    model_id: str = Field(description=\"Identifier of the LLM being used\")\n",
    "    memory_enabled: bool = Field(default=True, description=\"Whether semantic memory is enabled\")\n",
    "\n",
    "    # Agent's internal state\n",
    "    messages: Annotated[List[BaseMessage], operator.add] = Field(default_factory=list, description=\"History of messages exchanged with the LLM\")\n",
    "    execution_log: List[str] = Field(default_factory=list, description=\"Human-readable log of agent actions and outcomes\")\n",
    "    final_solution: Optional[Dict] = Field(default=None, description=\"The final structured solution proposed by the agent\") # Changed from AgentSolution model to dict for flexibility\n",
    "\n",
    "    # File Viewer State\n",
    "    current_open_file: Optional[str] = Field(default=None, description=\"Relative path of the file currently open in the viewer\")\n",
    "    current_file_lines: Optional[List[str]] = Field(default=None, description=\"Content of the open file, split into lines\")\n",
    "    current_window_start_line: int = Field(default=0, description=\"0-indexed start line of the current view window\")\n",
    "\n",
    "    # Metrics\n",
    "    tool_calls_count: int = Field(default=0, description=\"Total number of tool calls made\")\n",
    "    successful_tool_calls: int = Field(default=0, description=\"Number of tool calls that did not return an explicit error\")\n",
    "    errors_encountered: List[str] = Field(default_factory=list, description=\"Log of errors encountered during execution\")\n",
    "    turn_count: int = Field(default=0, description=\"Number of agent turns\")\n",
    "\n",
    "\n",
    "# --- Prompts (Adapted from SWE-agent Appendix C) ---\n",
    "\n",
    "# Utility to format tool documentation for the prompt\n",
    "def get_tool_documentation(tools: List[Callable]) -> str:\n",
    "    \"\"\"Generates markdown documentation for available tools.\"\"\"\n",
    "    doc_str = \"\"\n",
    "    for tool_func in tools:\n",
    "        schema = tool_func.args_schema\n",
    "        # Extract signature, docstring, args from schema\n",
    "        signature = f\"{tool_func.name} <args>\" # Simplified signature\n",
    "        docstring = schema.model_json_schema().get('description', 'No description available.')\n",
    "\n",
    "        args_desc = []\n",
    "        if schema.model_json_schema().get('properties'):\n",
    "            for name, prop in schema.model_json_schema()['properties'].items():\n",
    "                 # Skip repo_path and state fields in user-facing docs\n",
    "                if name in ['repo_path', 'current_open_file', 'current_file_lines', 'current_window_start_line']:\n",
    "                    continue\n",
    "                req = \"required\" if name in schema.model_json_schema().get('required', []) else \"optional\"\n",
    "                prop_type = prop.get('type', 'any')\n",
    "                prop_desc = prop.get('description', '')\n",
    "                args_desc.append(f\"  - {name} ({prop_type}, {req}): {prop_desc}\")\n",
    "\n",
    "        doc_str += f\"COMMAND: `{signature}`\\n\"\n",
    "        doc_str += f\"DESCRIPTION: {docstring}\\n\"\n",
    "        if args_desc:\n",
    "            doc_str += f\"ARGUMENTS:\\n\" + \"\\n\".join(args_desc) + \"\\n\"\n",
    "        doc_str += \"---\\n\"\n",
    "    return doc_str\n",
    "\n",
    "TOOL_DOCUMENTATION = get_tool_documentation(swe_agent_tools)\n",
    "\n",
    "# System Prompt (SWE-agent Style)\n",
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"SETTING: You are SWE-Agent-CL, an autonomous software engineering agent. You are working directly in a bash command line interface with a special file viewing/editing interface to solve GitHub issues.\n",
    "The repository is cloned locally, and you need to navigate, search, view, edit files, and run tests to implement the required changes.\n",
    "\n",
    "SPECIAL INTERFACE COMMANDS:\n",
    "{tool_documentation}\n",
    "\n",
    "STANDARD BASH COMMANDS: You also have access to standard bash commands like `ls`, `cd`, `cat`, `mkdir`, `echo`, etc. However, the environment does NOT support interactive session commands (e.g., `python` REPL, `vim`). Write scripts and execute them instead.\n",
    "\n",
    "IMPORTANT TIPS (Review Carefully!):\n",
    "1.  **Reproduce First:** Always try to replicate the bug described in the issue. If reproduction code is provided, implement and run it. Fix the bug, then re-run the reproduction script to verify.\n",
    "2.  **Check Environment:** Pay attention to the current working directory and the currently open file shown in the prompt. Some commands (`create`, `open`) change the open file.\n",
    "3.  **File Navigation:** Use `find_file` to locate files by name. Use `search_dir` to find relevant code snippets across the repository before opening files.\n",
    "4.  **File Viewing:** Use `open <path>` to open a file. If you need to see lines beyond the initial view (e.g., line 583), use `goto 583` instead of scrolling repeatedly.\n",
    "5.  **Editing:**\n",
    "    *   Use the `edit <start_line>:<end_line>` command. Provide the *full* replacement text, including correct indentation.\n",
    "    *   The linter will check your edit. If it fails, the edit is *not applied*. Read the error, fix your `edit` command, and try again. DO NOT run the same failed edit command.\n",
    "    *   After a successful edit, the file viewer will update. Check the result carefully.\n",
    "6.  **Error Recovery:** If a command fails, try a different command or modify the arguments. A command that failed once will likely fail again if run identically.\n",
    "7.  **Single Command:** You can only run ONE command per turn. Wait for the response before issuing the next command.\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Your shell prompt is formatted as follows:\n",
    "(Open file: <relative_path_or_None>) (Current directory: <relative_path>) $\n",
    "\n",
    "You MUST format your output using two fields: 'DISCUSSION' and 'COMMAND'. Your output must always include EXACTLY one discussion and one command field, like this:\n",
    "DISCUSSION\n",
    "<Your reasoning, analysis, and plan for the next step.>\n",
    "COMMAND\n",
    "<The single command you want to execute.>\n",
    "\"\"\"\n",
    "\n",
    "# Instance Prompt Content (to be formatted in the planning node)\n",
    "INSTANCE_PROMPT_TEMPLATE = \"\"\"We're currently solving the following issue within the '{repo_name}' repository.\n",
    "Repository is locally cloned at: {repo_path}\n",
    "\n",
    "ISSUE DETAILS:\n",
    "Problem Statement:\n",
    "{problem_statement}\n",
    "\n",
    "Hints:\n",
    "{hints}\n",
    "\n",
    "Your task is to understand the issue, identify the necessary code changes, implement them using the available tools, and verify the fix (e.g., by running provided tests or the reproduction steps). Remember the IMPORTANT TIPS. Start by planning your first step.\n",
    "\"\"\"\n",
    "\n",
    "# Error prompt for malformed agent response\n",
    "FORMAT_ERROR_PROMPT = \"\"\"Your output was not formatted correctly. You must always include one DISCUSSION section and one COMMAND section. Make sure you do not have multiple discussion/command tags.\n",
    "Please make sure your output precisely matches the following format:\n",
    "DISCUSSION\n",
    "<Your reasoning and plan for the next step.>\n",
    "COMMAND\n",
    "<The single command you want to execute.>\n",
    "\n",
    "Your previous attempt's content (if any) was:\n",
    "{previous_attempt}\n",
    "\n",
    "Please try again.\n",
    "\"\"\"\n",
    "\n",
    "# --- Node Functions ---\n",
    "\n",
    "def planning_and_setup_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Initial node: Sets up the system prompt, instance prompt, and potentially initial plan.\"\"\"\n",
    "    logger.info(f\"--- Planning/Setup Node for Task {state.task_id} (Seq: {state.sequence_id}) ---\")\n",
    "\n",
    "    # Format the system prompt with tool docs\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(tool_documentation=TOOL_DOCUMENTATION)\n",
    "\n",
    "    # Format the instance prompt\n",
    "    repo_name = state.task_data.get(\"metadata\", {}).get(\"repo\", \"N/A\")\n",
    "    instance_prompt = INSTANCE_PROMPT_TEMPLATE.format(\n",
    "        repo_name=repo_name,\n",
    "        repo_path=state.repo_path, # Show the agent where the repo is\n",
    "        problem_statement=state.problem_statement,\n",
    "        hints=state.hints or \"No hints provided.\"\n",
    "    )\n",
    "\n",
    "    # Construct the initial prompt for the LLM (System + Instance + Memory)\n",
    "    initial_messages = [SystemMessage(content=system_prompt)]\n",
    "\n",
    "    # Add memory context if enabled\n",
    "    memory_context = \"\"\n",
    "    if memory_system and state.memory_enabled:\n",
    "        # Query based on problem statement and hints\n",
    "        query_text = f\"Problem: {state.problem_statement}\\nHints: {state.hints or ''}\"\n",
    "        memory_context = memory_system.get_relevant_context_for_prompt(query_text, state.sequence_id)\n",
    "\n",
    "    # Combine instance prompt and memory context\n",
    "    full_instance_prompt = instance_prompt\n",
    "    if memory_context:\n",
    "        full_instance_prompt += \"\\n\" + memory_context\n",
    "\n",
    "    initial_messages.append(HumanMessage(content=full_instance_prompt))\n",
    "\n",
    "    # Log the start\n",
    "    state.execution_log.append(f\"Starting task {state.task_id}. Repo: {state.repo_path}. Memory: {'Enabled' if state.memory_enabled else 'Disabled'}.\")\n",
    "    if memory_context:\n",
    "        state.execution_log.append(\"Retrieved relevant context from memory.\")\n",
    "\n",
    "    # No LLM call here yet, just setting up the initial state and messages\n",
    "    # The first call will happen in the 'agent_node'\n",
    "    return {\"messages\": initial_messages, \"execution_log\": state.execution_log}\n",
    "\n",
    "\n",
    "def agent_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"The core agent logic: takes current state, calls LLM, returns action or final answer.\"\"\"\n",
    "    logger.info(f\"--- Agent Node (Turn {state.turn_count}) ---\")\n",
    "    state.turn_count += 1\n",
    "\n",
    "    # Check for recursion depth / max turns\n",
    "    if state.turn_count > 50: # Set a max turn limit\n",
    "        logger.warning(f\"Max turns (50) reached for task {state.task_id}. Ending run.\")\n",
    "        # Consider how to handle this - maybe force submit or mark as failed\n",
    "        # For now, just log and let it proceed to potential END\n",
    "        state.errors_encountered.append(\"Max turns reached.\")\n",
    "        # TODO: Decide if this should directly lead to a final state or just stop further LLM calls.\n",
    "        # Returning the current state might lead to an infinite loop if not handled by edges.\n",
    "        # Let's try adding a \"force_end\" flag or similar if needed.\n",
    "        # Simplest for now: return empty messages to signal no further action.\n",
    "        return {\"messages\": [], \"final_solution\": {\"solution_summary\": \"Max turns reached.\", \"tests_passed\": False, \"final_rationale\": \"Agent exceeded turn limit.\"}}\n",
    "\n",
    "\n",
    "    llm = initialized_models[state.model_id]\n",
    "    messages_for_llm = state.messages # Get current message history\n",
    "\n",
    "    # Add current environment state to the prompt for the agent\n",
    "    # This mimics the `bash-$` prompt structure from SWE-agent\n",
    "    cwd_rel = Path(state.repo_path).name # Simplified CWD for prompt\n",
    "    open_file_rel = state.current_open_file if state.current_open_file else \"None\"\n",
    "    env_prompt = f\"\\n(Open file: {open_file_rel}) (Current directory: {cwd_rel}) $\"\n",
    "    \n",
    "    # Append env prompt to the last human message or add a new one\n",
    "    if messages_for_llm and isinstance(messages_for_llm[-1], HumanMessage):\n",
    "         messages_for_llm[-1].content += env_prompt\n",
    "    elif messages_for_llm and isinstance(messages_for_llm[-1], AIMessage):\n",
    "         # If last message was AI (e.g. after tool call), add Human prompt\n",
    "         messages_for_llm.append(HumanMessage(content=env_prompt))\n",
    "    elif messages_for_llm and isinstance(messages_for_llm[-1], ToolMessage):\n",
    "         # If last message was Tool, add Human prompt\n",
    "         messages_for_llm.append(HumanMessage(content=env_prompt))\n",
    "    # Handle initial case where last message might be System - already handled by setup node\n",
    "\n",
    "    try:\n",
    "        # Invoke the LLM\n",
    "        # We don't bind tools here; the agent should output DISCUSSION/COMMAND format\n",
    "        response_message = llm.invoke(messages_for_llm)\n",
    "\n",
    "        # Parse the response (DISCUSSION and COMMAND)\n",
    "        parsed_output = parse_agent_response(response_message.content)\n",
    "\n",
    "        if parsed_output[\"error\"]:\n",
    "            logger.warning(f\"Agent response parsing error: {parsed_output['error']}\")\n",
    "            # Re-prompt with error message\n",
    "            error_prompt = FORMAT_ERROR_PROMPT.format(previous_attempt=response_message.content)\n",
    "            # Add the original malformed AI message and the new error prompt\n",
    "            return {\"messages\": [response_message, HumanMessage(content=error_prompt)], \"errors_encountered\": state.errors_encountered + [f\"Malformed response: {parsed_output['error']}\"]}\n",
    "\n",
    "        discussion = parsed_output[\"discussion\"]\n",
    "        command = parsed_output[\"command\"]\n",
    "        state.execution_log.append(f\"Agent Discussion: {discussion}\")\n",
    "        state.execution_log.append(f\"Agent Command: {command}\")\n",
    "\n",
    "        # Check if the command is 'submit'\n",
    "        if command.strip().lower() == \"submit\":\n",
    "            logger.info(\"Agent issued 'submit' command. Proceeding to finalize.\")\n",
    "            # TODO: Implement finalization logic - maybe call a solver node or just end.\n",
    "            # For now, create a basic solution summary.\n",
    "            final_solution = {\n",
    "                \"solution_summary\": \"Agent submitted changes.\",\n",
    "                \"final_rationale\": discussion, # Use last discussion as rationale\n",
    "                \"tests_passed\": None, # Needs evaluation after submission\n",
    "                \"tool_calls_count\": state.tool_calls_count,\n",
    "                \"successful_tool_calls\": state.successful_tool_calls,\n",
    "                \"problem_statement\": state.problem_statement # For memory\n",
    "            }\n",
    "            return {\"messages\": [response_message], \"final_solution\": final_solution} # Signal to end\n",
    "\n",
    "        # Prepare AIMessage with structured tool call if command is a known tool\n",
    "        tool_name, tool_args = parse_command_to_tool(command, state.repo_path)\n",
    "\n",
    "        if tool_name:\n",
    "            # Add state components needed by specific tools\n",
    "            if tool_name == \"file_viewer\":\n",
    "                tool_args[\"current_open_file\"] = state.current_open_file\n",
    "                tool_args[\"current_file_lines\"] = state.current_file_lines\n",
    "                tool_args[\"current_window_start_line\"] = state.current_window_start_line\n",
    "            elif tool_name == \"edit\":\n",
    "                 tool_args[\"current_open_file\"] = state.current_open_file\n",
    "                 tool_args[\"current_file_lines\"] = state.current_file_lines\n",
    "                 tool_args[\"current_window_start_line\"] = state.current_window_start_line # Pass for context if needed\n",
    "\n",
    "            ai_message = AIMessage(\n",
    "                content=response_message.content, # Keep original content for history\n",
    "                tool_calls=[{\n",
    "                    \"id\": f\"tool_{tool_name}_{state.tool_calls_count}\",\n",
    "                    \"name\": tool_name,\n",
    "                    \"args\": tool_args,\n",
    "                }]\n",
    "            )\n",
    "            state.tool_calls_count += 1\n",
    "            return {\"messages\": [ai_message]} # Pass to tool node\n",
    "        else:\n",
    "            # Command is not a known tool (likely a bash command)\n",
    "            # Treat it like a call to a generic 'bash' tool or run_tests\n",
    "            bash_tool_args = {\"command\": command, \"repo_path\": state.repo_path}\n",
    "            ai_message = AIMessage(\n",
    "                content=response_message.content,\n",
    "                tool_calls=[{\n",
    "                    \"id\": f\"tool_bash_{state.tool_calls_count}\",\n",
    "                    \"name\": \"run_tests\", # Use run_tests for general commands\n",
    "                    \"args\": bash_tool_args,\n",
    "                }]\n",
    "            )\n",
    "            state.tool_calls_count += 1\n",
    "            return {\"messages\": [ai_message]}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in agent node LLM call or parsing: {e}\", exc_info=True)\n",
    "        error_msg = f\"Agent Error: {e}\"\n",
    "        state.errors_encountered.append(error_msg)\n",
    "        # Return an error message to the agent? Or just end?\n",
    "        # Let's add the error to the history and let the agent try again.\n",
    "        return {\"messages\": [AIMessage(content=f\"Error occurred: {e}\"), HumanMessage(content=\"An error occurred. Please assess the situation and proceed.\")]}\n",
    "\n",
    "\n",
    "def parse_agent_response(response: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parses the LLM response into DISCUSSION and COMMAND.\"\"\"\n",
    "    discussion_match = re.search(r\"DISCUSSION\\s*(.*?)\\s*COMMAND\", response, re.DOTALL | re.IGNORECASE)\n",
    "    command_match = re.search(r\"COMMAND\\s*(.*)\", response, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if discussion_match and command_match:\n",
    "        discussion = discussion_match.group(1).strip()\n",
    "        command = command_match.group(1).strip()\n",
    "        # Remove potential backticks around command\n",
    "        if command.startswith(\"`\") and command.endswith(\"`\"):\n",
    "            command = command[1:-1]\n",
    "        return {\"discussion\": discussion, \"command\": command, \"error\": None}\n",
    "    else:\n",
    "        return {\"discussion\": None, \"command\": None, \"error\": \"Could not parse DISCUSSION and COMMAND sections.\"}\n",
    "\n",
    "def parse_command_to_tool(command: str, repo_path: str) -> Tuple[Optional[str], Dict]:\n",
    "    \"\"\"Tries to parse the command string into a known tool call.\"\"\"\n",
    "    parts = command.split()\n",
    "    tool_name = parts[0]\n",
    "\n",
    "    # Match known tools and parse args (simple parsing, assumes args are space-separated)\n",
    "    # This needs to be more robust in production (e.g., handle quoted args)\n",
    "    args = parts[1:]\n",
    "\n",
    "    known_tools = {tool.name: tool.args_schema for tool in swe_agent_tools}\n",
    "\n",
    "    if tool_name in known_tools:\n",
    "        schema = known_tools[tool_name]\n",
    "        parsed_args = {\"repo_path\": repo_path} # Always include repo_path\n",
    "\n",
    "        # --- Argument Parsing Logic (Needs Improvement) ---\n",
    "        # This is a simplified parser. A real implementation might use\n",
    "        # argparse or regex for more complex commands.\n",
    "        try:\n",
    "            if tool_name == \"find_file\":\n",
    "                if len(args) >= 1: parsed_args[\"filename\"] = args[0]\n",
    "                else: return None, {} # Invalid args\n",
    "            elif tool_name == \"search\":\n",
    "                 # search <query> <type> [target]\n",
    "                if len(args) >= 2:\n",
    "                    parsed_args[\"query\"] = args[0]\n",
    "                    parsed_args[\"search_type\"] = args[1]\n",
    "                    if len(args) > 2: parsed_args[\"target\"] = args[2]\n",
    "                else: return None, {}\n",
    "            elif tool_name == \"file_viewer\":\n",
    "                 # file_viewer <action> [path/line]\n",
    "                if len(args) >= 1:\n",
    "                    parsed_args[\"action\"] = args[0]\n",
    "                    if args[0] == \"open\" and len(args) >= 2:\n",
    "                        parsed_args[\"path\"] = args[1]\n",
    "                        if len(args) > 2 and args[2].isdigit(): parsed_args[\"line_number\"] = int(args[2])\n",
    "                    elif args[0] == \"goto\" and len(args) >= 2 and args[1].isdigit():\n",
    "                        parsed_args[\"line_number\"] = int(args[1])\n",
    "                    elif args[0] not in [\"scroll_up\", \"scroll_down\"]:\n",
    "                         return None, {} # Invalid action or args\n",
    "                else: return None, {}\n",
    "            elif tool_name == \"edit\":\n",
    "                 # edit <start>:<end> <replacement_text...>\n",
    "                if len(args) >= 2 and \":\" in args[0]:\n",
    "                    line_range = args[0].split(\":\")\n",
    "                    if len(line_range) == 2 and line_range[0].isdigit() and line_range[1].isdigit():\n",
    "                        parsed_args[\"start_line\"] = int(line_range[0])\n",
    "                        parsed_args[\"end_line\"] = int(line_range[1])\n",
    "                        parsed_args[\"replacement_text\"] = \" \".join(args[1:]) # Join remaining parts\n",
    "                        # Handle the 'end_of_edit' marker if present (though not strictly needed with this parsing)\n",
    "                        if parsed_args[\"replacement_text\"].endswith(\"end_of_edit\"):\n",
    "                             parsed_args[\"replacement_text\"] = parsed_args[\"replacement_text\"][:-len(\"end_of_edit\")].strip()\n",
    "                    else: return None, {}\n",
    "                else: return None, {}\n",
    "            elif tool_name == \"run_tests\":\n",
    "                 if len(args) >= 1: parsed_args[\"command\"] = \" \".join(args)\n",
    "                 else: return None, {}\n",
    "\n",
    "            # Validate required args (basic check)\n",
    "            required = schema.model_json_schema().get('required', [])\n",
    "            for req_arg in required:\n",
    "                 if req_arg not in parsed_args and req_arg != \"repo_path\": # repo_path is always added\n",
    "                     logger.warning(f\"Missing required argument '{req_arg}' for tool '{tool_name}' in command: {command}\")\n",
    "                     # return None, {} # Don't call tool if required args missing\n",
    "            return tool_name, parsed_args\n",
    "\n",
    "        except (ValueError, IndexError) as e:\n",
    "            logger.error(f\"Error parsing arguments for tool '{tool_name}' from command '{command}': {e}\")\n",
    "            return None, {} # Failed parsing\n",
    "\n",
    "    return None, {} # Not a known tool\n",
    "\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"Executes the tool call requested by the agent.\"\"\"\n",
    "    if not state.messages or not isinstance(state.messages[-1], AIMessage) or not state.messages[-1].tool_calls:\n",
    "        # Should not happen if graph logic is correct\n",
    "        logger.warning(\"Tool node called without a preceding tool call AIMessage.\")\n",
    "        return {}\n",
    "\n",
    "    tool_call = state.messages[-1].tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_args = tool_call[\"args\"]\n",
    "    tool_id = tool_call[\"id\"]\n",
    "\n",
    "    logger.info(f\"--- Tool Node: Executing '{tool_name}' ---\")\n",
    "\n",
    "    tool_map = {tool.name: tool for tool in swe_agent_tools}\n",
    "    if tool_name not in tool_map:\n",
    "        error_msg = f\"Error: Tool '{tool_name}' not found.\"\n",
    "        tool_message = ToolMessage(content=error_msg, tool_call_id=tool_id)\n",
    "        state.errors_encountered.append(error_msg)\n",
    "        return {\"messages\": [tool_message]}\n",
    "\n",
    "    selected_tool = tool_map[tool_name]\n",
    "    output_state_update = {}\n",
    "\n",
    "    try:\n",
    "        # Execute the tool\n",
    "        tool_output = selected_tool.invoke(tool_args)\n",
    "\n",
    "        # Handle tools that return state updates (file_viewer, edit)\n",
    "        if isinstance(tool_output, dict) and \"viewer_output\" in tool_output:\n",
    "            output_content = tool_output[\"viewer_output\"]\n",
    "            # Update agent state from tool output\n",
    "            output_state_update[\"current_open_file\"] = tool_output.get(\"current_open_file\", state.current_open_file)\n",
    "            output_state_update[\"current_file_lines\"] = tool_output.get(\"current_file_lines\", state.current_file_lines)\n",
    "            output_state_update[\"current_window_start_line\"] = tool_output.get(\"current_window_start_line\", state.current_window_start_line)\n",
    "        elif isinstance(tool_output, str):\n",
    "             output_content = tool_output\n",
    "        else:\n",
    "             output_content = json.dumps(tool_output) # Default to JSON string\n",
    "\n",
    "        # Check for errors indicated by the tool output string\n",
    "        if \"Error:\" in output_content or \"failed\" in output_content.lower() or \"Errno\" in output_content or \"Traceback\" in output_content:\n",
    "             state.errors_encountered.append(f\"Tool Error ({tool_name}): {output_content[:200]}\")\n",
    "             logger.warning(f\"Tool '{tool_name}' indicated an error: {output_content[:200]}...\")\n",
    "        else:\n",
    "             state.successful_tool_calls += 1\n",
    "\n",
    "        state.execution_log.append(f\"Tool '{tool_name}' Output: {output_content[:300]}...\")\n",
    "        tool_message = ToolMessage(content=output_content, tool_call_id=tool_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error executing tool '{tool_name}': {e}\"\n",
    "        logger.error(error_msg, exc_info=True)\n",
    "        state.errors_encountered.append(error_msg)\n",
    "        tool_message = ToolMessage(content=error_msg, tool_call_id=tool_id)\n",
    "\n",
    "    # Return the tool message and any state updates\n",
    "    return {\"messages\": [tool_message], **output_state_update} # Merge state updates\n",
    "\n",
    "\n",
    "# --- Conditional Edges ---\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Determines the next step after the agent or tool node.\"\"\"\n",
    "    if state.final_solution:\n",
    "        logger.info(\"Agent submitted or reached end state. Finalizing.\")\n",
    "        return \"finalize\" # Agent submitted or hit limit\n",
    "\n",
    "    last_message = state.messages[-1] if state.messages else None\n",
    "\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        # Agent requested a tool call\n",
    "        return \"call_tool\"\n",
    "    elif isinstance(last_message, ToolMessage):\n",
    "         # Tool just ran, go back to agent for next step\n",
    "        return \"agent\"\n",
    "    elif isinstance(last_message, HumanMessage) and \"error occurred\" in last_message.content.lower():\n",
    "         # An error occurred in the agent node, let agent try to recover\n",
    "         return \"agent\"\n",
    "    elif isinstance(last_message, HumanMessage) and \"formatted correctly\" in last_message.content.lower():\n",
    "         # Agent response was malformed, let agent try again\n",
    "         return \"agent\"\n",
    "    elif state.turn_count > 50: # Check max turns again\n",
    "         logger.warning(\"Max turns reached, forcing finalization.\")\n",
    "         return \"finalize\"\n",
    "    else:\n",
    "        # Default: continue to agent\n",
    "        return \"agent\"\n",
    "\n",
    "# --- Build Graph ---\n",
    "\n",
    "def build_agent_workflow():\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    workflow.add_node(\"setup\", planning_and_setup_node)\n",
    "    workflow.add_node(\"agent\", agent_node)\n",
    "    # workflow.add_node(\"tool_executor\", ToolNode(swe_agent_tools)) # Using custom tool node\n",
    "    workflow.add_node(\"tool_executor\", tool_node)\n",
    "\n",
    "    workflow.set_entry_point(\"setup\")\n",
    "\n",
    "    workflow.add_edge(\"setup\", \"agent\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"call_tool\": \"tool_executor\",\n",
    "            \"finalize\": END,\n",
    "            \"agent\": \"agent\", # Loop back if agent needs to retry (e.g., after format error)\n",
    "        }\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"tool_executor\",\n",
    "         should_continue, # After tool runs, decide next step (usually back to agent)\n",
    "         {\n",
    "            \"agent\": \"agent\",\n",
    "            \"finalize\": END, # Should ideally not happen directly from tool node\n",
    "            \"call_tool\": \"tool_executor\" # Should not happen\n",
    "         }\n",
    "    )\n",
    "\n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    logger.info(\"SWE-Agent-CL workflow compiled.\")\n",
    "    return app\n",
    "\n",
    "# Build the workflow only if models are available\n",
    "agent_workflow = build_agent_workflow() if initialized_models else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 18:57:41,046 - INFO - --- Testing Agent Workflow on First Task ---\n",
      "2025-05-13 18:57:41,047 - INFO - Using model 'google/gemini-2.0-flash' for the test run.\n",
      "2025-05-13 18:57:41,048 - INFO - Setting up local/dummy repository at: cloned_repos/dummy_math_project\n",
      "2025-05-13 18:57:41,049 - INFO - Dummy files created/reset in cloned_repos/dummy_math_project\n",
      "2025-05-13 18:57:41,049 - INFO - Repository for test task 'local__dummy_math_project_task_1' prepared at: /Users/Shayan/Library/CloudStorage/GoogleDrive-sc4040@columbia.edu/My Drive/Academics/Spring 2025/COMS 4995 - Neural Nets & Deep Learning/NNDL Final Project/agents-never-forget/agent_v2/cloned_repos/dummy_math_project\n",
      "2025-05-13 18:57:41,050 - INFO - Semantic memory cleared.\n",
      "2025-05-13 18:57:41,050 - INFO - Cleared semantic memory before test run.\n",
      "2025-05-13 18:57:41,050 - INFO - Invoking agent workflow for test task: local__dummy_math_project_task_1\n",
      "2025-05-13 18:57:41,051 - INFO - Problem: The function `add(a, b)` in `math_utils.py` currently returns `a - b`. It should return `a + b`....\n",
      "2025-05-13 18:57:41,053 - INFO - --- Planning/Setup Node for Task local__dummy_math_project_task_1 (Seq: dummy_math_project_sequence) ---\n",
      "2025-05-13 18:57:41,053 - WARNING - Semantic memory index not initialized or empty.\n",
      "2025-05-13 18:57:41,054 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:41,952 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:42,243 - WARNING - Tool 'run_tests' indicated an error: Command: file_viewer --action open --path math_utils.py\n",
      "Exit Code: 127\n",
      "--- STDERR (last 2000 chars) ---\n",
      "/bin/sh: file_viewer: command not found\n",
      "Command failed or produced errors (non-zero exit code)....\n",
      "2025-05-13 18:57:42,248 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:43,382 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:43,413 - WARNING - Tool 'run_tests' indicated an error: Command: . I should use the `file_viewer` command through the agent's API.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(action='open', path='math_utils.py'))\n",
      "```\n",
      "Exit Code: 2\n",
      "--- STDERR (last 2...\n",
      "2025-05-13 18:57:43,416 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:44,410 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:44,438 - WARNING - Tool 'run_tests' indicated an error: Command: , which is incorrect. I should use the `file_viewer` command directly through the agent's API.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(action='open', path='math_utils.py'))\n",
      "```\n",
      "Ex...\n",
      "2025-05-13 18:57:44,440 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:45,436 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:45,462 - WARNING - Tool 'run_tests' indicated an error: Command: directly through the agent's API.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(action='open', path='math_utils.py'))\n",
      "```\n",
      "Exit Code: 2\n",
      "--- STDERR (last 2000 chars) ---\n",
      "/bin/sh: -c: line...\n",
      "2025-05-13 18:57:45,465 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:46,584 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:46,634 - WARNING - Tool 'run_tests' indicated an error: Command: incorrectly. My goal is to open the `math_utils.py` file to inspect the `add` function. I should be using the `file_viewer` command directly.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_view...\n",
      "2025-05-13 18:57:46,636 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:47,682 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:48,158 - WARNING - Tool 'run_tests' indicated an error: Command: directly.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspect the `add` function. I should be using the `file_viewer` command directly.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file...\n",
      "2025-05-13 18:57:48,160 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:49,217 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:49,241 - WARNING - Tool 'run_tests' indicated an error: Command: . I need to call it directly without wrapping it in `run_tests` or any other unnecessary commands.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(action='open', path='math_utils.py'))\n",
      "``...\n",
      "2025-05-13 18:57:49,243 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:50,345 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:50,482 - WARNING - Tool 'run_tests' indicated an error: Command: s. I should just call the `file_viewer` API with the correct arguments.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(action='open', path='math_utils.py'))\n",
      "```\n",
      "Exit Code: 1\n",
      "--- STDERR (...\n",
      "2025-05-13 18:57:50,484 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:51,779 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:51,809 - WARNING - Tool 'run_tests' indicated an error: Command: using `run_tests` or wrapping it in a `print` statement. I need to use the `file_viewer` command directly through the agent's API.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(action='...\n",
      "2025-05-13 18:57:51,812 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:53,006 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:53,021 - WARNING - Tool 'run_tests' indicated an error: Command: .\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspect the `add` function. I should be using the `file_viewer` command directly.\n",
      "\n",
      "COMMAND\n",
      "```tool_code\n",
      "print(default_api.file_viewer(...\n",
      "2025-05-13 18:57:53,023 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:54,201 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:54,226 - WARNING - Tool 'run_tests' indicated an error: Command: using `run_tests` or wrapping it in a `print` statement. I need to use the `file_viewer` command directly through the agent's API.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspe...\n",
      "2025-05-13 18:57:54,228 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:55,567 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:55,609 - WARNING - Tool 'run_tests' indicated an error: Command: and also including the explanation and code block in the command itself. I need to directly use the `file_viewer` command.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspect the `...\n",
      "2025-05-13 18:57:55,615 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:56,901 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:56,928 - WARNING - Tool 'run_tests' indicated an error: Command: using `run_tests` or wrapping it in a `print` statement. I need to use the `file_viewer` command directly through the agent's API.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspe...\n",
      "2025-05-13 18:57:56,930 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:58,034 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:58,085 - WARNING - Tool 'run_tests' indicated an error: Command: s in `run_tests` or `print` statements. I should just call the tool directly.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspect the `add` function. I should be using the `file_vi...\n",
      "2025-05-13 18:57:58,087 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:57:59,367 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:57:59,429 - WARNING - Tool 'run_tests' indicated an error: Command: directly.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to inspect the `add` function. I should be using the `file_viewer` command directly.\n",
      "\n",
      "COMMAND\n",
      "file_viewer --action open --path ma...\n",
      "2025-05-13 18:57:59,431 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:00,901 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:00,933 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I keep making mistakes by either wrapping it in `run_tests` or `print` statements, or by including the explanation in the command itself. I need to remember that the `file_viewer` ...\n",
      "2025-05-13 18:58:00,935 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:02,336 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:02,366 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am making mistakes by either wrapping it in `run_tests` or `print` statements, or by including the explanation in the command itself. I need to remember that the `file_viewer` co...\n",
      "2025-05-13 18:58:02,368 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:03,588 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:03,602 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am making mistakes by including the explanation in the command itself. I need to remember that the `file_viewer` command is directly accessible through the agent's API and should...\n",
      "2025-05-13 18:58:03,604 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:04,785 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:04,817 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command.\n",
      "\n",
      "DISCUSSION\n",
      "I need to open the `math_utils.py` file to...\n",
      "2025-05-13 18:58:04,820 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:06,011 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:06,027 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:06,030 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:07,446 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:07,469 - WARNING - Tool 'run_tests' indicated an error: Command: . I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's API, not di...\n",
      "2025-05-13 18:58:07,471 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:08,730 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:08,762 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:08,765 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:10,114 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:10,145 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:10,184 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:11,962 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:12,697 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:12,701 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:14,210 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:14,241 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:14,244 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:15,825 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:15,842 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:15,844 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:17,379 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:17,410 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:17,413 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:19,026 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:19,597 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:19,601 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:21,429 - INFO - --- Tool Node: Executing 'run_tests' ---\n",
      "2025-05-13 18:58:21,452 - WARNING - Tool 'run_tests' indicated an error: Command: correctly. I am including the explanation in the command itself, which is incorrect. I need to separate the explanation and the command. I also need to call the command through the agent's AP...\n",
      "2025-05-13 18:58:21,454 - INFO - --- Agent Node (Turn 0) ---\n",
      "2025-05-13 18:58:23,433 - ERROR - Error during agent workflow test: Recursion limit of 60 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/gv/dhs3yrtj7qg404ybnlwrzsgh0000gn/T/ipykernel_69206/975380513.py\", line 64, in <module>\n",
      "    final_test_state_dict = agent_workflow.invoke(initial_state_test, config=config)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2336, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/agents/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2013, in stream\n",
      "    raise GraphRecursionError(msg)\n",
      "langgraph.errors.GraphRecursionError: Recursion limit of 60 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\n"
     ]
    }
   ],
   "source": [
    "# --- Test the agent workflow on a single task ---\n",
    "# Ensure swe_bench_cl is loaded (either real or dummy)\n",
    "logger.info(\"--- Testing Agent Workflow on First Task ---\")\n",
    "\n",
    "try:\n",
    "    # Select the first task from the first sequence\n",
    "    # test_sequence = swe_bench_cl[\"sequences\"][0]\n",
    "    test_sequence = dummy_swe_bench_cl[\"sequences\"][0]\n",
    "    test_task = test_sequence[\"tasks\"][0]\n",
    "    test_task_id = test_task[\"metadata\"][\"instance_id\"]\n",
    "    test_sequence_id = test_sequence[\"id\"]\n",
    "\n",
    "    # Select a model (prefer faster/cheaper if available)\n",
    "    test_model_id = MODELS[0] # Use the first available initialized model\n",
    "    logger.info(f\"Using model '{test_model_id}' for the test run.\")\n",
    "\n",
    "    # Setup the repository for the test task\n",
    "    task_repo_identifier = test_task[\"metadata\"][\"repo\"]\n",
    "    task_base_commit = test_task[\"metadata\"][\"base_commit\"]\n",
    "\n",
    "    # Determine dummy setup function if needed\n",
    "    dummy_setup_func = None\n",
    "    if task_repo_identifier.startswith(\"local/\"):\n",
    "        # Ensure the dummy setup function is available in this scope\n",
    "        if 'dummy_files_setup_for_test' in globals():\n",
    "            dummy_setup_func = dummy_files_setup_for_test\n",
    "        else:\n",
    "            logger.error(\"Dummy setup function 'dummy_files_setup_for_test' not found.\")\n",
    "            raise RuntimeError(\"Missing dummy setup function.\")\n",
    "\n",
    "    task_specific_repo_path = setup_repository(\n",
    "        task_repo_identifier,\n",
    "        task_base_commit,\n",
    "        REPOS_BASE_DIR,\n",
    "        dummy_files_setup=dummy_setup_func\n",
    "    )\n",
    "    logger.info(f\"Repository for test task '{test_task_id}' prepared at: {task_specific_repo_path}\")\n",
    "\n",
    "    # Prepare initial state\n",
    "    initial_state_test = AgentState(\n",
    "        problem_statement=test_task[\"task\"][\"problem_statement\"],\n",
    "        hints=test_task[\"task\"].get(\"hints_text\"),\n",
    "        repo_path=str(task_specific_repo_path.resolve()),\n",
    "        task_id=test_task_id,\n",
    "        sequence_id=test_sequence_id,\n",
    "        task_data=test_task,\n",
    "        model_id=test_model_id,\n",
    "        memory_enabled=(True if memory_system else False), # Enable memory if system exists\n",
    "        # Other fields default\n",
    "    )\n",
    "\n",
    "    # Clear memory before test run if memory system exists and is enabled\n",
    "    if memory_system and initial_state_test.memory_enabled:\n",
    "        memory_system.clear_memory()\n",
    "        logger.info(\"Cleared semantic memory before test run.\")\n",
    "\n",
    "    logger.info(f\"Invoking agent workflow for test task: {test_task_id}\")\n",
    "    logger.info(f\"Problem: {initial_state_test.problem_statement[:100]}...\")\n",
    "\n",
    "    # Configuration for the graph run\n",
    "    config = {\"recursion_limit\": 60} # Increase recursion limit for more steps\n",
    "\n",
    "    # Run the workflow\n",
    "    final_test_state_dict = agent_workflow.invoke(initial_state_test, config=config)\n",
    "\n",
    "    # Convert final state dict back to AgentState object for easier access\n",
    "    final_test_state = AgentState(**final_test_state_dict)\n",
    "\n",
    "    logger.info(\"--- Agent Workflow Test Completed ---\")\n",
    "\n",
    "    # Log final state details\n",
    "    logger.info(f\"Final turn count: {final_test_state.turn_count}\")\n",
    "    logger.info(f\"Total tool calls: {final_test_state.tool_calls_count}\")\n",
    "    logger.info(f\"Successful tool calls: {final_test_state.successful_tool_calls}\")\n",
    "    logger.info(f\"Errors encountered: {len(final_test_state.errors_encountered)}\")\n",
    "    if final_test_state.errors_encountered:\n",
    "            logger.warning(f\"Last error: {final_test_state.errors_encountered[-1]}\")\n",
    "\n",
    "    if final_test_state.final_solution:\n",
    "        logger.info(f\"Final Solution Summary: {final_test_state.final_solution.get('solution_summary', 'N/A')}\")\n",
    "        logger.info(f\"Final Rationale: {final_test_state.final_solution.get('final_rationale', 'N/A')}\")\n",
    "        # Note: tests_passed is evaluated *after* the run by the evaluator\n",
    "    else:\n",
    "        logger.warning(\"No final solution was generated (agent might have errored or timed out).\")\n",
    "\n",
    "    # Check dummy file content if applicable\n",
    "    if USE_DUMMY_DATA:\n",
    "        dummy_file_path = task_specific_repo_path / \"math_utils.py\"\n",
    "        if dummy_file_path.exists():\n",
    "            math_utils_content_after = dummy_file_path.read_text()\n",
    "            logger.info(f\"Content of math_utils.py after agent run:\\n{math_utils_content_after}\")\n",
    "            if \"return a + b\" in math_utils_content_after:\n",
    "                logger.info(\"Agent seems to have correctly edited math_utils.py for the dummy task!\")\n",
    "            else:\n",
    "                logger.warning(\"Agent did not correctly edit math_utils.py for the dummy task.\")\n",
    "        else:\n",
    "            logger.warning(f\"math_utils.py not found in {task_specific_repo_path} after test run.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during agent workflow test: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Evaluation Framework\n",
    "\n",
    "\n",
    "\n",
    " Implement the evaluation logic, including applying patches, running tests against `FAIL_TO_PASS` and `PASS_TO_PASS` criteria, and calculating metrics like Success Rate and Tool Use Efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patch as patch_parser # Use the 'patch' library: pip install python-patch\n",
    "\n",
    "def apply_patch(patch_content: str, repo_path: Path) -> bool:\n",
    "    \"\"\"Applies a patch file content to the repository.\"\"\"\n",
    "    # Use the 'patch' library\n",
    "    try:\n",
    "        pset = patch_parser.fromstring(patch_content.encode())\n",
    "        if not pset:\n",
    "             logger.warning(\"Patch string could not be parsed.\")\n",
    "             return False\n",
    "        success = pset.apply(root=str(repo_path))\n",
    "        if success:\n",
    "            logger.info(\"Patch applied successfully using python-patch.\")\n",
    "            return True\n",
    "        else:\n",
    "            # Try git apply as fallback? Requires patch file.\n",
    "            logger.warning(\"python-patch failed to apply patch. Trying git apply...\")\n",
    "            patch_file = repo_path / \".eval_temp.patch\"\n",
    "            with open(patch_file, \"w\") as f:\n",
    "                f.write(patch_content)\n",
    "            # Check patch applicability without applying\n",
    "            check_cmd = [\"git\", \"apply\", \"--check\", str(patch_file)]\n",
    "            check_proc = subprocess.run(check_cmd, cwd=repo_path, capture_output=True, text=True, timeout=30)\n",
    "            if check_proc.returncode != 0:\n",
    "                 logger.error(f\"git apply --check failed for patch:\\n{check_proc.stderr or check_proc.stdout}\")\n",
    "                 patch_file.unlink()\n",
    "                 return False\n",
    "\n",
    "            # Apply the patch\n",
    "            apply_cmd = [\"git\", \"apply\", str(patch_file)]\n",
    "            apply_proc = subprocess.run(apply_cmd, cwd=repo_path, capture_output=True, text=True, timeout=30)\n",
    "            patch_file.unlink() # Clean up temp file\n",
    "\n",
    "            if apply_proc.returncode == 0:\n",
    "                logger.info(\"Patch applied successfully using git apply.\")\n",
    "                return True\n",
    "            else:\n",
    "                logger.error(f\"git apply failed:\\n{apply_proc.stderr or apply_proc.stdout}\")\n",
    "                # Attempt to reverse if possible? Difficult. Best effort.\n",
    "                # subprocess.run([\"git\", \"apply\", \"--reverse\", str(patch_file)], cwd=repo_path, capture_output=True)\n",
    "                return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error applying patch: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "\n",
    "def run_evaluation_tests(repo_path: Path, test_command: str = \"python -m unittest discover\") -> Tuple[bool, str, str]:\n",
    "    \"\"\"Runs the test suite command and captures output.\"\"\"\n",
    "    try:\n",
    "        process = subprocess.run(test_command, shell=True, cwd=repo_path, capture_output=True, text=True, timeout=600)\n",
    "        passed = process.returncode == 0\n",
    "        return passed, process.stdout, process.stderr\n",
    "    except subprocess.TimeoutExpired:\n",
    "        logger.error(f\"Test command '{test_command}' timed out.\")\n",
    "        return False, \"\", \"Test execution timed out.\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error running evaluation tests: {e}\")\n",
    "        return False, \"\", f\"Error running tests: {e}\"\n",
    "\n",
    "def check_test_outcomes(stdout: str, stderr: str, fail_to_pass: List[str], pass_to_pass: List[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the test outcomes match the expected FAIL_TO_PASS and PASS_TO_PASS criteria.\n",
    "    This is a simplified check based on stderr typically containing failure info.\n",
    "    A more robust approach would parse specific test runner output formats (unittest, pytest).\n",
    "    \"\"\"\n",
    "    output = stdout + \"\\n\" + stderr\n",
    "    all_passed = True\n",
    "\n",
    "    # Check FAIL_TO_PASS: These should NOT appear as failing in the output\n",
    "    for test_case in fail_to_pass:\n",
    "        # Simple check: if test case name appears with \"FAIL\" or \"ERROR\" nearby in stderr\n",
    "        if re.search(rf\"(FAIL|ERROR): {re.escape(test_case)}\", stderr, re.IGNORECASE):\n",
    "            logger.warning(f\"FAIL_TO_PASS check failed: Test '{test_case}' still seems to be failing.\")\n",
    "            all_passed = False\n",
    "            break # One failure is enough\n",
    "        # Check if it passed (absence of failure is weak evidence, but best effort here)\n",
    "\n",
    "    if not all_passed: return False\n",
    "\n",
    "    # Check PASS_TO_PASS: These should also NOT appear as failing\n",
    "    for test_case in pass_to_pass:\n",
    "        if re.search(rf\"(FAIL|ERROR): {re.escape(test_case)}\", stderr, re.IGNORECASE):\n",
    "            logger.warning(f\"PASS_TO_PASS check failed: Test '{test_case}' seems to have failed (regression).\")\n",
    "            all_passed = False\n",
    "            break\n",
    "\n",
    "    # Additionally, check for overall failure indicators like \"FAILED (\" or \"Ran ... tests ... failures=...\"\n",
    "    if re.search(r\"FAILED \\(\", stderr) or re.search(r\"failures=\\d+\", stderr, re.IGNORECASE) and not re.search(r\"failures=0\", stderr, re.IGNORECASE):\n",
    "         # This might catch failures not explicitly listed, but could be overly strict\n",
    "         # Let's rely primarily on the specific test case checks for now.\n",
    "         # Consider adding a check if *any* unexpected test failed.\n",
    "         pass\n",
    "\n",
    "\n",
    "    # If stderr is empty and exit code was 0, assume success if specific checks passed\n",
    "    if not stderr.strip() and all_passed:\n",
    "         logger.info(\"Test outcome check: No failures found in stderr, assuming success based on specific checks.\")\n",
    "         return True\n",
    "\n",
    "    # If specific checks passed but there's other failure info, log it but maybe still return True?\n",
    "    # Let's be strict: if any specific check failed, return False. Otherwise, True.\n",
    "    return all_passed\n",
    "\n",
    "\n",
    "def evaluate_task_solution(task_data: Dict, repo_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluates the agent's solution for a single task by applying patches and running tests.\n",
    "    Returns True if the solution is considered successful, False otherwise.\n",
    "    \"\"\"\n",
    "    logger.info(f\"--- Evaluating Task: {task_data['metadata']['instance_id']} ---\")\n",
    "    eval_details = task_data[\"evaluation\"]\n",
    "    test_patch_content = eval_details.get(\"test_patch\")\n",
    "    fail_to_pass_tests = eval_details.get(\"FAIL_TO_PASS\", [])\n",
    "    pass_to_pass_tests = eval_details.get(\"PASS_TO_PASS\", [])\n",
    "\n",
    "    # 1. Apply the test patch to set up the evaluation environment\n",
    "    if test_patch_content:\n",
    "        logger.info(\"Applying test patch...\")\n",
    "        if not apply_patch(test_patch_content, repo_path):\n",
    "            logger.error(\"Failed to apply test patch. Cannot evaluate accurately.\")\n",
    "            # Should we return False or raise an error? Let's return False.\n",
    "            return False\n",
    "        logger.info(\"Test patch applied successfully.\")\n",
    "    else:\n",
    "        logger.info(\"No test patch provided for this task.\")\n",
    "\n",
    "    # 2. Run the tests (using a default command, could be customized)\n",
    "    # TODO: Potentially extract test command from dataset if available\n",
    "    test_command = \"python -m unittest discover\" # Default, adjust if needed\n",
    "    logger.info(f\"Running evaluation tests with command: '{test_command}'\")\n",
    "    tests_passed_exit_code, stdout, stderr = run_evaluation_tests(repo_path, test_command)\n",
    "\n",
    "    # 3. Check outcomes against FAIL_TO_PASS and PASS_TO_PASS\n",
    "    logger.info(\"Checking test outcomes...\")\n",
    "    # Basic check: Did the command exit successfully?\n",
    "    if not tests_passed_exit_code:\n",
    "         logger.warning(f\"Test command failed (exit code non-zero). Solution likely incorrect.\")\n",
    "         # Log snippets for debugging\n",
    "         logger.debug(f\"STDOUT Snippet:\\n{stdout[-500:]}\")\n",
    "         logger.debug(f\"STDERR Snippet:\\n{stderr[-1000:]}\")\n",
    "         # Even if exit code is bad, run specific checks in case only *some* tests failed as expected\n",
    "         # return False # Strict check\n",
    "\n",
    "    # Perform detailed check based on stdout/stderr and expected test lists\n",
    "    final_success = check_test_outcomes(stdout, stderr, fail_to_pass_tests, pass_to_pass_tests)\n",
    "\n",
    "    if final_success:\n",
    "        logger.info(f\"Evaluation PASSED for task {task_data['metadata']['instance_id']}.\")\n",
    "    else:\n",
    "        logger.info(f\"Evaluation FAILED for task {task_data['metadata']['instance_id']}.\")\n",
    "        # Log snippets if failure wasn't caught by exit code\n",
    "        if tests_passed_exit_code:\n",
    "             logger.debug(f\"STDOUT Snippet:\\n{stdout[-500:]}\")\n",
    "             logger.debug(f\"STDERR Snippet:\\n{stderr[-1000:]}\")\n",
    "\n",
    "\n",
    "    return final_success\n",
    "\n",
    "\n",
    "class SWEAgentCLEvaluator:\n",
    "    def __init__(self, dataset, workflow, memory_system):\n",
    "        if not dataset:\n",
    "            raise ValueError(\"Dataset not loaded or invalid.\")\n",
    "        self.dataset = dataset\n",
    "        self.agent_workflow = workflow\n",
    "        self.memory_system = memory_system\n",
    "        self.results = {} # Store results per model, per sequence\n",
    "\n",
    "    def run_evaluation(self, model_id: str, sequence_ids: Optional[List[str]] = None, memory_enabled: bool = True):\n",
    "        \"\"\"Runs evaluation for a given model on specified sequences.\"\"\"\n",
    "        if not self.agent_workflow:\n",
    "            logger.error(\"Agent workflow is not compiled. Cannot run evaluation.\")\n",
    "            return None\n",
    "        if model_id not in initialized_models:\n",
    "             logger.error(f\"Model {model_id} is not initialized. Cannot run evaluation.\")\n",
    "             return None\n",
    "\n",
    "        if sequence_ids is None:\n",
    "            sequence_ids = [seq[\"id\"] for seq in self.dataset[\"sequences\"]]\n",
    "\n",
    "        model_results = {\n",
    "            \"model_id\": model_id,\n",
    "            \"memory_enabled\": memory_enabled,\n",
    "            \"sequences\": {}\n",
    "        }\n",
    "\n",
    "        # Clear memory before starting evaluation run if the system exists\n",
    "        if self.memory_system:\n",
    "            self.memory_system.clear_memory()\n",
    "            logger.info(f\"Cleared semantic memory before evaluation run for {model_id} (Memory: {memory_enabled}).\")\n",
    "\n",
    "        for seq_id in sequence_ids:\n",
    "            sequence = next((s for s in self.dataset[\"sequences\"] if s[\"id\"] == seq_id), None)\n",
    "            if not sequence:\n",
    "                logger.warning(f\"Sequence {seq_id} not found in dataset. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"--- Evaluating Model '{model_id}' on Sequence '{seq_id}' (Memory: {memory_enabled}) ---\")\n",
    "            sequence_results = {\n",
    "                \"tasks_total\": sequence[\"num_tasks\"],\n",
    "                \"tasks_attempted\": 0,\n",
    "                \"tasks_succeeded\": 0,\n",
    "                \"total_tool_calls\": 0,\n",
    "                \"total_successful_tool_calls\": 0,\n",
    "                \"task_details\": {}\n",
    "            }\n",
    "\n",
    "            # Sort tasks by sequence position\n",
    "            tasks_in_sequence = sorted(sequence[\"tasks\"], key=lambda t: t[\"continual_learning\"][\"sequence_position\"])\n",
    "\n",
    "            for task in tqdm(tasks_in_sequence, desc=f\"Tasks in {seq_id}\"):\n",
    "                task_id = task[\"metadata\"][\"instance_id\"]\n",
    "                repo_identifier = task[\"metadata\"][\"repo\"]\n",
    "                base_commit = task[\"metadata\"][\"base_commit\"]\n",
    "\n",
    "                # Determine dummy setup function\n",
    "                dummy_setup_func = None\n",
    "                if repo_identifier.startswith(\"local/\"):\n",
    "                     if 'dummy_files_setup_for_test' in globals(): dummy_setup_func = dummy_files_setup_for_test\n",
    "                     else: raise RuntimeError(\"Missing dummy setup function for local repo.\")\n",
    "\n",
    "                try:\n",
    "                    # 1. Setup repository to the correct base commit (clean state)\n",
    "                    repo_path = setup_repository(repo_identifier, base_commit, REPOS_BASE_DIR, dummy_files_setup=dummy_setup_func)\n",
    "\n",
    "                    # 2. Prepare initial state for the agent\n",
    "                    initial_state = AgentState(\n",
    "                        problem_statement=task[\"task\"][\"problem_statement\"],\n",
    "                        hints=task[\"task\"].get(\"hints_text\"),\n",
    "                        repo_path=str(repo_path.resolve()),\n",
    "                        task_id=task_id,\n",
    "                        sequence_id=seq_id,\n",
    "                        task_data=task,\n",
    "                        model_id=model_id,\n",
    "                        memory_enabled=memory_enabled and bool(self.memory_system) # Only enable if system exists\n",
    "                    )\n",
    "\n",
    "                    # 3. Run the agent workflow\n",
    "                    config = {\"recursion_limit\": 60} # Max agent steps\n",
    "                    final_state_dict = self.agent_workflow.invoke(initial_state, config=config)\n",
    "                    final_state = AgentState(**final_state_dict) # Convert back to object\n",
    "\n",
    "                    sequence_results[\"tasks_attempted\"] += 1\n",
    "                    sequence_results[\"total_tool_calls\"] += final_state.tool_calls_count\n",
    "                    sequence_results[\"total_successful_tool_calls\"] += final_state.successful_tool_calls\n",
    "\n",
    "                    # 4. Evaluate the final state of the repository\n",
    "                    # The agent should have made edits directly.\n",
    "                    task_success = evaluate_task_solution(task, repo_path)\n",
    "\n",
    "                    if task_success:\n",
    "                        sequence_results[\"tasks_succeeded\"] += 1\n",
    "\n",
    "                    # 5. Record results and add experience to memory (if enabled)\n",
    "                    task_result_detail = {\n",
    "                        \"success\": task_success,\n",
    "                        \"tool_calls\": final_state.tool_calls_count,\n",
    "                        \"successful_tool_calls\": final_state.successful_tool_calls,\n",
    "                        \"turns\": final_state.turn_count,\n",
    "                        \"errors\": final_state.errors_encountered,\n",
    "                        \"final_solution_summary\": final_state.final_solution.get('solution_summary', 'N/A') if final_state.final_solution else 'N/A'\n",
    "                    }\n",
    "                    sequence_results[\"task_details\"][task_id] = task_result_detail\n",
    "\n",
    "                    if self.memory_system and memory_enabled:\n",
    "                        # Prepare data for memory storage\n",
    "                        memory_data = final_state.final_solution or {} # Use final solution if available\n",
    "                        memory_data[\"tests_passed\"] = task_success # Add evaluation result\n",
    "                        memory_data[\"tool_calls_count\"] = final_state.tool_calls_count # Ensure counts are included\n",
    "                        memory_data[\"successful_tool_calls\"] = final_state.successful_tool_calls\n",
    "                        memory_data[\"problem_statement\"] = initial_state.problem_statement # Add problem statement\n",
    "\n",
    "                        self.memory_system.add_experience_to_memory(task_id, seq_id, memory_data)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error evaluating task {task_id}: {e}\", exc_info=True)\n",
    "                    sequence_results[\"task_details\"][task_id] = {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "            # Calculate sequence summary metrics\n",
    "            seq_success_rate = (sequence_results[\"tasks_succeeded\"] / sequence_results[\"tasks_attempted\"]) if sequence_results[\"tasks_attempted\"] > 0 else 0\n",
    "            seq_avg_tool_calls = (sequence_results[\"total_tool_calls\"] / sequence_results[\"tasks_attempted\"]) if sequence_results[\"tasks_attempted\"] > 0 else 0\n",
    "            seq_tool_efficiency = (sequence_results[\"total_successful_tool_calls\"] / sequence_results[\"total_tool_calls\"]) if sequence_results[\"total_tool_calls\"] > 0 else 0\n",
    "\n",
    "            sequence_results[\"summary\"] = {\n",
    "                \"success_rate\": seq_success_rate,\n",
    "                \"avg_tool_calls\": seq_avg_tool_calls,\n",
    "                \"tool_use_efficiency\": seq_tool_efficiency\n",
    "            }\n",
    "            model_results[\"sequences\"][seq_id] = sequence_results\n",
    "            logger.info(f\"Sequence {seq_id} completed. Success Rate: {seq_success_rate*100:.1f}%, Avg Tools: {seq_avg_tool_calls:.1f}, Tool Efficiency: {seq_tool_efficiency*100:.1f}%\")\n",
    "\n",
    "        # Calculate overall metrics for the model run\n",
    "        total_succeeded = sum(s[\"tasks_succeeded\"] for s in model_results[\"sequences\"].values())\n",
    "        total_attempted = sum(s[\"tasks_attempted\"] for s in model_results[\"sequences\"].values())\n",
    "        overall_success_rate = (total_succeeded / total_attempted) if total_attempted > 0 else 0\n",
    "        overall_tool_calls = sum(s[\"total_tool_calls\"] for s in model_results[\"sequences\"].values())\n",
    "        overall_successful_tools = sum(s[\"total_successful_tool_calls\"] for s in model_results[\"sequences\"].values())\n",
    "        overall_tool_efficiency = (overall_successful_tools / overall_tool_calls) if overall_tool_calls > 0 else 0\n",
    "\n",
    "        model_results[\"overall_summary\"] = {\n",
    "            \"overall_success_rate\": overall_success_rate,\n",
    "            \"average_tool_calls_per_task\": (overall_tool_calls / total_attempted) if total_attempted > 0 else 0,\n",
    "            \"overall_tool_use_efficiency\": overall_tool_efficiency\n",
    "        }\n",
    "        logger.info(f\"--- Overall Results for Model '{model_id}' (Memory: {memory_enabled}) ---\")\n",
    "        logger.info(f\"Overall Success Rate: {overall_success_rate*100:.1f}% ({total_succeeded}/{total_attempted})\")\n",
    "        logger.info(f\"Overall Tool Use Efficiency: {overall_tool_efficiency*100:.1f}%\")\n",
    "\n",
    "        self.results[f\"{model_id}_{'mem' if memory_enabled else 'no_mem'}\"] = model_results\n",
    "        return model_results\n",
    "\n",
    "# Initialize evaluator (if workflow and dataset are ready)\n",
    "evaluator = None\n",
    "if agent_workflow and swe_bench_cl:\n",
    "    evaluator = SWEAgentCLEvaluator(swe_bench_cl, agent_workflow, memory_system)\n",
    "    logger.info(\"Evaluation framework initialized.\")\n",
    "else:\n",
    "    logger.warning(\"Evaluator not initialized because agent workflow or dataset is missing.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Experimental Design and Execution\n",
    "\n",
    "\n",
    "\n",
    " Define and run experiments to compare model performance with and without semantic memory, assessing continual learning capabilities like forward transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiments\n",
    "# Ensure MODELS list is up-to-date with initialized models\n",
    "experiments = []\n",
    "if MODELS:\n",
    "    # Experiment 1: Baseline (No Memory) - Run on first sequence\n",
    "    experiments.append({\n",
    "        \"name\": f\"Baseline_{MODELS[0].replace('/', '_')}_NoMemory\",\n",
    "        \"description\": f\"Evaluate {MODELS[0]} without memory on the first sequence.\",\n",
    "        \"model_id\": MODELS[0],\n",
    "        \"sequence_ids\": [swe_bench_cl[\"sequences\"][0][\"id\"]] if swe_bench_cl else [],\n",
    "        \"memory_enabled\": False\n",
    "    })\n",
    "    # Experiment 2: Memory-Augmented - Run on first sequence\n",
    "    experiments.append({\n",
    "        \"name\": f\"Memory_{MODELS[0].replace('/', '_')}_WithMemory\",\n",
    "        \"description\": f\"Evaluate {MODELS[0]} with memory on the first sequence.\",\n",
    "        \"model_id\": MODELS[0],\n",
    "        \"sequence_ids\": [swe_bench_cl[\"sequences\"][0][\"id\"]] if swe_bench_cl else [],\n",
    "        \"memory_enabled\": True\n",
    "    })\n",
    "    # Add more experiments for other models or sequences if desired\n",
    "    # Example: Evaluate a second model\n",
    "    if len(MODELS) > 1:\n",
    "         experiments.append({\n",
    "            \"name\": f\"Baseline_{MODELS[1].replace('/', '_')}_NoMemory\",\n",
    "            \"description\": f\"Evaluate {MODELS[1]} without memory on the first sequence.\",\n",
    "            \"model_id\": MODELS[1],\n",
    "            \"sequence_ids\": [swe_bench_cl[\"sequences\"][0][\"id\"]] if swe_bench_cl else [],\n",
    "            \"memory_enabled\": False\n",
    "         })\n",
    "         experiments.append({\n",
    "            \"name\": f\"Memory_{MODELS[1].replace('/', '_')}_WithMemory\",\n",
    "            \"description\": f\"Evaluate {MODELS[1]} with memory on the first sequence.\",\n",
    "            \"model_id\": MODELS[1],\n",
    "            \"sequence_ids\": [swe_bench_cl[\"sequences\"][0][\"id\"]] if swe_bench_cl else [],\n",
    "            \"memory_enabled\": True\n",
    "         })\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No models initialized, cannot define experiments.\")\n",
    "\n",
    "# Function to run experiments\n",
    "def run_experiments(experiments_to_run, evaluator_instance):\n",
    "    \"\"\"Runs the defined experiments using the evaluator.\"\"\"\n",
    "    if not evaluator_instance:\n",
    "        logger.error(\"Evaluator is not initialized. Cannot run experiments.\")\n",
    "        return None\n",
    "    if not experiments_to_run:\n",
    "        logger.warning(\"No experiments defined to run.\")\n",
    "        return {}\n",
    "\n",
    "    all_results = {}\n",
    "    for exp in experiments_to_run:\n",
    "        # Check if sequence ID is valid before running\n",
    "        if not exp[\"sequence_ids\"]:\n",
    "             logger.warning(f\"Skipping experiment '{exp['name']}' due to missing sequence IDs (dataset likely not loaded).\")\n",
    "             continue\n",
    "\n",
    "        logger.info(f\"\\n--- Running Experiment: {exp['name']} ---\")\n",
    "        logger.info(exp['description'])\n",
    "\n",
    "        exp_results = evaluator_instance.run_evaluation(\n",
    "            model_id=exp['model_id'],\n",
    "            sequence_ids=exp['sequence_ids'],\n",
    "            memory_enabled=exp['memory_enabled']\n",
    "        )\n",
    "        all_results[exp['name']] = exp_results\n",
    "        logger.info(f\"--- Completed Experiment: {exp['name']} ---\\n\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "# Run the experiments\n",
    "if evaluator and experiments:\n",
    "    logger.info(\"Starting experimental runs...\")\n",
    "    experiment_results = run_experiments(experiments, evaluator)\n",
    "    # Save results\n",
    "    results_path = Path(\"./swe_agent_cl_results.json\")\n",
    "    try:\n",
    "        with open(results_path, \"w\") as f:\n",
    "            # Use default=str to handle non-serializable types like Path if they sneak in\n",
    "            json.dump(experiment_results, f, indent=2, default=str)\n",
    "        logger.info(f\"Experiment results saved to {results_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save experiment results: {e}\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Skipping experiment execution as evaluator or experiments are not ready.\")\n",
    "    experiment_results = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Results Analysis and Visualization\n",
    "\n",
    "\n",
    "\n",
    " Analyze the collected results, focusing on comparing performance with and without memory to understand the impact of the semantic memory system and calculate forward transfer potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For easier data manipulation\n",
    "\n",
    "def analyze_results(results_data):\n",
    "    \"\"\"Analyzes and visualizes the experimental results.\"\"\"\n",
    "    if not results_data:\n",
    "        logger.warning(\"No experiment results to analyze.\")\n",
    "        return {\"summary\": \"No results available.\"}\n",
    "\n",
    "    analysis = {}\n",
    "    summary_data = []\n",
    "\n",
    "    # Process results into a flat structure for easier analysis\n",
    "    for exp_name, exp_result in results_data.items():\n",
    "        if not exp_result: continue # Skip failed/empty experiments\n",
    "\n",
    "        model_id = exp_result.get(\"model_id\", \"N/A\")\n",
    "        memory_enabled = exp_result.get(\"memory_enabled\", \"N/A\")\n",
    "        overall_summary = exp_result.get(\"overall_summary\", {})\n",
    "        success_rate = overall_summary.get(\"overall_success_rate\", 0)\n",
    "        tool_efficiency = overall_summary.get(\"overall_tool_use_efficiency\", 0)\n",
    "\n",
    "        # Extract sequence-level data if needed (e.g., for learning curves)\n",
    "        sequence_results = exp_result.get(\"sequences\", {})\n",
    "        for seq_id, seq_data in sequence_results.items():\n",
    "             seq_summary = seq_data.get(\"summary\", {})\n",
    "             summary_data.append({\n",
    "                 \"experiment\": exp_name,\n",
    "                 \"model_id\": model_id,\n",
    "                 \"memory_enabled\": memory_enabled,\n",
    "                 \"sequence_id\": seq_id,\n",
    "                 \"seq_success_rate\": seq_summary.get(\"success_rate\", 0),\n",
    "                 \"seq_avg_tool_calls\": seq_summary.get(\"avg_tool_calls\", 0),\n",
    "                 \"seq_tool_efficiency\": seq_summary.get(\"tool_use_efficiency\", 0),\n",
    "                 \"overall_success_rate\": success_rate, # Add overall for context\n",
    "                 \"overall_tool_efficiency\": tool_efficiency\n",
    "             })\n",
    "\n",
    "    if not summary_data:\n",
    "        logger.warning(\"No valid summary data extracted from results.\")\n",
    "        return {\"summary\": \"No valid summary data found.\"}\n",
    "\n",
    "    df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # --- Visualizations ---\n",
    "    output_dir = Path(\"./swe_agent_cl_plots\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # 1. Success Rate Comparison (Memory vs. No Memory per Model)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    pivot_success = df.pivot_table(index='model_id', columns='memory_enabled', values='overall_success_rate', aggfunc='mean')\n",
    "    if not pivot_success.empty:\n",
    "        pivot_success.plot(kind='bar', figsize=(12, 7))\n",
    "        plt.title('Overall Success Rate Comparison (Memory vs. No Memory)')\n",
    "        plt.ylabel('Success Rate')\n",
    "        plt.xlabel('Model ID')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Memory Enabled')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'success_rate_memory_comparison.png')\n",
    "        plt.show()\n",
    "        analysis[\"success_rate_plot\"] = 'success_rate_memory_comparison.png'\n",
    "\n",
    "    # 2. Tool Use Efficiency Comparison\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    pivot_tool = df.pivot_table(index='model_id', columns='memory_enabled', values='overall_tool_efficiency', aggfunc='mean')\n",
    "    if not pivot_tool.empty:\n",
    "        pivot_tool.plot(kind='bar', figsize=(12, 7))\n",
    "        plt.title('Overall Tool Use Efficiency Comparison (Memory vs. No Memory)')\n",
    "        plt.ylabel('Tool Use Efficiency (Successful/Total Calls)')\n",
    "        plt.xlabel('Model ID')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Memory Enabled')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / 'tool_efficiency_memory_comparison.png')\n",
    "        plt.show()\n",
    "        analysis[\"tool_efficiency_plot\"] = 'tool_efficiency_memory_comparison.png'\n",
    "\n",
    "    # --- Metric Calculation ---\n",
    "    analysis[\"metrics\"] = {}\n",
    "    for model_id in df['model_id'].unique():\n",
    "        model_df = df[df['model_id'] == model_id]\n",
    "        # Calculate Forward Transfer Potential (Simple version)\n",
    "        # Compare avg success rate with memory vs without memory on the *same* sequences\n",
    "        # Requires results from both memory=True and memory=False runs for the model.\n",
    "        mem_true_df = model_df[model_df['memory_enabled'] == True]\n",
    "        mem_false_df = model_df[model_df['memory_enabled'] == False]\n",
    "\n",
    "        if not mem_true_df.empty and not mem_false_df.empty:\n",
    "            # Average over sequences if multiple were run\n",
    "            avg_success_mem = mem_true_df['seq_success_rate'].mean()\n",
    "            avg_success_no_mem = mem_false_df['seq_success_rate'].mean()\n",
    "            forward_transfer_potential = avg_success_mem - avg_success_no_mem\n",
    "\n",
    "            analysis[\"metrics\"][model_id] = {\n",
    "                \"avg_success_rate_with_memory\": avg_success_mem,\n",
    "                \"avg_success_rate_without_memory\": avg_success_no_mem,\n",
    "                \"forward_transfer_potential\": forward_transfer_potential,\n",
    "                \"avg_tool_efficiency_with_memory\": mem_true_df['seq_tool_efficiency'].mean(),\n",
    "                 \"avg_tool_efficiency_without_memory\": mem_false_df['seq_tool_efficiency'].mean(),\n",
    "            }\n",
    "            print(f\"\\nMetrics for {model_id}:\")\n",
    "            print(f\"  Avg Success Rate (No Memory): {avg_success_no_mem:.3f}\")\n",
    "            print(f\"  Avg Success Rate (With Memory): {avg_success_mem:.3f}\")\n",
    "            print(f\"  Forward Transfer Potential: {forward_transfer_potential:.3f}\")\n",
    "        else:\n",
    "             analysis[\"metrics\"][model_id] = {\"notes\": \"Insufficient data for comparison (missing memory or no-memory run).\"}\n",
    "\n",
    "\n",
    "    analysis[\"summary\"] = \"Analysis complete. Plots saved to ./swe_agent_cl_plots/. Forward transfer potential calculated.\"\n",
    "    analysis[\"dataframe_summary\"] = df.to_dict('records') # Include processed data\n",
    "\n",
    "    return analysis\n",
    "\n",
    "# Analyze the results (if available)\n",
    "if experiment_results:\n",
    "    analysis_summary = analyze_results(experiment_results)\n",
    "    # Save analysis summary\n",
    "    analysis_path = Path(\"./swe_agent_cl_analysis.json\")\n",
    "    try:\n",
    "        with open(analysis_path, \"w\") as f:\n",
    "             json.dump(analysis_summary, f, indent=2, default=str)\n",
    "        logger.info(f\"Analysis summary saved to {analysis_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save analysis summary: {e}\")\n",
    "\n",
    "else:\n",
    "    logger.warning(\"No experiment results found to analyze.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 10. Findings and Conclusions (Example)\n",
    "\n",
    "\n",
    "\n",
    " *Based on the results (replace with actual findings after running):*\n",
    "\n",
    "\n",
    "\n",
    " *   **Memory Impact:** The semantic memory system demonstrated a [positive/negative/negligible] impact on overall success rates. For model X, the success rate improved by Y% when memory was enabled, suggesting [effective learning/no significant learning] from past tasks within the sequence.\n",
    "\n",
    " *   **Forward Transfer:** The calculated forward transfer potential was [positive/negative/zero] for most models, indicating that accessing solutions to earlier tasks [helped/hindered/did not affect] performance on later tasks. Model Z showed the highest positive transfer.\n",
    "\n",
    " *   **Tool Use:** Memory augmentation [did/did not] significantly affect tool use efficiency. Agents [used tools more effectively / showed similar efficiency] when memory was available.\n",
    "\n",
    " *   **Model Comparison:** Model A consistently outperformed Model B, both with and without memory, suggesting inherent differences in problem-solving capabilities for these tasks.\n",
    "\n",
    " *   **SWE-agent Adaptation:** The implemented ACI allowed the agents to interact with the environment effectively, using tools like `edit` with linting and structured file viewing. Challenges remain in [mention specific areas, e.g., complex multi-step edits, efficient search query formulation].\n",
    "\n",
    "\n",
    "\n",
    " **Conclusion:** This framework successfully integrates SWE-agent's ACI principles with a semantic memory system for continual learning evaluation. The results highlight the potential [and challenges] of using memory to improve agent performance on evolving software engineering tasks. Future work should focus on more sophisticated memory retrieval strategies, handling longer sequences, and refining the CL metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 11. Future Work and Extensions\n",
    "\n",
    "\n",
    "\n",
    " *   **Refined CL Metrics:** Implement robust calculation for Forgetting Rate and Backward Transfer, requiring more complex experimental setups (e.g., re-testing).\n",
    "\n",
    " *   **Advanced Memory:** Explore different memory structures (e.g., episodic memory, code-specific embeddings), retrieval strategies (e.g., filtering by code similarity), and context management techniques.\n",
    "\n",
    " *   **Longer Sequences:** Test on longer sequences to better evaluate long-term learning and forgetting.\n",
    "\n",
    " *   **Tool Improvement:** Enhance tool robustness, particularly argument parsing for complex commands. Implement search within the currently open file.\n",
    "\n",
    " *   **Agent Reasoning:** Improve agent planning and reflection, potentially adding explicit nodes for plan adjustment based on memory or reflection.\n",
    "\n",
    " *   **Evaluation Detail:** Parse test runner output more precisely for `check_test_outcomes` instead of relying solely on stderr patterns.\n",
    "\n",
    " *   **Fine-tuning:** Experiment with fine-tuning models on SWE-Bench-CL sequences to directly improve CL capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 12. References and Acknowledgments\n",
    "\n",
    "\n",
    "\n",
    " *   Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., & Press, O. (2024). *SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering*. arXiv preprint arXiv:2405.15793.\n",
    "\n",
    " *   Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., & Narasimhan, K. R. (2024). *SWE-bench: Can Language Models Resolve Real-world GitHub Issues?* Proceedings of the Twelfth International Conference on Learning Representations (ICLR).\n",
    "\n",
    " *   Anthropic Research. (2024). *Building Effective Agents with Foundation Models*. Retrieved from [https://www.anthropic.com/engineering/building-effective-agents](https://www.anthropic.com/engineering/building-effective-agents)\n",
    "\n",
    " *   LangChain & LangGraph documentation.\n",
    "\n",
    " *   SWE-Bench-CL Dataset (This project).\n",
    "\n",
    "\n",
    "\n",
    " **Acknowledgments:** Based on the original `eval_procedure.py` structure provided by the user. Incorporates ideas and methodologies from the SWE-agent paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
